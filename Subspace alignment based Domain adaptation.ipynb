{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsPiA2-qf8qt"
   },
   "source": [
    "## Method 3 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClnqOyCW4vG1"
   },
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYIPc9g0rPaT"
   },
   "source": [
    "Papers \n",
    "\n",
    " 1. https://arxiv.org/pdf/1507.05578.pdf\n",
    "\n",
    " 2.  https://openaccess.thecvf.com/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miJ8Fs6ysMpb"
   },
   "outputs": [],
   "source": [
    "# Initialize model and load (deep) fine-tuned weights\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 2\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('your_path/model_25.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQ8wh4aQ1hdh"
   },
   "source": [
    "### Source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YS2UJEJ6L_h"
   },
   "outputs": [],
   "source": [
    "# Source data/distribution = training set\n",
    "train_valid_img,_= get_img_with_bbox(train_ann_path)  \n",
    "training_data= CustomImageDataset(train_ann_path,img_folder, train_valid_img)\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True,collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytYGUu02_l8B"
   },
   "outputs": [],
   "source": [
    "\n",
    "thres_IoU= 0.50\n",
    "count=0\n",
    "\n",
    "X_source=torch.tensor([])\n",
    "bbox_idx=torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "      print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat=outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "\n",
    "    bbox_idx_to_keep=torch.tensor([])\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "      IoUs=bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "      IoUs = IoUs.reshape(1000)\n",
    "      bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRDe8YmF-lDY",
    "outputId": "d2dc5e91-eef2-45c9-e5cd-82c84f138a17"
   },
   "outputs": [],
   "source": [
    "X_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLH-_DqhCOMs"
   },
   "outputs": [],
   "source": [
    "#torch.save(X_source, '/content/gdrive/MyDrive/IFT-Projets/X_source_05.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGBMIBmXKIk_"
   },
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_source_scaled = scaler.fit_transform(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZsD6S0mFlnD"
   },
   "outputs": [],
   "source": [
    "# Apply PCA, keep only the first 100 components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKTqvinrL9dt",
    "outputId": "f58bfbc9-145f-452d-b1e2-d0d15a30cd07"
   },
   "outputs": [],
   "source": [
    "X_source_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "h9oHTG10G_29",
    "outputId": "55116b27-d1e6-4faf-cc8e-1ca9b2090f5f"
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gG8zdxL01kJ8"
   },
   "outputs": [],
   "source": [
    "#torch.save(X_source_proj, '/content/gdrive/MyDrive/IFT-Projets/X_source_proj_05.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHeI8z4r1kV5"
   },
   "source": [
    "### Target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFOlfrUIEbHB"
   },
   "outputs": [],
   "source": [
    "# Target data/distribution = trans test set\n",
    "trans_test_img,_ = get_img_with_bbox(trans_test_ann_path)   # takes about 1min to run on train data\n",
    "trans_test_data = CustomImageDataset(trans_test_ann_path,img_folder, trans_test_img)\n",
    "trans_test_dataloader = DataLoader(trans_test_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDQH0JbYJvIM"
   },
   "outputs": [],
   "source": [
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "      print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ2Fj3oMzqPD",
    "outputId": "0afe76c7-14b0-4701-e4d8-d0a632a43a7d"
   },
   "outputs": [],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHdw51l040_8"
   },
   "outputs": [],
   "source": [
    "#torch.save(X_target, '/content/gdrive/MyDrive/IFT-Projets/X_target_05.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfLNnlsASill"
   },
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_target_scaled = scaler.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYooL3BKSit2"
   },
   "outputs": [],
   "source": [
    "# Apply PCA, keep only the first 100 components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_target_scaled)\n",
    "X_target_proj = pca.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "4NMIGuEzKSzf",
    "outputId": "4fc25162-74ea-4729-e590-b9123b559699"
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) # we keep 100 dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNn2oAB1Si2e",
    "outputId": "da79f9c5-c74d-4ced-8c4d-f61a3e393e8f"
   },
   "outputs": [],
   "source": [
    "X_target_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfdtJdqLSi9X"
   },
   "outputs": [],
   "source": [
    "#torch.save(X_target_proj, '/content/gdrive/MyDrive/IFT-Projets/X_target_proj_05.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cq63Ro4raH-N"
   },
   "source": [
    "### Transformation matrix M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGqdZawIaNuK"
   },
   "outputs": [],
   "source": [
    " # ùëÄ is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHDiMJjiaN1a"
   },
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qdSDUcr9zipG",
    "outputId": "4f2337d4-e501-4570-d71f-bb5e6c0981cb"
   },
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rJkFsox0d9w"
   },
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1doSwwLX0HXO"
   },
   "outputs": [],
   "source": [
    "Xa= torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BapqtbYA0Had",
    "outputId": "8c92e4ec-7434-4445-d078-f015a3ecb71e"
   },
   "outputs": [],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Bn_aVE91QVQ"
   },
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x Xa (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b0h510b1QhD"
   },
   "source": [
    "### Projet target data in target subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fDRJgVD70HdQ"
   },
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x X_target_proj.T (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKIvLHQlZ2fi"
   },
   "source": [
    "### Train adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkGVVpieyTok"
   },
   "outputs": [],
   "source": [
    "# it takes time to generate the following matrices so they are saved \n",
    "X_traget_proj = torch.load('your_path')\n",
    "X_source_proj = torch.load('your_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adb5ZCyeyT4c"
   },
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) # transformation matrix\n",
    "print(M.shape)\n",
    "\n",
    "Xa= torch.matmul(X_source_proj.T,M) # target aligned source subspace\n",
    "print(Xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u28K7VNiZ0ad"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights\n",
    "model.load_state_dict(torch.load('your_path/model_25.pt'))\n",
    "\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Tricks to include transformation to subspace in the model\n",
    "model.roi_heads.box_head.add_module('transfo',nn.Linear(in_features=1024, out_features=100, bias=False)) # no bias\n",
    "model.roi_heads.box_head.transfo.weight = nn.Parameter(Xa, requires_grad = False) # we want to keep these weightd (which are Xa) fixed\n",
    "\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(100, 2) # vector are of size 100 after the transformation\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89HbDoy0eCyC",
    "outputId": "958c8a77-9250-442d-fdf8-4f12ae68d723"
   },
   "outputs": [],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "\n",
    "  print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hXVK5eVSODx"
   },
   "outputs": [],
   "source": [
    "train_valid_img,_= get_img_with_bbox(train_ann_path)  \n",
    "training_data= CustomImageDataset(train_ann_path,img_folder, train_valid_img)\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True,collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbT1XlL5RWA4"
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "model.train()\n",
    "\n",
    "all_train_logs=[]\n",
    "all_trans_valid_logs=[]\n",
    "all_cis_valid_logs=[]\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # train for one epoch, printing every 10 iterations\n",
    "  train_logs=train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=100)\n",
    "  all_train_logs.append(train_logs)\n",
    "  # update the learning rate\n",
    "  lr_scheduler.step()\n",
    "  # evaluate on the test dataset\n",
    "\n",
    "  for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "         trans_loss_dict = model(images, targets)\n",
    "         trans_loss_dict= [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "         all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "  for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "         cis_loss_dict = model(images, targets)\n",
    "         cis_loss_dict= [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "         all_cis_valid_logs.append(cis_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJj3hht9da4-"
   },
   "outputs": [],
   "source": [
    "# Before testing the model on TRANS TEST, Xa (weights of model.roi_heads.box_head.transfo), has to be replaced by X_traget_proj.T\n",
    "\n",
    "# Should probably do also for trans valid losses?..\n",
    "\n",
    "model.roi_heads.box_head.transfo.weight = nn.Parameter(X_traget_proj.T, requires_grad = False) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "DnDM9gbwrQNx"
   ],
   "name": "Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
