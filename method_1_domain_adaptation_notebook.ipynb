{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "from PIL import Image\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports local modules downloaded from TorchVision repo v0.8.2, references/detection\n",
    "# https://github.com/pytorch/vision/tree/v0.8.2/references/detection\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from local lib files\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and initiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "# Set the paths to the annotation files that will retrieve the images with the split based on the annotations\n",
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'\n",
    "\n",
    "# Load the json files of the annotations for better exploring of each images\n",
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# Make and horizontal flip data transformation with 50% chance to use as data augmentation in a data loader\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "# Class used to create a custom dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "### Pre-trained models\n",
    "Inspred from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train (CNN layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "# Get a pretrained model and set to train the last layer (CNN : model 1)\n",
    "def get_model_from_pretrained_cnn(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Get a pretrained model and set to train the last 2 layers (ROI + CNN : model 2)\n",
    "def get_model_from_pretrained_roi(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Get a pretrained model and set to train the last 3 layers (RPN + ROI + CNN : model 3)\n",
    "def get_model_from_pretrained_rpn(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model based on a type preference between the 3 proposed\n",
    "def create_model(model_type, num_classes=2, milestones=[5, 10]):\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = num_classes\n",
    "\n",
    "    # get the model from the type we want using our helper function\n",
    "    if model_type==1 or model_type=='cnn':\n",
    "        model = get_model_from_pretrained_cnn(num_classes)\n",
    "    elif model_type==2 or model_type=='roi':\n",
    "        model = get_model_from_pretrained_roi(num_classes)\n",
    "    elif model_type==3 or model_type=='rpn':\n",
    "        model = get_model_from_pretrained_rpn(num_classes)\n",
    "    else:\n",
    "        return 'Please select a valid model. 1:CNN - 2:ROI - 3:RPN'\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an SGD optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "    # like in the paper, construct the scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma=0.1)\n",
    "    \n",
    "    return model, optimizer, lr_scheduler\n",
    "\n",
    "# Save the model, the optimizer and the scheduler into 3 separate files (~165MB)\n",
    "def save_model(file_name = time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    filename = file_name\n",
    "\n",
    "    torch.save(model.state_dict(), 'saved_models/' + filename + '_model.pt')\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/' + filename + '_optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), 'saved_models/' + filename + '_scheduler.pt')\n",
    "\n",
    "def load_model(model_type, model_type_file_name, num_classes=2, milestones=[5, 10]):\n",
    "    model, optimizer, lr_scheduler = create_model(model_type, num_classes, milestones)\n",
    "    \n",
    "    # load the model, the optimizer and the scheduler\n",
    "    model.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_model.pt'))\n",
    "    optimizer.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_scheduler.pt'))\n",
    "    \n",
    "    return model, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(ann_path, batch_size, shuffle=True, transform=None):\n",
    "    images_with_bbox,_ = get_img_with_bbox(ann_path)\n",
    "    data = CustomImageDataset(ann_path, img_folder, images_with_bbox, transform)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 'evaluate' fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: méthode pour évaluer \n",
    "def evaluate(dataloader, coco, nms=True, iou=0.35):\n",
    "    apply_nms = nms\n",
    "    iou_threshold = iou # param to potentially tune (threshold for nms)\n",
    "    the_data_loader = dataloader # change to test set\n",
    "    \n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, targets in the_data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred=model(images)\n",
    "\n",
    "            if apply_nms:\n",
    "                boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "                pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "                pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "                pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "            outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "            res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "            coco_evaluator.update(res)\n",
    "    \n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the smoothed values to a dictionnary of each values\n",
    "def smoothed_value_to_str(smoothed_value):\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the valid logs from list of dictionnaries to string\n",
    "# TODO: add if type == list to not do anything if its already a list\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the cis validation logs into a json file with time dependent file name\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the trans validation logs into a json file with time dependent file name\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train function\n",
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lightweight configuration mode to use subset of data, simpler architecture and few epochs\n",
    "# to quickly test the code for evaluation\n",
    "lightweight_mode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can specify the data augmentation transformation at will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison\n",
    "if lightweight_mode:\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, data_transform)\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10)\n",
    "else:\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, data_transform)\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Loads the test dataset for coco evaluation later on\n",
    "cis_coco = get_coco_api_from_dataset(cis_test_dataloader.dataset)\n",
    "trans_coco = get_coco_api_from_dataset(trans_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler = create_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters before training\n",
    "num_epochs = 10\n",
    "\n",
    "# Check if using the right device before training\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [    0/12099]  eta: 8:27:02  lr: 0.000001  loss: 0.7460 (0.7460)  loss_classifier: 0.6163 (0.6163)  loss_box_reg: 0.0641 (0.0641)  loss_objectness: 0.0624 (0.0624)  loss_rpn_box_reg: 0.0032 (0.0032)  time: 2.5145  data: 0.0280  max mem: 594\n",
      "Epoch: [0]  [  100/12099]  eta: 0:30:18  lr: 0.000031  loss: 0.2891 (0.4846)  loss_classifier: 0.1477 (0.3222)  loss_box_reg: 0.1105 (0.1217)  loss_objectness: 0.0158 (0.0332)  loss_rpn_box_reg: 0.0033 (0.0075)  time: 0.1276  data: 0.0311  max mem: 652\n",
      "Epoch: [0]  [  200/12099]  eta: 0:29:27  lr: 0.000061  loss: 0.2473 (0.3768)  loss_classifier: 0.0959 (0.2154)  loss_box_reg: 0.1223 (0.1175)  loss_objectness: 0.0110 (0.0364)  loss_rpn_box_reg: 0.0039 (0.0075)  time: 0.1458  data: 0.0400  max mem: 652\n",
      "Epoch: [0]  [  300/12099]  eta: 0:28:09  lr: 0.000091  loss: 0.2284 (0.3356)  loss_classifier: 0.0811 (0.1760)  loss_box_reg: 0.1169 (0.1189)  loss_objectness: 0.0046 (0.0330)  loss_rpn_box_reg: 0.0022 (0.0077)  time: 0.1299  data: 0.0331  max mem: 652\n",
      "Epoch: [0]  [  400/12099]  eta: 0:27:18  lr: 0.000120  loss: 0.2041 (0.3113)  loss_classifier: 0.0765 (0.1526)  loss_box_reg: 0.1126 (0.1169)  loss_objectness: 0.0063 (0.0341)  loss_rpn_box_reg: 0.0017 (0.0076)  time: 0.1275  data: 0.0308  max mem: 652\n",
      "Epoch: [0]  [  500/12099]  eta: 0:26:41  lr: 0.000150  loss: 0.2210 (0.2960)  loss_classifier: 0.0688 (0.1379)  loss_box_reg: 0.0974 (0.1146)  loss_objectness: 0.0164 (0.0356)  loss_rpn_box_reg: 0.0043 (0.0078)  time: 0.1335  data: 0.0318  max mem: 652\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26304/318506580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TRAIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mall_train_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_trans_valid_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_cis_valid_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26304/150529973.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, num_epochs, save_logs, save_model, print_freq)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# train for one epoch, printing every 100 images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtrain_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mall_train_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\github\\projects\\Animal-Detection-and-Generalization\\engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\github\\projects\\Animal-Detection-and-Generalization\\utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    207\u001b[0m             ])\n\u001b[0;32m    208\u001b[0m         \u001b[0mMB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1024.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1024.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26304/4099434514.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# if image.shape[0]==1:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# some images have only one channel, we convert them to rgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \"\"\"\n\u001b[0;32m    888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"transparency\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\animals\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_train_logs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26304/151659712.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlast_train_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_train_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlast_trans_valid_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_trans_valid_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlast_cis_valid_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_cis_valid_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_train_logs' is not defined"
     ]
    }
   ],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "n = len(train_logs)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = len(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(n):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_rpn_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # trans\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_rpn_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, n + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, n + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, n + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_rpn_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10 random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "# Loads 10 images and makes the model do predictions on these images\n",
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "# TODO: faire des méthodes pour simplifier le code\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.001 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val_ann['images'][train_labels[3]['image_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 3 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(train_labels[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][1]), \n",
    "                            train_labels[image_i]['boxes'][i][2]-train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][3]-train_labels[image_i]['boxes'][i][1], \n",
    "                            linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[image_i]['boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[image_i]['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on COCO metrics from data loaders\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU ≥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cis test 10 epochs rpn + roi 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "cis_coco_evaluator = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "trans_coco_evaluator = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn + roi 3')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn + roi 3')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler = load_model(3, \"10_rpn_roi_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers \n",
    "\n",
    " 1. https://arxiv.org/pdf/1507.05578.pdf\n",
    "\n",
    " 2.  https://openaccess.thecvf.com/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "# 20 minutes\n",
    "thres_IoU = 0.50\n",
    "count = 0\n",
    "\n",
    "X_source = torch.tensor([])\n",
    "bbox_idx = torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat = outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "\n",
    "    bbox_idx_to_keep = torch.tensor([])\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs = bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101061, 1024])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source, 'saved_matrixes/X_source_05_row_col_10_rpn_roi_4_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # center data\n",
    "# scaler = StandardScaler()\n",
    "# X_source_scaled = scaler.fit_transform(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data per row\n",
    "scaler_row = StandardScaler()\n",
    "X_source_scaled_row = scaler_row.fit_transform(X_source.T)\n",
    "\n",
    "# center data per column\n",
    "scaler_col = StandardScaler()\n",
    "X_source_scaled = scaler_col.fit_transform(X_source_scaled_row.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=512)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1024])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source_proj, 'saved_matrixes/X_source_proj_05_row_col_10_rpn_roi_4_512.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data/distribution = trans test set - Batch Size 1\n",
    "trans_test_batch1_img,_ = get_img_with_bbox(trans_test_ann_path)\n",
    "trans_test_batch1_data = CustomImageDataset(trans_test_ann_path, img_folder, trans_test_batch1_img)\n",
    "trans_test_batch1_dataloader = DataLoader(trans_test_batch1_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n"
     ]
    }
   ],
   "source": [
    "# 30 minutes\n",
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target, 'saved_matrixes/X_target_05_row_col_10_rpn_roi_4_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # center data\n",
    "# scaler = StandardScaler()\n",
    "# X_target_scaled = scaler.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data per row\n",
    "scaler_row = StandardScaler()\n",
    "X_target_scaled_row = scaler_row.fit_transform(X_target.T)\n",
    "\n",
    "# center data per column\n",
    "scaler_col = StandardScaler()\n",
    "X_target_scaled = scaler_col.fit_transform(X_target_scaled_row.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca_proj = PCA(n_components=512)\n",
    "pca_proj.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca_proj.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_proj.explained_variance_ratio_) # we keep d dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target_proj, 'saved_matrixes/X_target_proj_05_row_col_10_rpn_roi_4_512.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "𝑀 is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x Xa (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet target data in target subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x X_target_proj.T (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load because it takes time to generate the following matrices so they are saved\n",
    "X_source_proj = torch.load('saved_matrixes/X_source_proj_05_50_rpn_roi_1_512.pt')\n",
    "X_target_proj = torch.load('saved_matrixes/X_target_proj_05_50_rpn_roi_1_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source_proj.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) # transformation matrix\n",
    "\n",
    "Xa = torch.matmul(X_source_proj.T,M) # target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FastRCNNPredictor_custom(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Standard classification + bounding box regression layers\n",
    "#     for Fast R-CNN.\n",
    "\n",
    "#     Args:\n",
    "#         in_channels (int): number of input channels\n",
    "#         num_classes (int): number of output classes (including background)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, num_classes, m_transfo):\n",
    "#         super(FastRCNNPredictor_custom, self).__init__()\n",
    "#         self.cls_score = nn.Sequential(nn.Linear(in_features = 1024, out_features = 100, bias=False), nn.Linear(in_channels, num_classes))\n",
    "#         self.bbox_pred = nn.Sequential(nn.Linear(in_features = 1024, out_features = 100, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "#         self.cls_score[0].weight= nn.Parameter(m_transfo, requires_grad = False)\n",
    "#         self.bbox_pred[0].weight= nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if x.dim() == 4:\n",
    "#             assert list(x.shape[2:]) == [1, 1]\n",
    "#         x = x.flatten(start_dim=1)\n",
    "#         scores = self.cls_score(x)\n",
    "#         bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "#         return scores, bbox_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False),nn.Linear(in_channels, num_classes))\n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/10_rpn_roi_4_model.pt'))\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "    print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of weights in the optimizer\n",
    "for i in range(len(optimizer.param_groups[0]['params'])):\n",
    "    print(optimizer.param_groups[0]['params'][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 15\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "cis_coco_evaluator_method = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans with method 3\n",
    "model.roi_heads.box_predictor.cls_score[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False) \n",
    "model.roi_heads.box_predictor.bbox_pred[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "trans_coco_evaluator_method = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn roi 3, method3 with 15 epochs & d=512')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn+roi 3, method3 with 15 epochs & d=512')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model with Method 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False),nn.Linear(in_channels, num_classes))\n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
