{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578fe4b4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from local lib files\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad7356",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfe821",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2abcf0",
   "metadata": {},
   "source": [
    "## Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31db297",
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test set length:', len(cis_test_ann['images']))\n",
    "print('cis val set length:', len(cis_val_ann['images']))\n",
    "print('train set length:', len(train_ann['images']))\n",
    "print('trans test set length:', len(trans_test_ann['images']))\n",
    "print('trans val set length:', len(trans_val_ann['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_ann.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e398d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_ann['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488761a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_test_ann['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4efe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val_ann['annotations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val_ann['images'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689394f",
   "metadata": {},
   "source": [
    "## Horizontal flip debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 500\n",
    "\n",
    "boxes = [trans_val_ann['annotations'][j]['bbox'] for j in range(len(trans_val_ann['annotations'])) \n",
    "         if trans_val_ann['annotations'][j]['image_id']==trans_val_ann['images'][i]['id'] \n",
    "         and 'bbox' in trans_val_ann['annotations'][j].keys()]\n",
    "\n",
    "img_path = os.path.join('eccv_18_all_images_sm', trans_val_ann['images'][i]['file_name']) # to change\n",
    "\n",
    "image = read_image(img_path)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image[0].squeeze(),cmap=\"gray\")\n",
    "\n",
    "scale_x = image.shape[2] / trans_val_ann['images'][i]['width'] \n",
    "scale_y = image.shape[1] / trans_val_ann['images'][i]['height']\n",
    "\n",
    "boxes = torch.as_tensor(boxes)\n",
    "\n",
    "for i in range(boxes.shape[0]):\n",
    "    boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "    boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "    boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "    boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "    boxes[i][2] = boxes[i][0] + boxes[i][2]\n",
    "    boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "target = {}\n",
    "target[\"boxes\"] = boxes\n",
    "\n",
    "rect = patches.Rectangle((boxes[0][0], boxes[0][1]), boxes[0][2]-boxes[0][0], \n",
    "                         boxes[0][3]-boxes[0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = Image.open(img_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa230744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8da8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image2.size[0], image2.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('width:', width)\n",
    "print('height:', height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_new = conv(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('image_new.shape:', image_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f62458",
   "metadata": {},
   "source": [
    "## Utils part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from local files (from github repo)\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11229c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None, target_transform=None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "            #boxes[i][0]*=scale_x\n",
    "            #boxes[i][1]*=scale_y\n",
    "            #boxes[i][2]*=scale_x\n",
    "            #boxes[i][3]*=scale_y\n",
    "\n",
    "        label=torch.as_tensor(label)\n",
    "        label=torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6d6a7",
   "metadata": {},
   "source": [
    "## Exemple of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d141254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images bounding boxes *takes about 25sec*\n",
    "train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "cis_val_valid_img,_ = get_img_with_bbox(cis_val_ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_ann_path, img_folder, train_valid_img)\n",
    "valid_data = CustomImageDataset(cis_val_ann_path, img_folder, cis_val_valid_img)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "# In paper : ' We use a batch size of 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72456c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Target (Bbox) batch shape: {train_labels[0]['boxes'].size()}\")\n",
    "print(f\"Target (category) batch shape: {train_labels[0]['labels'].size()}\")\n",
    "\n",
    "img = train_features[0][0].squeeze()\n",
    "label = train_labels[0]['labels']\n",
    "label_categ='animal'\n",
    "\n",
    "if label[0]==0:\n",
    "    label_categ='background'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img,cmap=\"gray\")\n",
    "rect = patches.Rectangle((train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][1]), train_labels[0]['boxes'][0][2]-train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][3]-train_labels[0]['boxes'][0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "print(f\"Label: {label_categ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f70a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = data_transform['train']\n",
    "img2, target2 = trans(image, target)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img2, cmap=\"gray\")\n",
    "\n",
    "rect = patches.Rectangle((target2['boxes'][0][0], target2['boxes'][0][1]), \n",
    "                         target2['boxes'][0][2] - target2['boxes'][0][0], \n",
    "                         target2['boxes'][0][3] - target2['boxes'][0][1], \n",
    "                         linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c50ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3866a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
