{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "from PIL import Image\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports local modules downloaded from TorchVision repo v0.8.2, references/detection\n",
    "# https://github.com/pytorch/vision/tree/v0.8.2/references/detection\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from local lib files\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and initiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "# Set the paths to the annotation files that will retrieve the images with the split based on the annotations\n",
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'\n",
    "\n",
    "# Load the json files of the annotations for better exploring of each images\n",
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# Make and horizontal flip data transformation with 50% chance to use as data augmentation in a data loader\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "### Pre-trained models\n",
    "Inspred from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train (CNN layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "# Get a pretrained model and set to train the last layer (CNN : model 1)\n",
    "def get_model_from_pretrained_cnn(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get a pretrained model and set to train the last 2 layers (ROI + CNN : model 2)\n",
    "def get_model_from_pretrained_roi(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get a pretrained model and set to train the last 3 layers (RPN + ROI + CNN : model 3)\n",
    "def get_model_from_pretrained_rpn(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a model based on a type preference between the 3 proposed\n",
    "def create_model(model_type, num_classes=2, milestones=[5, 10]):\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = num_classes\n",
    "\n",
    "    # get the model from the type we want using our helper function\n",
    "    if model_type==1 or model_type=='cnn':\n",
    "        model = get_model_from_pretrained_cnn(num_classes)\n",
    "    elif model_type==2 or model_type=='roi':\n",
    "        model = get_model_from_pretrained_roi(num_classes)\n",
    "    elif model_type==3 or model_type=='rpn':\n",
    "        model = get_model_from_pretrained_rpn(num_classes)\n",
    "    else:\n",
    "        return 'Please select a valid model. 1:CNN - 2:ROI - 3:RPN'\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an SGD optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "    # like in the paper, construct the scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma=0.1)\n",
    "    \n",
    "    return model, optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "# Save the model, the optimizer and the scheduler into 3 separate files (~165MB)\n",
    "def save_model(file_name = time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    filename = file_name\n",
    "\n",
    "    torch.save(model.state_dict(), 'saved_models/' + filename + '_model.pt')\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/' + filename + '_optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), 'saved_models/' + filename + '_scheduler.pt')\n",
    "    print(\"Succesfully saved!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Load a model, an optimizer and a schduler into 3 different variables\n",
    "def load_model(model_type, model_type_file_name, num_classes=2, milestones=[5, 10]):\n",
    "    model, optimizer, lr_scheduler = create_model(model_type, num_classes, milestones)\n",
    "    \n",
    "    # load the model, the optimizer and the scheduler\n",
    "    model.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_model.pt'))\n",
    "    optimizer.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_scheduler.pt'))\n",
    "    \n",
    "    return model, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full/light dataloader with the full/light dataset\n",
    "def create_dataloader(ann_path, batch_size, transform = None, light=False, shuffle=True):\n",
    "    images_with_bbox,_ = get_img_with_bbox(ann_path)\n",
    "    if light:\n",
    "        index = np.random.choice(range(len(images_with_bbox)), 100)\n",
    "        images_with_bbox = [images_with_bbox[i] for i in index]\n",
    "    data = CustomImageDataset(ann_path, img_folder, images_with_bbox, transform)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 'evaluate' fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the current model using the coco_evaluator passing through a test dataloader\n",
    "def evaluate(dataloader, coco, nms=True, iou=0.35):\n",
    "    apply_nms = nms\n",
    "    iou_threshold = iou # param to potentially tune (threshold for nms)\n",
    "    the_data_loader = dataloader # change to test set\n",
    "    \n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, targets in the_data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred=model(images)\n",
    "\n",
    "            if apply_nms:\n",
    "                boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "                pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "                pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "                pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "            outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "            res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "            coco_evaluator.update(res)\n",
    "    \n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the smoothed values to a dictionnary of each values\n",
    "def smoothed_value_to_str(smoothed_value):\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better\n",
    "    return None\n",
    "\n",
    "\n",
    "# Converts the valid logs from list of dictionnaries to string\n",
    "# TODO: add if type == list to not do anything if its already a list\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the cis validation logs into a json file with time dependent file name\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Puts the trans validation logs into a json file with time dependent file name\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part\n",
    "#### Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the lightweight configuration mode to use subset of data, simpler architecture and few epochs\n",
    "# to quickly test the code for evaluation\n",
    "lightweight_mode = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can specify the data augmentation transformation at will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison\n",
    "if lightweight_mode:\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, light=True)\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10, light=True)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10, light=True)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10, light=True)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10, light=True)\n",
    "else:\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1)\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Loads the test dataset for coco evaluation later on\n",
    "cis_coco = get_coco_api_from_dataset(cis_test_dataloader.dataset)\n",
    "trans_coco = get_coco_api_from_dataset(trans_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the model to create and the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler = create_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters before training\n",
    "num_epochs = 10\n",
    "\n",
    "# Check if using the right device before training\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "n = len(train_logs)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(n):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(cis_valid_dataloader)):\n",
    "        loss_interm += cis_valid_logs[(len(cis_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # trans\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(trans_valid_dataloader)):\n",
    "        loss_interm += trans_valid_logs[(len(trans_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, n + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, n + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, n + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_rpn_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on COCO metrics from data loaders\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU â‰¥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cis test 10 epochs rpn + roi 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "cis_coco_evaluator = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "trans_coco_evaluator = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn + roi 3')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn + roi 3')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10 random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "# Loads 10 images and makes the model do predictions on these images\n",
    "# WARNING: Takes GPU ram space\n",
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.001 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 3 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(train_labels[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][1]), \n",
    "                            train_labels[image_i]['boxes'][i][2]-train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][3]-train_labels[image_i]['boxes'][i][1], \n",
    "                            linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 (Subspace alignment based Domain adaptation)\n",
    "### Using the same modeel that we just trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can just load a model by uncommenting the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE FOLLOWING LINE:\n",
    "model, optimizer, lr_scheduler = load_model(3, \"10_rpn_roi_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers \n",
    "\n",
    " 1. https://arxiv.org/pdf/1507.05578.pdf\n",
    "\n",
    " 2.  https://openaccess.thecvf.com/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "thres_IoU = 0.50\n",
    "count = 0\n",
    "\n",
    "X_source = torch.tensor([])\n",
    "bbox_idx = torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat = outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "\n",
    "    bbox_idx_to_keep = torch.tensor([])\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs = bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101061, 1024])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source, 'saved_matrixes/X_source_05_row_col_10_rpn_roi_4_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_source_scaled = scaler.fit_transform(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # center data per row\n",
    "# scaler_row = StandardScaler()\n",
    "# X_source_scaled_row = scaler_row.fit_transform(X_source.T)\n",
    "\n",
    "# # center data per column\n",
    "# scaler_col = StandardScaler()\n",
    "# X_source_scaled = scaler_col.fit_transform(X_source_scaled_row.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source_proj, 'saved_matrixes/X_source_proj_05_row_col_10_rpn_roi_4_100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data/distribution = trans test set - Batch Size 1\n",
    "trans_test_batch1_img,_ = get_img_with_bbox(trans_test_ann_path)\n",
    "trans_test_batch1_data = CustomImageDataset(trans_test_ann_path, img_folder, trans_test_batch1_img)\n",
    "trans_test_batch1_dataloader = DataLoader(trans_test_batch1_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "# 30 minutes\n",
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37508, 1024])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target, 'saved_matrixes/X_target_05_row_col_10_rpn_roi_4_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_target_scaled = scaler.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # center data per row\n",
    "# scaler_row = StandardScaler()\n",
    "# X_target_scaled_row = scaler_row.fit_transform(X_target.T)\n",
    "\n",
    "# # center data per column\n",
    "# scaler_col = StandardScaler()\n",
    "# X_target_scaled = scaler_col.fit_transform(X_target_scaled_row.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca_proj = PCA(n_components=100)\n",
    "pca_proj.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca_proj.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAghElEQVR4nO3dfZAc9X3n8fd3nvZRq9XjSloJSYAMlpVgwyLJGJyNiWOJOFbukjiQOCSuOCouELDjnA8nlfLdVbnuzsU5hiqCItv44kfOxviiIorBxmwcYxASD5YRy8MiHrRIQk8gaVa7Ozsz3/uje8VotattrWY12u7Pq2prZ7p/v5nfVw+f7vl1T7e5OyIiEl+pWg9AREQml4JeRCTmFPQiIjGnoBcRiTkFvYhIzGVqPYDRzJ4925csWTKhvn19fTQ1NVV3QOe4JNYMyaw7iTVDMus+3ZqfeOKJA+4+Z7R152TQL1myhG3btk2ob1dXF52dndUd0DkuiTVDMutOYs2QzLpPt2Yze3WsdZq6ERGJOQW9iEjMKehFRGJOQS8iEnORgt7M1pjZ82bWY2a3jrL+YjN71MwGzeyvK5YvMrOHzazbzHaY2S3VHLyIiIxv3LNuzCwN3Al8EOgFtprZJnd/tqLZIeBm4HdGdC8Cn3b3J81sGvCEmf1oRF8REZlEUfboVwI97r7T3QvAPcC6ygbuvs/dtwJDI5bvcfcnw8dHgW6gvSojFxGRSKIEfTuwq+J5LxMIazNbArwH2HK6faNwd+546EV+ub84GS8vIjJlRfnClI2y7LQuYm9mzcD3gU+6+5Ex2qwH1gO0tbXR1dV1Om8BwF0P97Fyjk+o71SWz+cTVzMks+4k1gzJrLuaNUcJ+l5gUcXzhcDuqG9gZlmCkP+Wu983Vjt33whsBOjo6PCJfAtu3rYu+n1A36BLiCTWncSaIZl1V7PmKFM3W4FlZrbUzHLAtcCmKC9uZgZ8Feh29y9OfJjRzGrKcaSgO2aJiFQad4/e3YtmdhPwAJAG7nb3HWZ2Q7h+g5nNA7YBLUDZzD4JLAd+Ffhj4Jdm9nT4kn/j7purXgkwu7mO3QcV9CIilSJd1CwM5s0jlm2oeLyXYEpnpJ8x+hz/pJjVnOPIoIJeRKRSrL4ZO7u5jvwQFEvlWg9FROScEbOgzwFwqK9Q45GIiJw7Yhb0dQAcyCvoRUSGxSroZx0P+sEaj0RE5NwRq6Afnro52KegFxEZFqugP75Hf1RTNyIiw2IV9C31GTIGB7RHLyJyXKyC3sxoqTMO6mCsiMhxsQp6gGk508FYEZEKsQv66Tnt0YuIVIpd0GuPXkTkRLEL+unhHL27rnkjIgIxDPppOaNQKnNkQHeaEhGBGAb99LrgYpkHNX0jIgLEMOhbgi/H6no3IiKhGAa99uhFRCrFNuh15o2ISCB2QT/teNBr6kZEBGIY9OmUMaMxqz16EZFQ7IIeghuQ6NuxIiKBWAb9rOac9uhFREIxDfo6Duq+sSIiQEyDfk5zHQeOao9eRARiGvSzmnIcHSwyMFSq9VBERGoulkE/e1pwS0FN34iIRAx6M1tjZs+bWY+Z3TrK+ovN7FEzGzSzvz6dvpNhVlN4k3AdkBURGT/ozSwN3AmsBZYD15nZ8hHNDgE3A7dNoG/VDe/R68wbEZFoe/QrgR533+nuBeAeYF1lA3ff5+5bgaHT7TsZZjcNB72mbkREMhHatAO7Kp73Aqsivn7kvma2HlgP0NbWRldXV8S3OFE+n6f7qS0AbN3ezdz8SxN6nakkn89P+M9rKkti3UmsGZJZdzVrjhL0NsqyqLdvitzX3TcCGwE6Ojq8s7Mz4lucqKuri87OThr+7Ye0zF1IZ+ekzxTV3HDNSZPEupNYMySz7mrWHGXqphdYVPF8IbA74uufSd8zMrMpx5s660ZEJFLQbwWWmdlSM8sB1wKbIr7+mfQ9I425NMcKOo9eRGTcqRt3L5rZTcADQBq42913mNkN4foNZjYP2Aa0AGUz+ySw3N2PjNZ3kmo5QWMuzTF9YUpEJNIcPe6+Gdg8YtmGisd7CaZlIvU9GxpzGfoLukG4iEgsvxkLwR5936D26EVE4hv0dRn6NXUjIhLjoM+m6RvU1I2ISGyDviGXpl9n3YiIxDfom+rS9BWKuEf9bpeISDzFNugbcxnKDoPFcq2HIiJSUzEO+jSAvjQlIomXgKDXAVkRSbYYB33wXTAdkBWRpItx0Ad79H0KehFJuBgHfbBHr6kbEUm6GAd9OEevyyCISMLFP+h1GQQRSbj4Bn1dOHWjyyCISMLFN+izOo9eRATiHPR1QdDrCpYiknSxDfpcOkU6ZbqCpYgkXmyD3sx031gREWIc9DB8g3Dt0YtIssU86DPaoxeRxIt50GvqRkQkAUGvqRsRSbaYB31GV68UkcSLedCndfVKEUm8SEFvZmvM7Hkz6zGzW0dZb2Z2R7h+u5ldWrHuU2a2w8yeMbPvmFl9NQs4Fe3Ri4hECHozSwN3AmuB5cB1ZrZ8RLO1wLLwZz1wV9i3HbgZ6HD3FUAauLZqox9HsEevOXoRSbYoe/QrgR533+nuBeAeYN2INuuAr3vgMaDVzOaH6zJAg5llgEZgd5XGPi6ddSMiEoTweNqBXRXPe4FVEdq0u/s2M7sNeA3oBx509wdHexMzW0/waYC2tja6uroiFTBSPp8/3nff7gKFYpmHfvIw6ZRN6PWmgsqakySJdSexZkhm3dWsOUrQj5aQHqWNmc0g2NtfCrwFfM/MPubu3zypsftGYCNAR0eHd3Z2Rhjaybq6uhju+2JqJz/o6ebyK66kpT47odebCiprTpIk1p3EmiGZdVez5ihTN73AoornCzl5+mWsNr8BvOzu+919CLgPuGLiwz09x69gqekbEUmwKEG/FVhmZkvNLEdwMHXTiDabgOvDs29WA4fdfQ/BlM1qM2s0MwOuBrqrOP5TOn6DcF3BUkQSbNypG3cvmtlNwAMEZ83c7e47zOyGcP0GYDNwDdADHAM+Hq7bYmb3Ak8CReApwumZs+HtG4Rrj15EkivKHD3uvpkgzCuXbah47MCNY/T9HPC5MxjjhB2/b6yCXkQSLPbfjAV0vRsRSbSYB72mbkREYh70mroREYl50A/v0WvqRkSSK+ZBrz16EZFYB31DVkEvIhLroE+ljIZsmmP6wpSIJFisgx6gqS7NsSHt0YtIcsU+6Bty2qMXkWSLfdA3ZjOaoxeRRIt/0Nfp5iMikmzxD/pcWufRi0iiJSDoNXUjIsmWgKDX1I2IJFsCgl579CKSbAkIes3Ri0iyJSLo+4dKlMsj72cuIpIMCQj6DO4wUNT0jYgkUwKCXhc2E5FkS0zQ9yvoRSShEhD0wc1H+nRAVkQSKv5BX6epGxFJtvgH/fDNRwYV9CKSTPEPet03VkQSLlLQm9kaM3vezHrM7NZR1puZ3RGu325ml1asazWze83sOTPrNrP3VrOA8WjqRkSSbtygN7M0cCewFlgOXGdmy0c0WwssC3/WA3dVrLsd+KG7XwxcAnRXYdyR6fRKEUm6KHv0K4Eed9/p7gXgHmDdiDbrgK974DGg1czmm1kL8H7gqwDuXnD3t6o3/PFp6kZEki5K0LcDuyqe94bLorQ5H9gPfM3MnjKzr5hZ0xmM97Rpj15Eki4ToY2NsmzkhWPGapMBLgX+0t23mNntwK3A3530JmbrCaZ9aGtro6urK8LQTpbP50/qmzF4rudlutKvT+g1z3Wj1ZwESaw7iTVDMuuuZs1Rgr4XWFTxfCGwO2IbB3rdfUu4/F6CoD+Ju28ENgJ0dHR4Z2dnhKGdrKuri5F9m/7tQWbPW0Bn54oJvea5brSakyCJdSexZkhm3dWsOcrUzVZgmZktNbMccC2waUSbTcD14dk3q4HD7r7H3fcCu8zsorDd1cCzVRn5aWjSzUdEJMHG3aN396KZ3QQ8AKSBu919h5ndEK7fAGwGrgF6gGPAxyte4i+Bb4UbiZ0j1p0VDbomvYgkWJSpG9x9M0GYVy7bUPHYgRvH6Ps00DHxIZ65mU05DuQLtRyCiEjNxP6bsQCLZjay69CxWg9DRKQmEhH0i2c2sffIAANDmqcXkeRJRNCfN6sBd+h9s7/WQxEROeuSEfQzg+9ovXaor8YjERE5+xIS9I0AvHZQ8/QikjyJCPrZzTkac2le1QFZEUmgRAS9mXHezEbt0YtIIiUi6CGYvnlNe/QikkCJC/pyeeT12ERE4i0xQb94ViODxTL784O1HoqIyFmVmKBfFJ5586rm6UUkYRIT9ItnDZ9Lr6AXkWRJTNC3tzaQMnjtoL40JSLJkpigz2VSzJ/eoD16EUmcxAQ9BAdk9aUpEUmaRAX9ebpcsYgkULKCflYjB/IF8oO625SIJEeygj48xVJ79SKSJIkK+sXh5Yp1Lr2IJEmigl579CKSRIkK+umNWaY3ZHlVNyARkQRJVNBDsFevqRsRSZLEBf27FrTwi11v6SqWIpIYiQv6lUtncmSgyHN7j9Z6KCIiZ0Xign7V+bMAePzlgzUeiYjI2REp6M1sjZk9b2Y9ZnbrKOvNzO4I1283s0tHrE+b2VNmdn+1Bj5R7a0NtLc2sOXlQ7UeiojIWTFu0JtZGrgTWAssB64zs+Ujmq0FloU/64G7Rqy/Beg+49FWyarzZ/L4y4dw1zy9iMRflD36lUCPu+909wJwD7BuRJt1wNc98BjQambzAcxsIfBbwFeqOO4zsmrpTA72FXhpf77WQxERmXSZCG3agV0Vz3uBVRHatAN7gC8BnwGmnepNzGw9wacB2tra6OrqijC0k+Xz+fH79pUB+MYPH+PXz8tO6H3OJZFqjqEk1p3EmiGZdVez5ihBb6MsGznnMWobM/swsM/dnzCzzlO9ibtvBDYCdHR0eGfnKZuPqauri/H6ujv/++mHeDM7i87O90zofc4lUWqOoyTWncSaIZl1V7PmKFM3vcCiiucLgd0R27wP+IiZvUIw5fMBM/vmhEdbJWbGqvNnaZ5eRBIhStBvBZaZ2VIzywHXAptGtNkEXB+efbMaOOzue9z9s+6+0N2XhP1+4u4fq2YBE7Vy6Uz2HhnQHadEJPbGnbpx96KZ3QQ8AKSBu919h5ndEK7fAGwGrgF6gGPAxydvyNWxeulMALa8fOj4jcNFROIoyhw97r6ZIMwrl22oeOzAjeO8RhfQddojnCQXzm1mZlOOx18+xEc7Fo3fQURkikrcN2OHmRnvPX8WD+7Yy97DA7UejojIpEls0AP81W++g6GS8+nvPa2LnIlIbCU66C+Y08znfns5j/Qc5Mv/vrPWwxERmRSJDnqAP7h8EWtXzOO2B5/nmdcP13o4IiJVl/igNzP+x3/8FWY31/Gp/6spHBGJn8QHPUBrY46/ueadvLgvz0+e21fr4YiIVJWCPrR2xTwWTK/n7kdervVQRESqSkEfyqRTXH/FEn7+0kGe3X2k1sMREakaBX2F6y4/j4ZsWnv1IhIrCvoK0xuz/N5lC9n09G72Hx2s9XBERKpCQT/Cx9+3hEKpzDcfe7XWQxERqQoF/Qjnz2nm6ovn8q0trzJUKtd6OCIiZ0xBP4o/uHwRB/IFfv7SwVoPRUTkjCnoR/H+d8xhWl2G+38x8v4qIiJTj4J+FPXZNB9c3sYDO/ZSKGr6RkSmNgX9GD58yXyODBT5Wc/+Wg9FROSMKOjHcOWFc2ipz3D/L/bUeigiImdEQT+GXCbFh941jx89+wYDQ6VaD0dEZMIU9Kfw4UsWcHSwyE9f0PSNiExdCvpTuOKCWcxozHL/dk3fiMjUpaA/hWw6xZoV8/hx9xts732r1sMREZkQBf041r//AmY05vi9ux7l21tew103JhGRqUVBP46ls5u4/y+vZPUFs/ibH/yS/3zvdvKDxVoPS0QkMgV9BDOacnztTy/n5quX8f0ne/nQ3/9UB2hFZMqIFPRmtsbMnjezHjO7dZT1ZmZ3hOu3m9ml4fJFZvawmXWb2Q4zu6XaBZwt6ZTxVx98B/fecAX12RTX3/04n7n3Fzr1UkTOeeMGvZmlgTuBtcBy4DozWz6i2VpgWfizHrgrXF4EPu3u7wRWAzeO0ndKuWzxDP7l5qv4T50X8N1tvfztD57RvL2InNMyEdqsBHrcfSeAmd0DrAOerWizDvi6B4n3mJm1mtl8d98D7AFw96Nm1g20j+g75dRn0/yXNReTS6e4/aEXeef8aXziqvNrPSwRkVFFCfp2YFfF815gVYQ27YQhD2BmS4D3AFtGexMzW0/waYC2tja6uroiDO1k+Xx+wn1P1yUZ57K2NJ//l2769+7kV+ZE+eOsvrNZ87kkiXUnsWZIZt3VrDlKMtkoy0bOVZyyjZk1A98HPunuo9552903AhsBOjo6vLOzM8LQTtbV1cVE+07EqiuK/O5dP+fLO/r5xFWL+bV3zOFX2qeTSo32RzI5znbN54ok1p3EmiGZdVez5igHY3uBRRXPFwIjL9Q+ZhszyxKE/Lfc/b6JD/Xc1FSX4cvXd3Dh3Gb+/scvsO7OR+j4/I/57tZd43cWETkLouzRbwWWmdlS4HXgWuAPR7TZBNwUzt+vAg67+x4zM+CrQLe7f7GK4z6nLJrZyH1/8T4O5gf5Wc8Bvr3lNT7z/e1sfeUQ/33dChpy6VoPUUQSbNw9encvAjcBDwDdwHfdfYeZ3WBmN4TNNgM7gR7gy8BfhMvfB/wx8AEzezr8uabaRZwrZjXXse7d7Xz7z1dz8wcu5HtP9PIf/uERdh06VuuhiUiCRTp66O6bCcK8ctmGiscO3DhKv58x+vx9rKVTxl/95kVcungGt9zzNB/9x0f59p+vZunsploPTUQSSN+MnUSdF83lO3++msFimT/4x0fp2Zev9ZBEJIEU9JNs+YIW7lm/mrLDtRsf5WcvHtAXrETkrFLQnwXvaJvGPetXk0un+NhXt7D29n/nu1t3MVjU5RNEZPIp6M+SC+c285O/7uQLv/urAHzm+9tZ+6V/Z+srh2o8MhGJOwX9WVSfTfPRyxfxr7dcxdf+9HIKpTIf/cdH+dw/P6NLH4vIpFHQ14CZ8esXz+WBT76fP3nvEr7+2Kv82hce5ss/3Ul/QdM5IlJdCvoaaqrL8F8/8i5+8BfvY/mCFj6/uZurvvAwdzz0Is/vPaqDtiJSFbW5Cpec4N2LWvnGn63i8ZcPcftDL/DFHwU/i2c18vuXLWT9+y8gl9E2WUQmRkF/Dlm5dCbf+sRq3jgywI+73+CHz+zltgdf4P7te7jt9y9hRfv0Wg9RRKYgBf05qK2lnj9atZg/WrWYh7rf4LP3/ZJ1dz7CuksWMGdaHU11GZbMbuKaFfPIpLWnLyKnpqA/x139zjZ+9KmZfH7zszzUvY/8YJHBYhmAO+Y285kPXURWc/kicgoK+ilgemOWL/zeJcefD5XK/OS5ffyvHz7H+m88wYWtKTIL93PlhbMJLhgqIvI2Bf0UlE2n+NC75nH1xXP57rZebvvXZ/jjrz7Ouxe18omrltLWUk99Jk1TXZrzZjZqekck4RT0U1gmneIPV53HnL6X2Nd0Pv/w8Evc9O2nTmiTy6S4qG0aK9pb+NC75nHlhbMV/CIJo6CPgWzK+KNVi/n9yxbxy9cPc6xQZGCozFvHCrzwxlG69xzl/u17+M7ju5jdXMdvXzKf8+c009qQpbUxS1tLPfOn1zOtPlvrUkRkEijoYySXSXHZ4hmjrhsslnj4uf3c92Qv33zsVYZKJx/Aba7LsGR2IxfMaebCOc1cungGly2eQX1Wd8gSmcoU9AlRl0mzZsU81qyYR6EY7O2/1T/Eob4CbxwZYO/hAXa/1c/OA31se+VN/vnp4LbA9dkUK5fOYsWCFha0NrCgtZ6FMxo5b2ajNgAiU4SCPoFymRRzW+qZ21I/ZpujA0NsfeUQP33hAI/0HODnPQcolk/8FDCvpZ4V7S1c/c42rr547ilfT0RqR0Evo5pWn+UDF7fxgYvbACiVnQP5QXrf7Kf3zWO8evAYrxzo4/FXDvHj7n0ALJ3dxNxpdcyZVsesphzTG7K0NGRpqc/SkAvOAprekOXCOdOY3qjjASJni4JeIkmnjLaWetpa6k84DuDuvPBGnh93v8GO3Yc5cLTAjt1HOJAfJD9YZKzvcs1rqefCuc20tdQzZ1odc6fVsaC1nvbWRua31tNSn9X1fUSqREEvZ8TMuGjeNC6aN+2kdaWykx8ocmRgiP6hEscKJQ71DfLiG3mef+MoL+3vY+dLB9ifHxz14HAunaKpLk1bSz3trQ0saG2graWO2c3Bp4bdR8v0DRZpqtM/Y5FT0f8QmTTplDG9MXvSNM3wdNAwd+fNY0Psfquf3jf72Xu4n/xgkfxgifzgEHsPD/L6W/1se/VNDvcPndD37x55gNnNdbQ0ZMikjHQqRS6ToiGboj6bpqkuQ0t9lpaGDNMbsrQ2BFNKMxqzzJkWbDRaG7P6RrHEmoJeas7MmNmUY2ZTbtwrdA4MlTiQH2T/0UEefOQJmuct4bWDx+grFCmWnGK5TKHkDBRKHOor8NqhYxzpL3Kkf4hCqTzqa6YsuDdAc/jT0pBlekOWafUZptVnaK7L0pRLk04baTPSKSOTMjLpFNl0sHHJpo1sOhVuWDJMq8+STRuGYRYcAG/IpWnMpvWFNTnrFPQypdRn0yyc0cjCGY0c3pmhs/PCyH37CyUO9w9xODytdHiDcaivEH6CKHJ0YIijA0X2Hx2kZ1+evsEiRweKY24kJiKXTlGfTdGYy5DLpMKNhpEJP43UZYY/jaRpymVozKWpy6bJpVPs7i3wnL1ELp0im0mFGx5Ip4J+dZkUddk0Tbk0jWHfdCrYOKXMSBmkwsdpM1Kp4JNXLp3SBijGIgW9ma0BbgfSwFfc/X+OWG/h+muAY8CfuvuTUfqKnC0NuTQNuTTzpp/+aaCFYpmyO6WyU3IPPj2UygyVg9/FsjNUCo4ZHOkPjksUS44TTE0NFsv0F4LjFP1DJQaGShwrFBkqBf1KZadQLFMolRkslnnzWIHX3yrRN1jkWKFEoVhmoFgKDm6/+FzV/2ygIvBTBkawMUgZ2YqNULCBCT69ZMINyNsbkbcfp8MNSCaVIpM2sqkU6bQd7zO80TGraG/B69ZlU8c3ZNl08Pi5PUXy23cff49gwxRsrIbHeuJrcvx9LHycNsPC5wbH11X+rtwYBhtCO6FOI3j94DdTZspv3KA3szRwJ/BBoBfYamab3P3ZimZrgWXhzyrgLmBVxL4i57xz4Qwgd+ehh7u44sqrGCo6g6US5TKU3CmVnEKpxMBQmcFiib7BYEPSN1ii5I67UypD+fhjp+wc33gVisEGZmBouH3wfiV3horBxmio7BSKpRM2TsWyMzQUbATL4UawVOb442KpzFA4pVYqE/wON4Bl97BfWEN5nMtt/+KpU6+vkVTFxsLs7Q1DNp0KNzZvb0SGVW58CDc8ZsbMxhzfveG9VR9jlD36lUCPu+8MBmj3AOuAyrBeB3zdg5ucPmZmrWY2H1gSoa+IRDAcIo25DOQA4vVdBHdnqOQUSmUKxXKwcQkfP7blcS6//PK3P02V397YDG+U3t54BBua4cel8PHw7+C9hjd6BP3KjvN2e/fgPYb7FMt+fAMV9Hn7fcvh8+ENZ7nsJ2zchjesw32Hay35230Il0+rn5zZ9Civ2g7sqnjeS7DXPl6b9oh9RUQwM3IZCz491Z247rXmFMvaTj6FV6KJEvSjTUKN/Iw1VpsofYMXMFsPrAdoa2ujq6srwtBOls/nJ9x3qkpizZDMupNYMySz7mrWHCXoe4FFFc8XArsjtslF6AuAu28ENgJ0dHR4Z2dnhKGdrKuri4n2naqSWDMks+4k1gzJrLuaNUc5wrQVWGZmS80sB1wLbBrRZhNwvQVWA4fdfU/EviIiMonG3aN396KZ3QQ8QHCK5N3uvsPMbgjXbwA2E5xa2UNweuXHT9V3UioREZFRRTrE6+6bCcK8ctmGiscO3Bi1r4iInD21PzlYREQmlYJeRCTmFPQiIjFnPtadIWrIzPYDr06w+2zgQBWHMxUksWZIZt1JrBmSWffp1rzY3eeMtuKcDPozYWbb3L2j1uM4m5JYMySz7iTWDMmsu5o1a+pGRCTmFPQiIjEXx6DfWOsB1EASa4Zk1p3EmiGZdVet5tjN0YuIyIniuEcvIiIVFPQiIjEXm6A3szVm9ryZ9ZjZrbUez2Qxs0Vm9rCZdZvZDjO7JVw+08x+ZGYvhr9n1Hqs1WZmaTN7yszuD58noeZWM7vXzJ4L/87fG/e6zexT4b/tZ8zsO2ZWH8eazexuM9tnZs9ULBuzTjP7bJhvz5vZh07nvWIR9BX3pl0LLAeuM7PltR3VpCkCn3b3dwKrgRvDWm8FHnL3ZcBD4fO4uQXorniehJpvB37o7hcDlxDUH9u6zawduBnocPcVBFe9vZZ41vx/gDUjlo1aZ/h//FrgXWGffwhzL5JYBD0V97V19wIwfG/a2HH3Pe7+ZPj4KMF//HaCev8pbPZPwO/UZICTxMwWAr8FfKVicdxrbgHeD3wVwN0L7v4WMa+b4Kq6DWaWARoJblYUu5rd/afAoRGLx6pzHXCPuw+6+8sEl4RfGfW94hL0Y92zNtbMbAnwHmAL0Bbe7IXw99waDm0yfAn4DFCuWBb3ms8H9gNfC6esvmJmTcS4bnd/HbgNeA3YQ3AToweJcc0jjFXnGWVcXII+8r1p48LMmoHvA5909yO1Hs9kMrMPA/vc/Ylaj+UsywCXAne5+3uAPuIxZTGmcE56HbAUWAA0mdnHajuqc8IZZVxcgj7KfW1jw8yyBCH/LXe/L1z8hpnND9fPB/bVanyT4H3AR8zsFYJpuQ+Y2TeJd80Q/Lvudfct4fN7CYI/znX/BvCyu+939yHgPuAK4l1zpbHqPKOMi0vQJ+betGZmBHO23e7+xYpVm4A/CR//CfDPZ3tsk8XdP+vuC919CcHf7U/c/WPEuGYAd98L7DKzi8JFVwPPEu+6XwNWm1lj+G/9aoLjUHGuudJYdW4CrjWzOjNbCiwDHo/8qu4eix+Ce9a+ALwE/G2txzOJdV5J8JFtO/B0+HMNMIvgKP2L4e+ZtR7rJNXfCdwfPo59zcC7gW3h3/f/A2bEvW7gvwHPAc8A3wDq4lgz8B2C4xBDBHvsf3aqOoG/DfPteWDt6byXLoEgIhJzcZm6ERGRMSjoRURiTkEvIhJzCnoRkZhT0IuIxJyCXkQk5hT0IiIx9/8B1AZ0CB2J6vcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca_proj.explained_variance_ratio_) # we keep d dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target_proj, 'saved_matrixes/X_target_proj_05_row_col_10_rpn_roi_4_100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "ð‘€ is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 100])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x Xa (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet target data in target subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x X_target_proj.T (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load because it takes time to generate the following matrices so they are saved\n",
    "X_source_proj = torch.load('saved_matrixes/X_source_proj_05_50_rpn_roi_1_512.pt')\n",
    "X_target_proj = torch.load('saved_matrixes/X_target_proj_05_50_rpn_roi_1_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source_proj.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) # transformation matrix\n",
    "\n",
    "Xa = torch.matmul(X_source_proj.T,M) # target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4434,  0.7386,  0.1757,  ..., -0.0093,  0.0029, -0.0125],\n",
       "        [-0.6475,  0.4753,  0.0199,  ...,  0.0033,  0.0087,  0.0044],\n",
       "        [ 0.3298,  0.1828,  0.0121,  ...,  0.0080,  0.0031,  0.0091],\n",
       "        ...,\n",
       "        [ 0.0103, -0.0269, -0.0094,  ..., -0.0367,  0.0435,  0.1514],\n",
       "        [-0.0013, -0.0078, -0.0040,  ...,  0.0923,  0.0439, -0.0597],\n",
       "        [-0.0073,  0.0163, -0.0086,  ..., -0.0063,  0.0310, -0.1176]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0180,  0.0381, -0.0097,  ...,  0.0193,  0.0241, -0.0154],\n",
       "        [ 0.0180,  0.0624, -0.0041,  ...,  0.0064,  0.0122,  0.0018],\n",
       "        [ 0.0014, -0.0437, -0.0273,  ...,  0.0209,  0.0036,  0.0040],\n",
       "        ...,\n",
       "        [-0.0136,  0.0341, -0.0119,  ...,  0.0421, -0.0318,  0.0345],\n",
       "        [ 0.0614,  0.0147, -0.0270,  ...,  0.0100,  0.0297,  0.0152],\n",
       "        [-0.0150,  0.0615,  0.0033,  ..., -0.0001, -0.0055, -0.0076]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        \n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes))\n",
    "        \n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes * 4))\n",
    "        \n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained_rpn(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/10_rpn_roi_4_model.pt'))\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n",
      "torch.Size([2])\n",
      "torch.Size([8, 100])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "    print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n",
      "torch.Size([2])\n",
      "torch.Size([8, 100])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Nb of weights in the optimizer\n",
    "for i in range(len(optimizer.param_groups[0]['params'])):\n",
    "    print(optimizer.param_groups[0]['params'][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 15\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [    0/12099]  eta: 0:29:44  lr: 0.000001  loss: 0.8235 (0.8235)  loss_classifier: 0.6764 (0.6764)  loss_box_reg: 0.1393 (0.1393)  loss_objectness: 0.0027 (0.0027)  loss_rpn_box_reg: 0.0051 (0.0051)  time: 0.1475  data: 0.0410  max mem: 3841\n",
      "Epoch: [0]  [  100/12099]  eta: 0:25:24  lr: 0.000031  loss: 0.5246 (0.7756)  loss_classifier: 0.3416 (0.5901)  loss_box_reg: 0.1545 (0.1743)  loss_objectness: 0.0033 (0.0071)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1191  data: 0.0345  max mem: 3841\n",
      "Epoch: [0]  [  200/12099]  eta: 0:24:45  lr: 0.000061  loss: 0.3096 (0.5830)  loss_classifier: 0.1489 (0.3992)  loss_box_reg: 0.1550 (0.1727)  loss_objectness: 0.0031 (0.0071)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1213  data: 0.0339  max mem: 3841\n",
      "Epoch: [0]  [  300/12099]  eta: 0:24:37  lr: 0.000091  loss: 0.2548 (0.4900)  loss_classifier: 0.1138 (0.3096)  loss_box_reg: 0.1372 (0.1682)  loss_objectness: 0.0030 (0.0080)  loss_rpn_box_reg: 0.0023 (0.0041)  time: 0.1229  data: 0.0346  max mem: 3841\n",
      "Epoch: [0]  [  400/12099]  eta: 0:24:07  lr: 0.000120  loss: 0.2963 (0.4427)  loss_classifier: 0.1130 (0.2614)  loss_box_reg: 0.1598 (0.1695)  loss_objectness: 0.0042 (0.0078)  loss_rpn_box_reg: 0.0021 (0.0041)  time: 0.1192  data: 0.0338  max mem: 3841\n",
      "Epoch: [0]  [  500/12099]  eta: 0:23:38  lr: 0.000150  loss: 0.2524 (0.4147)  loss_classifier: 0.1005 (0.2322)  loss_box_reg: 0.1406 (0.1700)  loss_objectness: 0.0071 (0.0084)  loss_rpn_box_reg: 0.0023 (0.0041)  time: 0.1161  data: 0.0313  max mem: 3841\n",
      "Epoch: [0]  [  600/12099]  eta: 0:23:19  lr: 0.000180  loss: 0.2472 (0.3897)  loss_classifier: 0.0932 (0.2101)  loss_box_reg: 0.1492 (0.1671)  loss_objectness: 0.0072 (0.0085)  loss_rpn_box_reg: 0.0023 (0.0040)  time: 0.1208  data: 0.0337  max mem: 3841\n",
      "Epoch: [0]  [  700/12099]  eta: 0:23:07  lr: 0.000210  loss: 0.2382 (0.3691)  loss_classifier: 0.0814 (0.1928)  loss_box_reg: 0.1420 (0.1637)  loss_objectness: 0.0025 (0.0085)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1179  data: 0.0322  max mem: 3841\n",
      "Epoch: [0]  [  800/12099]  eta: 0:22:50  lr: 0.000240  loss: 0.2440 (0.3545)  loss_classifier: 0.0925 (0.1802)  loss_box_reg: 0.1443 (0.1619)  loss_objectness: 0.0044 (0.0083)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1216  data: 0.0341  max mem: 3841\n",
      "Epoch: [0]  [  900/12099]  eta: 0:22:35  lr: 0.000270  loss: 0.2182 (0.3423)  loss_classifier: 0.0769 (0.1691)  loss_box_reg: 0.1354 (0.1601)  loss_objectness: 0.0034 (0.0090)  loss_rpn_box_reg: 0.0023 (0.0041)  time: 0.1178  data: 0.0323  max mem: 3841\n",
      "Epoch: [0]  [ 1000/12099]  eta: 0:22:19  lr: 0.000300  loss: 0.1871 (0.3304)  loss_classifier: 0.0541 (0.1593)  loss_box_reg: 0.1161 (0.1583)  loss_objectness: 0.0020 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1185  data: 0.0339  max mem: 3841\n",
      "Epoch: [0]  [ 1100/12099]  eta: 0:22:05  lr: 0.000300  loss: 0.2144 (0.3201)  loss_classifier: 0.0632 (0.1517)  loss_box_reg: 0.1265 (0.1556)  loss_objectness: 0.0041 (0.0088)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1212  data: 0.0342  max mem: 3841\n",
      "Epoch: [0]  [ 1200/12099]  eta: 0:21:53  lr: 0.000300  loss: 0.2021 (0.3110)  loss_classifier: 0.0703 (0.1448)  loss_box_reg: 0.1206 (0.1536)  loss_objectness: 0.0021 (0.0087)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1264  data: 0.0348  max mem: 3841\n",
      "Epoch: [0]  [ 1300/12099]  eta: 0:21:51  lr: 0.000300  loss: 0.1721 (0.3026)  loss_classifier: 0.0592 (0.1386)  loss_box_reg: 0.1013 (0.1515)  loss_objectness: 0.0045 (0.0085)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1256  data: 0.0354  max mem: 3841\n",
      "Epoch: [0]  [ 1400/12099]  eta: 0:21:39  lr: 0.000300  loss: 0.1774 (0.2951)  loss_classifier: 0.0511 (0.1332)  loss_box_reg: 0.1109 (0.1493)  loss_objectness: 0.0037 (0.0085)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1195  data: 0.0330  max mem: 3841\n",
      "Epoch: [0]  [ 1500/12099]  eta: 0:21:27  lr: 0.000300  loss: 0.1618 (0.2875)  loss_classifier: 0.0465 (0.1282)  loss_box_reg: 0.0999 (0.1468)  loss_objectness: 0.0029 (0.0085)  loss_rpn_box_reg: 0.0019 (0.0041)  time: 0.1191  data: 0.0322  max mem: 3841\n",
      "Epoch: [0]  [ 1600/12099]  eta: 0:21:15  lr: 0.000300  loss: 0.1862 (0.2811)  loss_classifier: 0.0533 (0.1239)  loss_box_reg: 0.1162 (0.1448)  loss_objectness: 0.0024 (0.0083)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1206  data: 0.0336  max mem: 3841\n",
      "Epoch: [0]  [ 1700/12099]  eta: 0:21:02  lr: 0.000300  loss: 0.1482 (0.2749)  loss_classifier: 0.0413 (0.1200)  loss_box_reg: 0.0879 (0.1425)  loss_objectness: 0.0037 (0.0083)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1222  data: 0.0343  max mem: 3841\n",
      "Epoch: [0]  [ 1800/12099]  eta: 0:20:50  lr: 0.000300  loss: 0.1644 (0.2692)  loss_classifier: 0.0491 (0.1164)  loss_box_reg: 0.0925 (0.1404)  loss_objectness: 0.0049 (0.0084)  loss_rpn_box_reg: 0.0024 (0.0040)  time: 0.1225  data: 0.0353  max mem: 3841\n",
      "Epoch: [0]  [ 1900/12099]  eta: 0:20:37  lr: 0.000300  loss: 0.1481 (0.2641)  loss_classifier: 0.0408 (0.1131)  loss_box_reg: 0.1031 (0.1386)  loss_objectness: 0.0037 (0.0084)  loss_rpn_box_reg: 0.0022 (0.0040)  time: 0.1189  data: 0.0326  max mem: 3841\n",
      "Epoch: [0]  [ 2000/12099]  eta: 0:20:25  lr: 0.000300  loss: 0.1431 (0.2590)  loss_classifier: 0.0379 (0.1100)  loss_box_reg: 0.1010 (0.1368)  loss_objectness: 0.0044 (0.0082)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1206  data: 0.0345  max mem: 3841\n",
      "Epoch: [0]  [ 2100/12099]  eta: 0:20:12  lr: 0.000300  loss: 0.1265 (0.2544)  loss_classifier: 0.0342 (0.1072)  loss_box_reg: 0.0902 (0.1349)  loss_objectness: 0.0032 (0.0083)  loss_rpn_box_reg: 0.0032 (0.0039)  time: 0.1266  data: 0.0376  max mem: 3841\n",
      "Epoch: [0]  [ 2200/12099]  eta: 0:20:02  lr: 0.000300  loss: 0.1539 (0.2504)  loss_classifier: 0.0456 (0.1050)  loss_box_reg: 0.0764 (0.1330)  loss_objectness: 0.0045 (0.0084)  loss_rpn_box_reg: 0.0028 (0.0040)  time: 0.1227  data: 0.0342  max mem: 3841\n",
      "Epoch: [0]  [ 2300/12099]  eta: 0:19:50  lr: 0.000300  loss: 0.1372 (0.2466)  loss_classifier: 0.0446 (0.1027)  loss_box_reg: 0.0840 (0.1314)  loss_objectness: 0.0023 (0.0084)  loss_rpn_box_reg: 0.0011 (0.0040)  time: 0.1220  data: 0.0343  max mem: 3841\n",
      "Epoch: [0]  [ 2400/12099]  eta: 0:19:39  lr: 0.000300  loss: 0.1351 (0.2429)  loss_classifier: 0.0334 (0.1007)  loss_box_reg: 0.0916 (0.1299)  loss_objectness: 0.0022 (0.0084)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1298  data: 0.0357  max mem: 3841\n",
      "Epoch: [0]  [ 2500/12099]  eta: 0:19:30  lr: 0.000300  loss: 0.1321 (0.2393)  loss_classifier: 0.0425 (0.0987)  loss_box_reg: 0.0839 (0.1282)  loss_objectness: 0.0022 (0.0085)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1304  data: 0.0363  max mem: 3841\n",
      "Epoch: [0]  [ 2600/12099]  eta: 0:19:18  lr: 0.000300  loss: 0.1490 (0.2359)  loss_classifier: 0.0477 (0.0967)  loss_box_reg: 0.0893 (0.1266)  loss_objectness: 0.0052 (0.0085)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1228  data: 0.0333  max mem: 3841\n",
      "Epoch: [0]  [ 2700/12099]  eta: 0:19:05  lr: 0.000300  loss: 0.1282 (0.2324)  loss_classifier: 0.0431 (0.0949)  loss_box_reg: 0.0851 (0.1250)  loss_objectness: 0.0023 (0.0085)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1194  data: 0.0326  max mem: 3841\n",
      "Epoch: [0]  [ 2800/12099]  eta: 0:18:55  lr: 0.000300  loss: 0.1245 (0.2292)  loss_classifier: 0.0343 (0.0932)  loss_box_reg: 0.0783 (0.1235)  loss_objectness: 0.0020 (0.0085)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1292  data: 0.0377  max mem: 3841\n",
      "Epoch: [0]  [ 2900/12099]  eta: 0:18:44  lr: 0.000300  loss: 0.1346 (0.2261)  loss_classifier: 0.0442 (0.0916)  loss_box_reg: 0.0906 (0.1219)  loss_objectness: 0.0036 (0.0086)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1162  data: 0.0324  max mem: 3841\n",
      "Epoch: [0]  [ 3000/12099]  eta: 0:18:30  lr: 0.000300  loss: 0.1265 (0.2233)  loss_classifier: 0.0314 (0.0901)  loss_box_reg: 0.0754 (0.1206)  loss_objectness: 0.0032 (0.0086)  loss_rpn_box_reg: 0.0025 (0.0040)  time: 0.1203  data: 0.0337  max mem: 3841\n",
      "Epoch: [0]  [ 3100/12099]  eta: 0:18:19  lr: 0.000300  loss: 0.1401 (0.2204)  loss_classifier: 0.0449 (0.0886)  loss_box_reg: 0.0818 (0.1193)  loss_objectness: 0.0032 (0.0085)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1151  data: 0.0312  max mem: 3841\n",
      "Epoch: [0]  [ 3200/12099]  eta: 0:18:07  lr: 0.000300  loss: 0.1225 (0.2183)  loss_classifier: 0.0376 (0.0876)  loss_box_reg: 0.0727 (0.1181)  loss_objectness: 0.0034 (0.0086)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1146  data: 0.0316  max mem: 3841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 3300/12099]  eta: 0:17:53  lr: 0.000300  loss: 0.1317 (0.2158)  loss_classifier: 0.0416 (0.0864)  loss_box_reg: 0.0727 (0.1168)  loss_objectness: 0.0038 (0.0085)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1171  data: 0.0319  max mem: 3841\n",
      "Epoch: [0]  [ 3400/12099]  eta: 0:17:39  lr: 0.000300  loss: 0.1226 (0.2139)  loss_classifier: 0.0366 (0.0854)  loss_box_reg: 0.0743 (0.1157)  loss_objectness: 0.0051 (0.0088)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1145  data: 0.0315  max mem: 3841\n",
      "Epoch: [0]  [ 3500/12099]  eta: 0:17:26  lr: 0.000300  loss: 0.1103 (0.2118)  loss_classifier: 0.0277 (0.0844)  loss_box_reg: 0.0735 (0.1147)  loss_objectness: 0.0019 (0.0087)  loss_rpn_box_reg: 0.0010 (0.0040)  time: 0.1158  data: 0.0325  max mem: 3841\n",
      "Epoch: [0]  [ 3600/12099]  eta: 0:17:12  lr: 0.000300  loss: 0.1261 (0.2098)  loss_classifier: 0.0367 (0.0834)  loss_box_reg: 0.0788 (0.1137)  loss_objectness: 0.0053 (0.0087)  loss_rpn_box_reg: 0.0023 (0.0040)  time: 0.1135  data: 0.0307  max mem: 3841\n",
      "Epoch: [0]  [ 3700/12099]  eta: 0:16:58  lr: 0.000300  loss: 0.1145 (0.2077)  loss_classifier: 0.0393 (0.0824)  loss_box_reg: 0.0782 (0.1127)  loss_objectness: 0.0034 (0.0087)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1148  data: 0.0313  max mem: 3841\n",
      "Epoch: [0]  [ 3800/12099]  eta: 0:16:45  lr: 0.000300  loss: 0.1109 (0.2056)  loss_classifier: 0.0303 (0.0813)  loss_box_reg: 0.0728 (0.1117)  loss_objectness: 0.0042 (0.0086)  loss_rpn_box_reg: 0.0011 (0.0040)  time: 0.1136  data: 0.0311  max mem: 3841\n",
      "Epoch: [0]  [ 3900/12099]  eta: 0:16:32  lr: 0.000300  loss: 0.1225 (0.2037)  loss_classifier: 0.0325 (0.0804)  loss_box_reg: 0.0659 (0.1108)  loss_objectness: 0.0051 (0.0085)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1201  data: 0.0347  max mem: 3841\n",
      "Epoch: [0]  [ 4000/12099]  eta: 0:16:18  lr: 0.000300  loss: 0.1458 (0.2020)  loss_classifier: 0.0418 (0.0795)  loss_box_reg: 0.0865 (0.1100)  loss_objectness: 0.0046 (0.0086)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1150  data: 0.0318  max mem: 3841\n",
      "Epoch: [0]  [ 4100/12099]  eta: 0:16:05  lr: 0.000300  loss: 0.1268 (0.2003)  loss_classifier: 0.0376 (0.0787)  loss_box_reg: 0.0684 (0.1090)  loss_objectness: 0.0058 (0.0086)  loss_rpn_box_reg: 0.0023 (0.0040)  time: 0.1132  data: 0.0297  max mem: 3841\n",
      "Epoch: [0]  [ 4200/12099]  eta: 0:15:52  lr: 0.000300  loss: 0.1013 (0.1986)  loss_classifier: 0.0317 (0.0779)  loss_box_reg: 0.0509 (0.1081)  loss_objectness: 0.0022 (0.0086)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1148  data: 0.0319  max mem: 3841\n",
      "Epoch: [0]  [ 4300/12099]  eta: 0:15:41  lr: 0.000300  loss: 0.1089 (0.1969)  loss_classifier: 0.0358 (0.0771)  loss_box_reg: 0.0696 (0.1073)  loss_objectness: 0.0051 (0.0085)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1271  data: 0.0390  max mem: 3841\n",
      "Epoch: [0]  [ 4400/12099]  eta: 0:15:29  lr: 0.000300  loss: 0.1124 (0.1954)  loss_classifier: 0.0360 (0.0764)  loss_box_reg: 0.0664 (0.1065)  loss_objectness: 0.0050 (0.0085)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1167  data: 0.0317  max mem: 3841\n",
      "Epoch: [0]  [ 4500/12099]  eta: 0:15:17  lr: 0.000300  loss: 0.1163 (0.1942)  loss_classifier: 0.0398 (0.0757)  loss_box_reg: 0.0664 (0.1057)  loss_objectness: 0.0071 (0.0086)  loss_rpn_box_reg: 0.0018 (0.0041)  time: 0.1175  data: 0.0313  max mem: 3841\n",
      "Epoch: [0]  [ 4600/12099]  eta: 0:15:05  lr: 0.000300  loss: 0.1090 (0.1927)  loss_classifier: 0.0315 (0.0750)  loss_box_reg: 0.0702 (0.1050)  loss_objectness: 0.0027 (0.0086)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1222  data: 0.0342  max mem: 3841\n",
      "Epoch: [0]  [ 4700/12099]  eta: 0:14:56  lr: 0.000300  loss: 0.1150 (0.1913)  loss_classifier: 0.0300 (0.0743)  loss_box_reg: 0.0719 (0.1042)  loss_objectness: 0.0037 (0.0086)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1411  data: 0.0439  max mem: 3841\n",
      "Epoch: [0]  [ 4800/12099]  eta: 0:14:43  lr: 0.000300  loss: 0.1204 (0.1903)  loss_classifier: 0.0354 (0.0738)  loss_box_reg: 0.0688 (0.1036)  loss_objectness: 0.0030 (0.0087)  loss_rpn_box_reg: 0.0008 (0.0041)  time: 0.1214  data: 0.0351  max mem: 3841\n",
      "Epoch: [0]  [ 4900/12099]  eta: 0:14:32  lr: 0.000300  loss: 0.0931 (0.1891)  loss_classifier: 0.0315 (0.0732)  loss_box_reg: 0.0503 (0.1030)  loss_objectness: 0.0060 (0.0087)  loss_rpn_box_reg: 0.0017 (0.0042)  time: 0.1162  data: 0.0326  max mem: 3841\n",
      "Epoch: [0]  [ 5000/12099]  eta: 0:14:20  lr: 0.000300  loss: 0.0941 (0.1879)  loss_classifier: 0.0363 (0.0726)  loss_box_reg: 0.0520 (0.1023)  loss_objectness: 0.0060 (0.0088)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1374  data: 0.0388  max mem: 3841\n",
      "Epoch: [0]  [ 5100/12099]  eta: 0:14:09  lr: 0.000300  loss: 0.1482 (0.1868)  loss_classifier: 0.0382 (0.0721)  loss_box_reg: 0.0792 (0.1018)  loss_objectness: 0.0024 (0.0088)  loss_rpn_box_reg: 0.0029 (0.0042)  time: 0.1218  data: 0.0330  max mem: 3841\n",
      "Epoch: [0]  [ 5200/12099]  eta: 0:13:57  lr: 0.000300  loss: 0.0847 (0.1857)  loss_classifier: 0.0296 (0.0716)  loss_box_reg: 0.0519 (0.1011)  loss_objectness: 0.0031 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1228  data: 0.0339  max mem: 3841\n",
      "Epoch: [0]  [ 5300/12099]  eta: 0:13:45  lr: 0.000300  loss: 0.0923 (0.1845)  loss_classifier: 0.0297 (0.0710)  loss_box_reg: 0.0505 (0.1005)  loss_objectness: 0.0038 (0.0088)  loss_rpn_box_reg: 0.0006 (0.0042)  time: 0.1184  data: 0.0340  max mem: 3841\n",
      "Epoch: [0]  [ 5400/12099]  eta: 0:13:33  lr: 0.000300  loss: 0.1025 (0.1834)  loss_classifier: 0.0311 (0.0705)  loss_box_reg: 0.0615 (0.0999)  loss_objectness: 0.0057 (0.0088)  loss_rpn_box_reg: 0.0020 (0.0042)  time: 0.1277  data: 0.0389  max mem: 3841\n",
      "Epoch: [0]  [ 5500/12099]  eta: 0:13:22  lr: 0.000300  loss: 0.1299 (0.1826)  loss_classifier: 0.0466 (0.0702)  loss_box_reg: 0.0703 (0.0994)  loss_objectness: 0.0035 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1513  data: 0.0471  max mem: 3841\n",
      "Epoch: [0]  [ 5600/12099]  eta: 0:13:12  lr: 0.000300  loss: 0.1303 (0.1815)  loss_classifier: 0.0304 (0.0696)  loss_box_reg: 0.0560 (0.0989)  loss_objectness: 0.0033 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1347  data: 0.0388  max mem: 3841\n",
      "Epoch: [0]  [ 5700/12099]  eta: 0:13:01  lr: 0.000300  loss: 0.1080 (0.1805)  loss_classifier: 0.0319 (0.0691)  loss_box_reg: 0.0600 (0.0983)  loss_objectness: 0.0075 (0.0089)  loss_rpn_box_reg: 0.0023 (0.0042)  time: 0.1321  data: 0.0407  max mem: 3841\n",
      "Epoch: [0]  [ 5800/12099]  eta: 0:12:49  lr: 0.000300  loss: 0.1212 (0.1796)  loss_classifier: 0.0389 (0.0687)  loss_box_reg: 0.0579 (0.0979)  loss_objectness: 0.0067 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1239  data: 0.0362  max mem: 3841\n",
      "Epoch: [0]  [ 5900/12099]  eta: 0:12:37  lr: 0.000300  loss: 0.1133 (0.1789)  loss_classifier: 0.0354 (0.0683)  loss_box_reg: 0.0668 (0.0975)  loss_objectness: 0.0040 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1273  data: 0.0372  max mem: 3841\n",
      "Epoch: [0]  [ 6000/12099]  eta: 0:12:25  lr: 0.000300  loss: 0.0970 (0.1780)  loss_classifier: 0.0298 (0.0679)  loss_box_reg: 0.0510 (0.0970)  loss_objectness: 0.0026 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1297  data: 0.0386  max mem: 3841\n",
      "Epoch: [0]  [ 6100/12099]  eta: 0:12:13  lr: 0.000300  loss: 0.1326 (0.1772)  loss_classifier: 0.0389 (0.0676)  loss_box_reg: 0.0636 (0.0966)  loss_objectness: 0.0044 (0.0089)  loss_rpn_box_reg: 0.0023 (0.0042)  time: 0.1155  data: 0.0322  max mem: 3841\n",
      "Epoch: [0]  [ 6200/12099]  eta: 0:12:01  lr: 0.000300  loss: 0.1155 (0.1765)  loss_classifier: 0.0275 (0.0672)  loss_box_reg: 0.0689 (0.0962)  loss_objectness: 0.0039 (0.0089)  loss_rpn_box_reg: 0.0009 (0.0042)  time: 0.1244  data: 0.0355  max mem: 3841\n",
      "Epoch: [0]  [ 6300/12099]  eta: 0:11:49  lr: 0.000300  loss: 0.1103 (0.1757)  loss_classifier: 0.0322 (0.0668)  loss_box_reg: 0.0602 (0.0957)  loss_objectness: 0.0059 (0.0089)  loss_rpn_box_reg: 0.0019 (0.0042)  time: 0.1268  data: 0.0351  max mem: 3841\n",
      "Epoch: [0]  [ 6400/12099]  eta: 0:11:37  lr: 0.000300  loss: 0.0982 (0.1749)  loss_classifier: 0.0310 (0.0664)  loss_box_reg: 0.0552 (0.0953)  loss_objectness: 0.0039 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1155  data: 0.0327  max mem: 3841\n",
      "Epoch: [0]  [ 6500/12099]  eta: 0:11:25  lr: 0.000300  loss: 0.1154 (0.1740)  loss_classifier: 0.0284 (0.0661)  loss_box_reg: 0.0673 (0.0949)  loss_objectness: 0.0025 (0.0089)  loss_rpn_box_reg: 0.0009 (0.0042)  time: 0.1463  data: 0.0437  max mem: 3841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 6600/12099]  eta: 0:11:15  lr: 0.000300  loss: 0.1127 (0.1733)  loss_classifier: 0.0399 (0.0657)  loss_box_reg: 0.0670 (0.0945)  loss_objectness: 0.0041 (0.0089)  loss_rpn_box_reg: 0.0008 (0.0042)  time: 0.1597  data: 0.0480  max mem: 3841\n",
      "Epoch: [0]  [ 6700/12099]  eta: 0:11:03  lr: 0.000300  loss: 0.1082 (0.1727)  loss_classifier: 0.0285 (0.0654)  loss_box_reg: 0.0699 (0.0942)  loss_objectness: 0.0025 (0.0090)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1321  data: 0.0371  max mem: 3841\n",
      "Epoch: [0]  [ 6800/12099]  eta: 0:10:52  lr: 0.000300  loss: 0.0932 (0.1719)  loss_classifier: 0.0286 (0.0650)  loss_box_reg: 0.0588 (0.0938)  loss_objectness: 0.0022 (0.0090)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1296  data: 0.0381  max mem: 3841\n",
      "Epoch: [0]  [ 6900/12099]  eta: 0:10:40  lr: 0.000300  loss: 0.1046 (0.1712)  loss_classifier: 0.0302 (0.0647)  loss_box_reg: 0.0613 (0.0934)  loss_objectness: 0.0039 (0.0090)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1312  data: 0.0363  max mem: 3841\n",
      "Epoch: [0]  [ 7000/12099]  eta: 0:10:28  lr: 0.000300  loss: 0.1079 (0.1705)  loss_classifier: 0.0307 (0.0643)  loss_box_reg: 0.0774 (0.0931)  loss_objectness: 0.0041 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0042)  time: 0.1236  data: 0.0333  max mem: 3841\n",
      "Epoch: [0]  [ 7100/12099]  eta: 0:10:16  lr: 0.000300  loss: 0.0980 (0.1699)  loss_classifier: 0.0317 (0.0640)  loss_box_reg: 0.0646 (0.0928)  loss_objectness: 0.0037 (0.0089)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1315  data: 0.0375  max mem: 3841\n",
      "Epoch: [0]  [ 7200/12099]  eta: 0:10:04  lr: 0.000300  loss: 0.1178 (0.1693)  loss_classifier: 0.0359 (0.0637)  loss_box_reg: 0.0592 (0.0925)  loss_objectness: 0.0036 (0.0089)  loss_rpn_box_reg: 0.0021 (0.0042)  time: 0.1268  data: 0.0350  max mem: 3841\n",
      "Epoch: [0]  [ 7300/12099]  eta: 0:09:52  lr: 0.000300  loss: 0.1031 (0.1686)  loss_classifier: 0.0305 (0.0634)  loss_box_reg: 0.0534 (0.0921)  loss_objectness: 0.0028 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1344  data: 0.0384  max mem: 3841\n",
      "Epoch: [0]  [ 7400/12099]  eta: 0:09:40  lr: 0.000300  loss: 0.1056 (0.1680)  loss_classifier: 0.0392 (0.0631)  loss_box_reg: 0.0604 (0.0918)  loss_objectness: 0.0066 (0.0089)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1322  data: 0.0356  max mem: 3841\n",
      "Epoch: [0]  [ 7500/12099]  eta: 0:09:29  lr: 0.000300  loss: 0.1163 (0.1675)  loss_classifier: 0.0380 (0.0629)  loss_box_reg: 0.0658 (0.0916)  loss_objectness: 0.0049 (0.0089)  loss_rpn_box_reg: 0.0009 (0.0042)  time: 0.1417  data: 0.0413  max mem: 3841\n",
      "Epoch: [0]  [ 7600/12099]  eta: 0:09:17  lr: 0.000300  loss: 0.0866 (0.1669)  loss_classifier: 0.0277 (0.0626)  loss_box_reg: 0.0492 (0.0913)  loss_objectness: 0.0035 (0.0089)  loss_rpn_box_reg: 0.0010 (0.0042)  time: 0.1330  data: 0.0363  max mem: 3841\n",
      "Epoch: [0]  [ 7700/12099]  eta: 0:09:05  lr: 0.000300  loss: 0.1102 (0.1664)  loss_classifier: 0.0283 (0.0623)  loss_box_reg: 0.0723 (0.0910)  loss_objectness: 0.0038 (0.0089)  loss_rpn_box_reg: 0.0008 (0.0042)  time: 0.1270  data: 0.0361  max mem: 3841\n",
      "Epoch: [0]  [ 7800/12099]  eta: 0:08:52  lr: 0.000300  loss: 0.1152 (0.1658)  loss_classifier: 0.0356 (0.0621)  loss_box_reg: 0.0607 (0.0907)  loss_objectness: 0.0038 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1282  data: 0.0366  max mem: 3841\n",
      "Epoch: [0]  [ 7900/12099]  eta: 0:08:40  lr: 0.000300  loss: 0.1030 (0.1654)  loss_classifier: 0.0320 (0.0618)  loss_box_reg: 0.0587 (0.0905)  loss_objectness: 0.0057 (0.0089)  loss_rpn_box_reg: 0.0010 (0.0042)  time: 0.1274  data: 0.0367  max mem: 3841\n",
      "Epoch: [0]  [ 8000/12099]  eta: 0:08:28  lr: 0.000300  loss: 0.0906 (0.1650)  loss_classifier: 0.0243 (0.0616)  loss_box_reg: 0.0530 (0.0902)  loss_objectness: 0.0031 (0.0090)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1315  data: 0.0374  max mem: 3841\n",
      "Epoch: [0]  [ 8100/12099]  eta: 0:08:16  lr: 0.000300  loss: 0.0903 (0.1644)  loss_classifier: 0.0299 (0.0614)  loss_box_reg: 0.0551 (0.0899)  loss_objectness: 0.0021 (0.0090)  loss_rpn_box_reg: 0.0017 (0.0042)  time: 0.1279  data: 0.0358  max mem: 3841\n",
      "Epoch: [0]  [ 8200/12099]  eta: 0:08:04  lr: 0.000300  loss: 0.0789 (0.1639)  loss_classifier: 0.0265 (0.0611)  loss_box_reg: 0.0454 (0.0896)  loss_objectness: 0.0022 (0.0091)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1290  data: 0.0363  max mem: 3841\n",
      "Epoch: [0]  [ 8300/12099]  eta: 0:07:51  lr: 0.000300  loss: 0.0832 (0.1634)  loss_classifier: 0.0313 (0.0608)  loss_box_reg: 0.0557 (0.0893)  loss_objectness: 0.0040 (0.0091)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1269  data: 0.0354  max mem: 3841\n",
      "Epoch: [0]  [ 8400/12099]  eta: 0:07:39  lr: 0.000300  loss: 0.0996 (0.1628)  loss_classifier: 0.0342 (0.0606)  loss_box_reg: 0.0638 (0.0890)  loss_objectness: 0.0030 (0.0091)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1292  data: 0.0361  max mem: 3841\n",
      "Epoch: [0]  [ 8500/12099]  eta: 0:07:27  lr: 0.000300  loss: 0.1057 (0.1623)  loss_classifier: 0.0361 (0.0603)  loss_box_reg: 0.0536 (0.0888)  loss_objectness: 0.0083 (0.0091)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1281  data: 0.0361  max mem: 3841\n",
      "Epoch: [0]  [ 8600/12099]  eta: 0:07:15  lr: 0.000300  loss: 0.0993 (0.1619)  loss_classifier: 0.0261 (0.0601)  loss_box_reg: 0.0521 (0.0885)  loss_objectness: 0.0035 (0.0091)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1306  data: 0.0386  max mem: 3841\n",
      "Epoch: [0]  [ 8700/12099]  eta: 0:07:02  lr: 0.000300  loss: 0.1010 (0.1614)  loss_classifier: 0.0287 (0.0599)  loss_box_reg: 0.0699 (0.0883)  loss_objectness: 0.0032 (0.0091)  loss_rpn_box_reg: 0.0006 (0.0042)  time: 0.1296  data: 0.0376  max mem: 3841\n",
      "Epoch: [0]  [ 8800/12099]  eta: 0:06:50  lr: 0.000300  loss: 0.1204 (0.1610)  loss_classifier: 0.0339 (0.0597)  loss_box_reg: 0.0658 (0.0881)  loss_objectness: 0.0061 (0.0090)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1287  data: 0.0360  max mem: 3841\n",
      "Epoch: [0]  [ 8900/12099]  eta: 0:06:38  lr: 0.000300  loss: 0.1277 (0.1606)  loss_classifier: 0.0402 (0.0595)  loss_box_reg: 0.0709 (0.0879)  loss_objectness: 0.0026 (0.0091)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1402  data: 0.0415  max mem: 3841\n",
      "Epoch: [0]  [ 9000/12099]  eta: 0:06:26  lr: 0.000300  loss: 0.1517 (0.1603)  loss_classifier: 0.0396 (0.0593)  loss_box_reg: 0.0913 (0.0877)  loss_objectness: 0.0060 (0.0090)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1567  data: 0.0494  max mem: 3841\n",
      "Epoch: [0]  [ 9100/12099]  eta: 0:06:14  lr: 0.000300  loss: 0.1157 (0.1599)  loss_classifier: 0.0346 (0.0592)  loss_box_reg: 0.0668 (0.0875)  loss_objectness: 0.0036 (0.0090)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1289  data: 0.0355  max mem: 3841\n",
      "Epoch: [0]  [ 9200/12099]  eta: 0:06:02  lr: 0.000300  loss: 0.0917 (0.1596)  loss_classifier: 0.0296 (0.0590)  loss_box_reg: 0.0530 (0.0874)  loss_objectness: 0.0040 (0.0090)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1297  data: 0.0361  max mem: 3841\n",
      "Epoch: [0]  [ 9300/12099]  eta: 0:05:50  lr: 0.000300  loss: 0.0999 (0.1592)  loss_classifier: 0.0293 (0.0588)  loss_box_reg: 0.0593 (0.0872)  loss_objectness: 0.0032 (0.0090)  loss_rpn_box_reg: 0.0010 (0.0041)  time: 0.1315  data: 0.0375  max mem: 3841\n",
      "Epoch: [0]  [ 9400/12099]  eta: 0:05:37  lr: 0.000300  loss: 0.0869 (0.1588)  loss_classifier: 0.0329 (0.0586)  loss_box_reg: 0.0491 (0.0870)  loss_objectness: 0.0026 (0.0090)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1275  data: 0.0348  max mem: 3841\n",
      "Epoch: [0]  [ 9500/12099]  eta: 0:05:25  lr: 0.000300  loss: 0.1381 (0.1584)  loss_classifier: 0.0422 (0.0585)  loss_box_reg: 0.0930 (0.0868)  loss_objectness: 0.0066 (0.0090)  loss_rpn_box_reg: 0.0022 (0.0041)  time: 0.1290  data: 0.0352  max mem: 3841\n",
      "Epoch: [0]  [ 9600/12099]  eta: 0:05:12  lr: 0.000300  loss: 0.1296 (0.1582)  loss_classifier: 0.0346 (0.0583)  loss_box_reg: 0.0648 (0.0867)  loss_objectness: 0.0047 (0.0090)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1262  data: 0.0345  max mem: 3841\n",
      "Epoch: [0]  [ 9700/12099]  eta: 0:05:00  lr: 0.000300  loss: 0.1178 (0.1578)  loss_classifier: 0.0343 (0.0582)  loss_box_reg: 0.0667 (0.0865)  loss_objectness: 0.0030 (0.0090)  loss_rpn_box_reg: 0.0019 (0.0041)  time: 0.1264  data: 0.0348  max mem: 3841\n",
      "Epoch: [0]  [ 9800/12099]  eta: 0:04:47  lr: 0.000300  loss: 0.1077 (0.1575)  loss_classifier: 0.0333 (0.0580)  loss_box_reg: 0.0466 (0.0863)  loss_objectness: 0.0037 (0.0090)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1268  data: 0.0353  max mem: 3841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 9900/12099]  eta: 0:04:35  lr: 0.000300  loss: 0.1204 (0.1572)  loss_classifier: 0.0355 (0.0578)  loss_box_reg: 0.0734 (0.0861)  loss_objectness: 0.0040 (0.0091)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1318  data: 0.0395  max mem: 3841\n",
      "Epoch: [0]  [10000/12099]  eta: 0:04:23  lr: 0.000300  loss: 0.1062 (0.1569)  loss_classifier: 0.0253 (0.0577)  loss_box_reg: 0.0634 (0.0860)  loss_objectness: 0.0025 (0.0091)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1280  data: 0.0361  max mem: 3841\n",
      "Epoch: [0]  [10100/12099]  eta: 0:04:10  lr: 0.000300  loss: 0.1191 (0.1565)  loss_classifier: 0.0273 (0.0575)  loss_box_reg: 0.0676 (0.0858)  loss_objectness: 0.0028 (0.0090)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1288  data: 0.0360  max mem: 3841\n",
      "Epoch: [0]  [10200/12099]  eta: 0:03:58  lr: 0.000300  loss: 0.1163 (0.1561)  loss_classifier: 0.0391 (0.0573)  loss_box_reg: 0.0676 (0.0856)  loss_objectness: 0.0068 (0.0090)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1266  data: 0.0364  max mem: 3841\n",
      "Epoch: [0]  [10300/12099]  eta: 0:03:45  lr: 0.000300  loss: 0.1461 (0.1559)  loss_classifier: 0.0483 (0.0572)  loss_box_reg: 0.0826 (0.0855)  loss_objectness: 0.0040 (0.0090)  loss_rpn_box_reg: 0.0029 (0.0041)  time: 0.1618  data: 0.0539  max mem: 3841\n",
      "Epoch: [0]  [10400/12099]  eta: 0:03:33  lr: 0.000300  loss: 0.0932 (0.1555)  loss_classifier: 0.0339 (0.0570)  loss_box_reg: 0.0589 (0.0854)  loss_objectness: 0.0038 (0.0090)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1261  data: 0.0336  max mem: 3841\n",
      "Epoch: [0]  [10500/12099]  eta: 0:03:20  lr: 0.000300  loss: 0.1069 (0.1552)  loss_classifier: 0.0362 (0.0568)  loss_box_reg: 0.0735 (0.0852)  loss_objectness: 0.0033 (0.0090)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1269  data: 0.0347  max mem: 3841\n",
      "Epoch: [0]  [10600/12099]  eta: 0:03:08  lr: 0.000300  loss: 0.1124 (0.1550)  loss_classifier: 0.0357 (0.0567)  loss_box_reg: 0.0715 (0.0851)  loss_objectness: 0.0042 (0.0090)  loss_rpn_box_reg: 0.0021 (0.0041)  time: 0.1266  data: 0.0330  max mem: 3841\n",
      "Epoch: [0]  [10700/12099]  eta: 0:02:55  lr: 0.000300  loss: 0.0923 (0.1546)  loss_classifier: 0.0259 (0.0565)  loss_box_reg: 0.0602 (0.0849)  loss_objectness: 0.0036 (0.0090)  loss_rpn_box_reg: 0.0016 (0.0042)  time: 0.1272  data: 0.0366  max mem: 3841\n",
      "Epoch: [0]  [10800/12099]  eta: 0:02:43  lr: 0.000300  loss: 0.1176 (0.1543)  loss_classifier: 0.0328 (0.0564)  loss_box_reg: 0.0588 (0.0847)  loss_objectness: 0.0048 (0.0090)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1257  data: 0.0353  max mem: 3841\n",
      "Epoch: [0]  [10900/12099]  eta: 0:02:30  lr: 0.000300  loss: 0.1337 (0.1540)  loss_classifier: 0.0428 (0.0562)  loss_box_reg: 0.0788 (0.0846)  loss_objectness: 0.0034 (0.0090)  loss_rpn_box_reg: 0.0021 (0.0041)  time: 0.1276  data: 0.0352  max mem: 3841\n",
      "Epoch: [0]  [11000/12099]  eta: 0:02:18  lr: 0.000300  loss: 0.1121 (0.1537)  loss_classifier: 0.0365 (0.0561)  loss_box_reg: 0.0721 (0.0844)  loss_objectness: 0.0034 (0.0090)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1255  data: 0.0340  max mem: 3841\n",
      "Epoch: [0]  [11100/12099]  eta: 0:02:05  lr: 0.000300  loss: 0.1036 (0.1533)  loss_classifier: 0.0264 (0.0559)  loss_box_reg: 0.0491 (0.0842)  loss_objectness: 0.0040 (0.0090)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1264  data: 0.0339  max mem: 3841\n",
      "Epoch: [0]  [11200/12099]  eta: 0:01:52  lr: 0.000300  loss: 0.1013 (0.1530)  loss_classifier: 0.0295 (0.0558)  loss_box_reg: 0.0650 (0.0841)  loss_objectness: 0.0033 (0.0090)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1271  data: 0.0351  max mem: 3841\n",
      "Epoch: [0]  [11300/12099]  eta: 0:01:40  lr: 0.000300  loss: 0.1118 (0.1528)  loss_classifier: 0.0295 (0.0557)  loss_box_reg: 0.0679 (0.0840)  loss_objectness: 0.0024 (0.0090)  loss_rpn_box_reg: 0.0019 (0.0041)  time: 0.1272  data: 0.0356  max mem: 3841\n",
      "Epoch: [0]  [11400/12099]  eta: 0:01:27  lr: 0.000300  loss: 0.1080 (0.1525)  loss_classifier: 0.0342 (0.0556)  loss_box_reg: 0.0507 (0.0838)  loss_objectness: 0.0050 (0.0090)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1283  data: 0.0350  max mem: 3841\n",
      "Epoch: [0]  [11500/12099]  eta: 0:01:15  lr: 0.000300  loss: 0.0862 (0.1522)  loss_classifier: 0.0290 (0.0554)  loss_box_reg: 0.0447 (0.0837)  loss_objectness: 0.0032 (0.0090)  loss_rpn_box_reg: 0.0009 (0.0041)  time: 0.1279  data: 0.0342  max mem: 3841\n",
      "Epoch: [0]  [11600/12099]  eta: 0:01:02  lr: 0.000300  loss: 0.0977 (0.1520)  loss_classifier: 0.0266 (0.0553)  loss_box_reg: 0.0565 (0.0835)  loss_objectness: 0.0020 (0.0090)  loss_rpn_box_reg: 0.0025 (0.0042)  time: 0.1274  data: 0.0359  max mem: 3841\n",
      "Epoch: [0]  [11700/12099]  eta: 0:00:50  lr: 0.000300  loss: 0.1013 (0.1517)  loss_classifier: 0.0259 (0.0552)  loss_box_reg: 0.0597 (0.0834)  loss_objectness: 0.0034 (0.0090)  loss_rpn_box_reg: 0.0008 (0.0041)  time: 0.1286  data: 0.0353  max mem: 3841\n",
      "Epoch: [0]  [11800/12099]  eta: 0:00:37  lr: 0.000300  loss: 0.0967 (0.1514)  loss_classifier: 0.0307 (0.0550)  loss_box_reg: 0.0540 (0.0832)  loss_objectness: 0.0031 (0.0090)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1327  data: 0.0389  max mem: 3841\n",
      "Epoch: [0]  [11900/12099]  eta: 0:00:25  lr: 0.000300  loss: 0.0830 (0.1512)  loss_classifier: 0.0298 (0.0549)  loss_box_reg: 0.0538 (0.0832)  loss_objectness: 0.0029 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1270  data: 0.0348  max mem: 3841\n",
      "Epoch: [0]  [12000/12099]  eta: 0:00:12  lr: 0.000300  loss: 0.1150 (0.1509)  loss_classifier: 0.0309 (0.0548)  loss_box_reg: 0.0665 (0.0830)  loss_objectness: 0.0030 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1267  data: 0.0353  max mem: 3841\n",
      "Epoch: [0]  [12098/12099]  eta: 0:00:00  lr: 0.000300  loss: 0.1162 (0.1507)  loss_classifier: 0.0367 (0.0547)  loss_box_reg: 0.0733 (0.0829)  loss_objectness: 0.0050 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1405  data: 0.0402  max mem: 3841\n",
      "Epoch: [0] Total time: 0:25:24 (0.1260 s / it)\n",
      "Epoch: [1]  [    0/12099]  eta: 0:25:12  lr: 0.000300  loss: 0.0943 (0.0943)  loss_classifier: 0.0326 (0.0326)  loss_box_reg: 0.0559 (0.0559)  loss_objectness: 0.0019 (0.0019)  loss_rpn_box_reg: 0.0039 (0.0039)  time: 0.1250  data: 0.0285  max mem: 4002\n",
      "Epoch: [1]  [  100/12099]  eta: 0:25:36  lr: 0.000300  loss: 0.1119 (0.1255)  loss_classifier: 0.0323 (0.0442)  loss_box_reg: 0.0626 (0.0698)  loss_objectness: 0.0027 (0.0083)  loss_rpn_box_reg: 0.0013 (0.0032)  time: 0.1275  data: 0.0354  max mem: 4002\n",
      "Epoch: [1]  [  200/12099]  eta: 0:25:25  lr: 0.000300  loss: 0.1055 (0.1186)  loss_classifier: 0.0309 (0.0404)  loss_box_reg: 0.0610 (0.0667)  loss_objectness: 0.0067 (0.0083)  loss_rpn_box_reg: 0.0021 (0.0033)  time: 0.1258  data: 0.0343  max mem: 4002\n",
      "Epoch: [1]  [  300/12099]  eta: 0:25:12  lr: 0.000300  loss: 0.0973 (0.1181)  loss_classifier: 0.0250 (0.0404)  loss_box_reg: 0.0522 (0.0661)  loss_objectness: 0.0039 (0.0084)  loss_rpn_box_reg: 0.0019 (0.0033)  time: 0.1280  data: 0.0357  max mem: 4002\n",
      "Epoch: [1]  [  400/12099]  eta: 0:24:56  lr: 0.000300  loss: 0.1066 (0.1184)  loss_classifier: 0.0314 (0.0404)  loss_box_reg: 0.0661 (0.0663)  loss_objectness: 0.0028 (0.0083)  loss_rpn_box_reg: 0.0016 (0.0034)  time: 0.1264  data: 0.0340  max mem: 4002\n",
      "Epoch: [1]  [  500/12099]  eta: 0:24:41  lr: 0.000300  loss: 0.0951 (0.1200)  loss_classifier: 0.0339 (0.0404)  loss_box_reg: 0.0568 (0.0668)  loss_objectness: 0.0054 (0.0089)  loss_rpn_box_reg: 0.0022 (0.0039)  time: 0.1255  data: 0.0341  max mem: 4002\n",
      "Epoch: [1]  [  600/12099]  eta: 0:24:28  lr: 0.000300  loss: 0.0974 (0.1206)  loss_classifier: 0.0313 (0.0408)  loss_box_reg: 0.0502 (0.0672)  loss_objectness: 0.0047 (0.0087)  loss_rpn_box_reg: 0.0019 (0.0039)  time: 0.1270  data: 0.0357  max mem: 4002\n",
      "Epoch: [1]  [  700/12099]  eta: 0:24:15  lr: 0.000300  loss: 0.1195 (0.1210)  loss_classifier: 0.0398 (0.0411)  loss_box_reg: 0.0741 (0.0673)  loss_objectness: 0.0047 (0.0086)  loss_rpn_box_reg: 0.0012 (0.0039)  time: 0.1290  data: 0.0359  max mem: 4002\n",
      "Epoch: [1]  [  800/12099]  eta: 0:24:02  lr: 0.000300  loss: 0.1186 (0.1205)  loss_classifier: 0.0331 (0.0411)  loss_box_reg: 0.0706 (0.0671)  loss_objectness: 0.0033 (0.0084)  loss_rpn_box_reg: 0.0018 (0.0040)  time: 0.1257  data: 0.0336  max mem: 4002\n",
      "Epoch: [1]  [  900/12099]  eta: 0:23:49  lr: 0.000300  loss: 0.1168 (0.1210)  loss_classifier: 0.0376 (0.0410)  loss_box_reg: 0.0692 (0.0675)  loss_objectness: 0.0038 (0.0085)  loss_rpn_box_reg: 0.0023 (0.0039)  time: 0.1271  data: 0.0352  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 1000/12099]  eta: 0:23:44  lr: 0.000300  loss: 0.0780 (0.1217)  loss_classifier: 0.0220 (0.0412)  loss_box_reg: 0.0471 (0.0676)  loss_objectness: 0.0027 (0.0089)  loss_rpn_box_reg: 0.0010 (0.0039)  time: 0.1269  data: 0.0347  max mem: 4002\n",
      "Epoch: [1]  [ 1100/12099]  eta: 0:23:31  lr: 0.000300  loss: 0.0973 (0.1218)  loss_classifier: 0.0315 (0.0414)  loss_box_reg: 0.0447 (0.0675)  loss_objectness: 0.0041 (0.0090)  loss_rpn_box_reg: 0.0016 (0.0039)  time: 0.1250  data: 0.0348  max mem: 4002\n",
      "Epoch: [1]  [ 1200/12099]  eta: 0:23:17  lr: 0.000300  loss: 0.1186 (0.1220)  loss_classifier: 0.0351 (0.0414)  loss_box_reg: 0.0550 (0.0677)  loss_objectness: 0.0040 (0.0090)  loss_rpn_box_reg: 0.0017 (0.0039)  time: 0.1255  data: 0.0341  max mem: 4002\n",
      "Epoch: [1]  [ 1300/12099]  eta: 0:23:03  lr: 0.000300  loss: 0.0938 (0.1211)  loss_classifier: 0.0291 (0.0410)  loss_box_reg: 0.0539 (0.0673)  loss_objectness: 0.0026 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0039)  time: 0.1281  data: 0.0361  max mem: 4002\n",
      "Epoch: [1]  [ 1400/12099]  eta: 0:22:49  lr: 0.000300  loss: 0.1123 (0.1210)  loss_classifier: 0.0278 (0.0410)  loss_box_reg: 0.0577 (0.0672)  loss_objectness: 0.0041 (0.0088)  loss_rpn_box_reg: 0.0027 (0.0039)  time: 0.1279  data: 0.0337  max mem: 4002\n",
      "Epoch: [1]  [ 1500/12099]  eta: 0:22:36  lr: 0.000300  loss: 0.0603 (0.1199)  loss_classifier: 0.0170 (0.0406)  loss_box_reg: 0.0450 (0.0669)  loss_objectness: 0.0036 (0.0086)  loss_rpn_box_reg: 0.0007 (0.0039)  time: 0.1281  data: 0.0372  max mem: 4002\n",
      "Epoch: [1]  [ 1600/12099]  eta: 0:22:23  lr: 0.000300  loss: 0.1093 (0.1198)  loss_classifier: 0.0401 (0.0404)  loss_box_reg: 0.0688 (0.0670)  loss_objectness: 0.0022 (0.0085)  loss_rpn_box_reg: 0.0010 (0.0039)  time: 0.1283  data: 0.0344  max mem: 4002\n",
      "Epoch: [1]  [ 1700/12099]  eta: 0:22:09  lr: 0.000300  loss: 0.1209 (0.1203)  loss_classifier: 0.0346 (0.0405)  loss_box_reg: 0.0745 (0.0673)  loss_objectness: 0.0043 (0.0086)  loss_rpn_box_reg: 0.0025 (0.0039)  time: 0.1285  data: 0.0350  max mem: 4002\n",
      "Epoch: [1]  [ 1800/12099]  eta: 0:21:56  lr: 0.000300  loss: 0.1073 (0.1205)  loss_classifier: 0.0255 (0.0405)  loss_box_reg: 0.0746 (0.0674)  loss_objectness: 0.0038 (0.0085)  loss_rpn_box_reg: 0.0024 (0.0040)  time: 0.1252  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [ 1900/12099]  eta: 0:21:43  lr: 0.000300  loss: 0.1166 (0.1208)  loss_classifier: 0.0364 (0.0407)  loss_box_reg: 0.0663 (0.0675)  loss_objectness: 0.0020 (0.0086)  loss_rpn_box_reg: 0.0014 (0.0039)  time: 0.1270  data: 0.0342  max mem: 4002\n",
      "Epoch: [1]  [ 2000/12099]  eta: 0:21:30  lr: 0.000300  loss: 0.0875 (0.1209)  loss_classifier: 0.0279 (0.0408)  loss_box_reg: 0.0445 (0.0675)  loss_objectness: 0.0029 (0.0086)  loss_rpn_box_reg: 0.0023 (0.0039)  time: 0.1283  data: 0.0347  max mem: 4002\n",
      "Epoch: [1]  [ 2100/12099]  eta: 0:21:17  lr: 0.000300  loss: 0.1104 (0.1208)  loss_classifier: 0.0302 (0.0408)  loss_box_reg: 0.0687 (0.0675)  loss_objectness: 0.0045 (0.0085)  loss_rpn_box_reg: 0.0014 (0.0039)  time: 0.1306  data: 0.0374  max mem: 4002\n",
      "Epoch: [1]  [ 2200/12099]  eta: 0:21:04  lr: 0.000300  loss: 0.1027 (0.1209)  loss_classifier: 0.0291 (0.0408)  loss_box_reg: 0.0585 (0.0675)  loss_objectness: 0.0024 (0.0086)  loss_rpn_box_reg: 0.0025 (0.0040)  time: 0.1270  data: 0.0360  max mem: 4002\n",
      "Epoch: [1]  [ 2300/12099]  eta: 0:20:52  lr: 0.000300  loss: 0.0918 (0.1205)  loss_classifier: 0.0280 (0.0405)  loss_box_reg: 0.0629 (0.0674)  loss_objectness: 0.0047 (0.0086)  loss_rpn_box_reg: 0.0019 (0.0040)  time: 0.1306  data: 0.0370  max mem: 4002\n",
      "Epoch: [1]  [ 2400/12099]  eta: 0:20:39  lr: 0.000300  loss: 0.1063 (0.1204)  loss_classifier: 0.0288 (0.0404)  loss_box_reg: 0.0761 (0.0675)  loss_objectness: 0.0040 (0.0086)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1256  data: 0.0342  max mem: 4002\n",
      "Epoch: [1]  [ 2500/12099]  eta: 0:20:26  lr: 0.000300  loss: 0.0965 (0.1202)  loss_classifier: 0.0282 (0.0402)  loss_box_reg: 0.0503 (0.0673)  loss_objectness: 0.0032 (0.0086)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1279  data: 0.0355  max mem: 4002\n",
      "Epoch: [1]  [ 2600/12099]  eta: 0:20:14  lr: 0.000300  loss: 0.0888 (0.1203)  loss_classifier: 0.0293 (0.0403)  loss_box_reg: 0.0578 (0.0674)  loss_objectness: 0.0039 (0.0086)  loss_rpn_box_reg: 0.0009 (0.0040)  time: 0.1268  data: 0.0352  max mem: 4002\n",
      "Epoch: [1]  [ 2700/12099]  eta: 0:20:01  lr: 0.000300  loss: 0.1135 (0.1201)  loss_classifier: 0.0323 (0.0402)  loss_box_reg: 0.0688 (0.0673)  loss_objectness: 0.0028 (0.0085)  loss_rpn_box_reg: 0.0024 (0.0040)  time: 0.1298  data: 0.0355  max mem: 4002\n",
      "Epoch: [1]  [ 2800/12099]  eta: 0:19:49  lr: 0.000300  loss: 0.1012 (0.1202)  loss_classifier: 0.0276 (0.0402)  loss_box_reg: 0.0649 (0.0674)  loss_objectness: 0.0040 (0.0086)  loss_rpn_box_reg: 0.0010 (0.0041)  time: 0.1293  data: 0.0377  max mem: 4002\n",
      "Epoch: [1]  [ 2900/12099]  eta: 0:19:36  lr: 0.000300  loss: 0.1210 (0.1202)  loss_classifier: 0.0313 (0.0401)  loss_box_reg: 0.0572 (0.0673)  loss_objectness: 0.0054 (0.0087)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1304  data: 0.0358  max mem: 4002\n",
      "Epoch: [1]  [ 3000/12099]  eta: 0:19:23  lr: 0.000300  loss: 0.0837 (0.1199)  loss_classifier: 0.0256 (0.0400)  loss_box_reg: 0.0487 (0.0671)  loss_objectness: 0.0019 (0.0086)  loss_rpn_box_reg: 0.0017 (0.0041)  time: 0.1274  data: 0.0353  max mem: 4002\n",
      "Epoch: [1]  [ 3100/12099]  eta: 0:19:10  lr: 0.000300  loss: 0.0913 (0.1200)  loss_classifier: 0.0286 (0.0401)  loss_box_reg: 0.0596 (0.0670)  loss_objectness: 0.0026 (0.0088)  loss_rpn_box_reg: 0.0008 (0.0041)  time: 0.1275  data: 0.0349  max mem: 4002\n",
      "Epoch: [1]  [ 3200/12099]  eta: 0:18:57  lr: 0.000300  loss: 0.1033 (0.1202)  loss_classifier: 0.0291 (0.0401)  loss_box_reg: 0.0549 (0.0673)  loss_objectness: 0.0061 (0.0088)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1273  data: 0.0366  max mem: 4002\n",
      "Epoch: [1]  [ 3300/12099]  eta: 0:18:44  lr: 0.000300  loss: 0.0930 (0.1203)  loss_classifier: 0.0324 (0.0401)  loss_box_reg: 0.0510 (0.0673)  loss_objectness: 0.0032 (0.0088)  loss_rpn_box_reg: 0.0017 (0.0041)  time: 0.1283  data: 0.0347  max mem: 4002\n",
      "Epoch: [1]  [ 3400/12099]  eta: 0:18:31  lr: 0.000300  loss: 0.0975 (0.1205)  loss_classifier: 0.0249 (0.0401)  loss_box_reg: 0.0658 (0.0673)  loss_objectness: 0.0038 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1287  data: 0.0373  max mem: 4002\n",
      "Epoch: [1]  [ 3500/12099]  eta: 0:18:19  lr: 0.000300  loss: 0.1202 (0.1203)  loss_classifier: 0.0389 (0.0401)  loss_box_reg: 0.0521 (0.0672)  loss_objectness: 0.0035 (0.0089)  loss_rpn_box_reg: 0.0032 (0.0041)  time: 0.1262  data: 0.0358  max mem: 4002\n",
      "Epoch: [1]  [ 3600/12099]  eta: 0:18:06  lr: 0.000300  loss: 0.1014 (0.1203)  loss_classifier: 0.0315 (0.0400)  loss_box_reg: 0.0565 (0.0673)  loss_objectness: 0.0030 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1260  data: 0.0349  max mem: 4002\n",
      "Epoch: [1]  [ 3700/12099]  eta: 0:17:53  lr: 0.000300  loss: 0.0874 (0.1203)  loss_classifier: 0.0285 (0.0400)  loss_box_reg: 0.0489 (0.0673)  loss_objectness: 0.0051 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1295  data: 0.0380  max mem: 4002\n",
      "Epoch: [1]  [ 3800/12099]  eta: 0:17:40  lr: 0.000300  loss: 0.0911 (0.1204)  loss_classifier: 0.0270 (0.0400)  loss_box_reg: 0.0511 (0.0674)  loss_objectness: 0.0030 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1292  data: 0.0370  max mem: 4002\n",
      "Epoch: [1]  [ 3900/12099]  eta: 0:17:27  lr: 0.000300  loss: 0.0953 (0.1200)  loss_classifier: 0.0287 (0.0398)  loss_box_reg: 0.0505 (0.0672)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1270  data: 0.0350  max mem: 4002\n",
      "Epoch: [1]  [ 4000/12099]  eta: 0:17:16  lr: 0.000300  loss: 0.0668 (0.1201)  loss_classifier: 0.0249 (0.0398)  loss_box_reg: 0.0392 (0.0672)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0007 (0.0042)  time: 0.1279  data: 0.0364  max mem: 4002\n",
      "Epoch: [1]  [ 4100/12099]  eta: 0:17:03  lr: 0.000300  loss: 0.1271 (0.1201)  loss_classifier: 0.0372 (0.0398)  loss_box_reg: 0.0719 (0.0672)  loss_objectness: 0.0051 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0042)  time: 0.1252  data: 0.0337  max mem: 4002\n",
      "Epoch: [1]  [ 4200/12099]  eta: 0:16:51  lr: 0.000300  loss: 0.0900 (0.1201)  loss_classifier: 0.0302 (0.0399)  loss_box_reg: 0.0534 (0.0672)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1420  data: 0.0465  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 4300/12099]  eta: 0:16:40  lr: 0.000300  loss: 0.1383 (0.1200)  loss_classifier: 0.0454 (0.0398)  loss_box_reg: 0.0811 (0.0672)  loss_objectness: 0.0058 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1332  data: 0.0386  max mem: 4002\n",
      "Epoch: [1]  [ 4400/12099]  eta: 0:16:29  lr: 0.000300  loss: 0.1066 (0.1199)  loss_classifier: 0.0305 (0.0398)  loss_box_reg: 0.0534 (0.0671)  loss_objectness: 0.0039 (0.0088)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1484  data: 0.0431  max mem: 4002\n",
      "Epoch: [1]  [ 4500/12099]  eta: 0:16:16  lr: 0.000300  loss: 0.1389 (0.1200)  loss_classifier: 0.0411 (0.0399)  loss_box_reg: 0.0728 (0.0672)  loss_objectness: 0.0024 (0.0088)  loss_rpn_box_reg: 0.0023 (0.0042)  time: 0.1267  data: 0.0332  max mem: 4002\n",
      "Epoch: [1]  [ 4600/12099]  eta: 0:16:02  lr: 0.000300  loss: 0.0997 (0.1202)  loss_classifier: 0.0336 (0.0400)  loss_box_reg: 0.0522 (0.0672)  loss_objectness: 0.0037 (0.0088)  loss_rpn_box_reg: 0.0021 (0.0042)  time: 0.1241  data: 0.0332  max mem: 4002\n",
      "Epoch: [1]  [ 4700/12099]  eta: 0:15:49  lr: 0.000300  loss: 0.1266 (0.1202)  loss_classifier: 0.0365 (0.0400)  loss_box_reg: 0.0640 (0.0672)  loss_objectness: 0.0053 (0.0088)  loss_rpn_box_reg: 0.0020 (0.0042)  time: 0.1260  data: 0.0345  max mem: 4002\n",
      "Epoch: [1]  [ 4800/12099]  eta: 0:15:36  lr: 0.000300  loss: 0.0794 (0.1200)  loss_classifier: 0.0291 (0.0399)  loss_box_reg: 0.0476 (0.0672)  loss_objectness: 0.0024 (0.0088)  loss_rpn_box_reg: 0.0008 (0.0041)  time: 0.1292  data: 0.0364  max mem: 4002\n",
      "Epoch: [1]  [ 4900/12099]  eta: 0:15:23  lr: 0.000300  loss: 0.0902 (0.1202)  loss_classifier: 0.0286 (0.0400)  loss_box_reg: 0.0605 (0.0672)  loss_objectness: 0.0026 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0041)  time: 0.1301  data: 0.0360  max mem: 4002\n",
      "Epoch: [1]  [ 5000/12099]  eta: 0:15:10  lr: 0.000300  loss: 0.1054 (0.1203)  loss_classifier: 0.0287 (0.0400)  loss_box_reg: 0.0659 (0.0672)  loss_objectness: 0.0044 (0.0089)  loss_rpn_box_reg: 0.0009 (0.0042)  time: 0.1198  data: 0.0323  max mem: 4002\n",
      "Epoch: [1]  [ 5100/12099]  eta: 0:14:56  lr: 0.000300  loss: 0.0940 (0.1202)  loss_classifier: 0.0231 (0.0400)  loss_box_reg: 0.0614 (0.0671)  loss_objectness: 0.0044 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1231  data: 0.0338  max mem: 4002\n",
      "Epoch: [1]  [ 5200/12099]  eta: 0:14:43  lr: 0.000300  loss: 0.1017 (0.1201)  loss_classifier: 0.0327 (0.0400)  loss_box_reg: 0.0591 (0.0671)  loss_objectness: 0.0028 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0042)  time: 0.1223  data: 0.0331  max mem: 4002\n",
      "Epoch: [1]  [ 5300/12099]  eta: 0:14:29  lr: 0.000300  loss: 0.0994 (0.1200)  loss_classifier: 0.0296 (0.0399)  loss_box_reg: 0.0584 (0.0670)  loss_objectness: 0.0033 (0.0089)  loss_rpn_box_reg: 0.0009 (0.0042)  time: 0.1232  data: 0.0328  max mem: 4002\n",
      "Epoch: [1]  [ 5400/12099]  eta: 0:14:16  lr: 0.000300  loss: 0.1039 (0.1202)  loss_classifier: 0.0309 (0.0400)  loss_box_reg: 0.0623 (0.0671)  loss_objectness: 0.0032 (0.0088)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1267  data: 0.0343  max mem: 4002\n",
      "Epoch: [1]  [ 5500/12099]  eta: 0:14:03  lr: 0.000300  loss: 0.1123 (0.1201)  loss_classifier: 0.0395 (0.0400)  loss_box_reg: 0.0646 (0.0671)  loss_objectness: 0.0047 (0.0088)  loss_rpn_box_reg: 0.0017 (0.0042)  time: 0.1240  data: 0.0340  max mem: 4002\n",
      "Epoch: [1]  [ 5600/12099]  eta: 0:13:50  lr: 0.000300  loss: 0.1054 (0.1202)  loss_classifier: 0.0250 (0.0401)  loss_box_reg: 0.0516 (0.0671)  loss_objectness: 0.0033 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1227  data: 0.0341  max mem: 4002\n",
      "Epoch: [1]  [ 5700/12099]  eta: 0:13:37  lr: 0.000300  loss: 0.0781 (0.1201)  loss_classifier: 0.0248 (0.0401)  loss_box_reg: 0.0328 (0.0671)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1265  data: 0.0346  max mem: 4002\n",
      "Epoch: [1]  [ 5800/12099]  eta: 0:13:24  lr: 0.000300  loss: 0.1265 (0.1201)  loss_classifier: 0.0289 (0.0401)  loss_box_reg: 0.0535 (0.0670)  loss_objectness: 0.0038 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1208  data: 0.0326  max mem: 4002\n",
      "Epoch: [1]  [ 5900/12099]  eta: 0:13:10  lr: 0.000300  loss: 0.1048 (0.1202)  loss_classifier: 0.0309 (0.0401)  loss_box_reg: 0.0601 (0.0671)  loss_objectness: 0.0062 (0.0088)  loss_rpn_box_reg: 0.0020 (0.0042)  time: 0.1236  data: 0.0334  max mem: 4002\n",
      "Epoch: [1]  [ 6000/12099]  eta: 0:12:57  lr: 0.000300  loss: 0.1147 (0.1204)  loss_classifier: 0.0390 (0.0401)  loss_box_reg: 0.0607 (0.0672)  loss_objectness: 0.0058 (0.0088)  loss_rpn_box_reg: 0.0025 (0.0042)  time: 0.1237  data: 0.0335  max mem: 4002\n",
      "Epoch: [1]  [ 6100/12099]  eta: 0:12:45  lr: 0.000300  loss: 0.1138 (0.1204)  loss_classifier: 0.0317 (0.0402)  loss_box_reg: 0.0666 (0.0672)  loss_objectness: 0.0036 (0.0088)  loss_rpn_box_reg: 0.0019 (0.0042)  time: 0.1264  data: 0.0350  max mem: 4002\n",
      "Epoch: [1]  [ 6200/12099]  eta: 0:12:32  lr: 0.000300  loss: 0.1421 (0.1206)  loss_classifier: 0.0336 (0.0402)  loss_box_reg: 0.0922 (0.0674)  loss_objectness: 0.0052 (0.0088)  loss_rpn_box_reg: 0.0032 (0.0042)  time: 0.1249  data: 0.0340  max mem: 4002\n",
      "Epoch: [1]  [ 6300/12099]  eta: 0:12:19  lr: 0.000300  loss: 0.0869 (0.1206)  loss_classifier: 0.0275 (0.0402)  loss_box_reg: 0.0545 (0.0673)  loss_objectness: 0.0034 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0043)  time: 0.1271  data: 0.0353  max mem: 4002\n",
      "Epoch: [1]  [ 6400/12099]  eta: 0:12:06  lr: 0.000300  loss: 0.0803 (0.1204)  loss_classifier: 0.0194 (0.0401)  loss_box_reg: 0.0471 (0.0672)  loss_objectness: 0.0049 (0.0088)  loss_rpn_box_reg: 0.0010 (0.0042)  time: 0.1215  data: 0.0333  max mem: 4002\n",
      "Epoch: [1]  [ 6500/12099]  eta: 0:11:53  lr: 0.000300  loss: 0.0922 (0.1204)  loss_classifier: 0.0292 (0.0401)  loss_box_reg: 0.0580 (0.0672)  loss_objectness: 0.0034 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1289  data: 0.0384  max mem: 4002\n",
      "Epoch: [1]  [ 6600/12099]  eta: 0:11:40  lr: 0.000300  loss: 0.1110 (0.1205)  loss_classifier: 0.0276 (0.0401)  loss_box_reg: 0.0576 (0.0673)  loss_objectness: 0.0041 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0042)  time: 0.1225  data: 0.0330  max mem: 4002\n",
      "Epoch: [1]  [ 6700/12099]  eta: 0:11:27  lr: 0.000300  loss: 0.0891 (0.1205)  loss_classifier: 0.0301 (0.0401)  loss_box_reg: 0.0509 (0.0672)  loss_objectness: 0.0043 (0.0089)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1233  data: 0.0341  max mem: 4002\n",
      "Epoch: [1]  [ 6800/12099]  eta: 0:11:14  lr: 0.000300  loss: 0.0993 (0.1202)  loss_classifier: 0.0286 (0.0401)  loss_box_reg: 0.0587 (0.0670)  loss_objectness: 0.0059 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1244  data: 0.0346  max mem: 4002\n",
      "Epoch: [1]  [ 6900/12099]  eta: 0:11:01  lr: 0.000300  loss: 0.1052 (0.1201)  loss_classifier: 0.0329 (0.0401)  loss_box_reg: 0.0626 (0.0670)  loss_objectness: 0.0043 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1278  data: 0.0369  max mem: 4002\n",
      "Epoch: [1]  [ 7000/12099]  eta: 0:10:48  lr: 0.000300  loss: 0.1146 (0.1203)  loss_classifier: 0.0344 (0.0401)  loss_box_reg: 0.0851 (0.0671)  loss_objectness: 0.0028 (0.0089)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1282  data: 0.0357  max mem: 4002\n",
      "Epoch: [1]  [ 7100/12099]  eta: 0:10:35  lr: 0.000300  loss: 0.0905 (0.1202)  loss_classifier: 0.0256 (0.0401)  loss_box_reg: 0.0608 (0.0670)  loss_objectness: 0.0024 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1221  data: 0.0332  max mem: 4002\n",
      "Epoch: [1]  [ 7200/12099]  eta: 0:10:23  lr: 0.000300  loss: 0.1022 (0.1202)  loss_classifier: 0.0324 (0.0401)  loss_box_reg: 0.0515 (0.0670)  loss_objectness: 0.0042 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1239  data: 0.0352  max mem: 4002\n",
      "Epoch: [1]  [ 7300/12099]  eta: 0:10:10  lr: 0.000300  loss: 0.0820 (0.1201)  loss_classifier: 0.0248 (0.0400)  loss_box_reg: 0.0485 (0.0670)  loss_objectness: 0.0046 (0.0089)  loss_rpn_box_reg: 0.0007 (0.0042)  time: 0.1272  data: 0.0374  max mem: 4002\n",
      "Epoch: [1]  [ 7400/12099]  eta: 0:09:57  lr: 0.000300  loss: 0.0659 (0.1199)  loss_classifier: 0.0209 (0.0399)  loss_box_reg: 0.0415 (0.0669)  loss_objectness: 0.0019 (0.0089)  loss_rpn_box_reg: 0.0010 (0.0042)  time: 0.1281  data: 0.0349  max mem: 4002\n",
      "Epoch: [1]  [ 7500/12099]  eta: 0:09:44  lr: 0.000300  loss: 0.1250 (0.1199)  loss_classifier: 0.0315 (0.0399)  loss_box_reg: 0.0796 (0.0669)  loss_objectness: 0.0048 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1250  data: 0.0336  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 7600/12099]  eta: 0:09:31  lr: 0.000300  loss: 0.0954 (0.1198)  loss_classifier: 0.0393 (0.0399)  loss_box_reg: 0.0512 (0.0669)  loss_objectness: 0.0040 (0.0088)  loss_rpn_box_reg: 0.0016 (0.0042)  time: 0.1275  data: 0.0359  max mem: 4002\n",
      "Epoch: [1]  [ 7700/12099]  eta: 0:09:19  lr: 0.000300  loss: 0.0977 (0.1197)  loss_classifier: 0.0247 (0.0399)  loss_box_reg: 0.0660 (0.0668)  loss_objectness: 0.0021 (0.0088)  loss_rpn_box_reg: 0.0017 (0.0042)  time: 0.1328  data: 0.0365  max mem: 4002\n",
      "Epoch: [1]  [ 7800/12099]  eta: 0:09:06  lr: 0.000300  loss: 0.1347 (0.1199)  loss_classifier: 0.0456 (0.0400)  loss_box_reg: 0.0727 (0.0669)  loss_objectness: 0.0038 (0.0088)  loss_rpn_box_reg: 0.0026 (0.0042)  time: 0.1276  data: 0.0360  max mem: 4002\n",
      "Epoch: [1]  [ 7900/12099]  eta: 0:08:54  lr: 0.000300  loss: 0.0877 (0.1199)  loss_classifier: 0.0300 (0.0400)  loss_box_reg: 0.0536 (0.0669)  loss_objectness: 0.0030 (0.0088)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1272  data: 0.0368  max mem: 4002\n",
      "Epoch: [1]  [ 8000/12099]  eta: 0:08:41  lr: 0.000300  loss: 0.1116 (0.1199)  loss_classifier: 0.0297 (0.0400)  loss_box_reg: 0.0615 (0.0669)  loss_objectness: 0.0036 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1272  data: 0.0377  max mem: 4002\n",
      "Epoch: [1]  [ 8100/12099]  eta: 0:08:29  lr: 0.000300  loss: 0.0955 (0.1199)  loss_classifier: 0.0322 (0.0400)  loss_box_reg: 0.0637 (0.0669)  loss_objectness: 0.0034 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1347  data: 0.0396  max mem: 4002\n",
      "Epoch: [1]  [ 8200/12099]  eta: 0:08:16  lr: 0.000300  loss: 0.0788 (0.1200)  loss_classifier: 0.0264 (0.0400)  loss_box_reg: 0.0570 (0.0670)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1193  data: 0.0346  max mem: 4002\n",
      "Epoch: [1]  [ 8300/12099]  eta: 0:08:03  lr: 0.000300  loss: 0.0905 (0.1198)  loss_classifier: 0.0272 (0.0399)  loss_box_reg: 0.0579 (0.0669)  loss_objectness: 0.0027 (0.0088)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1164  data: 0.0329  max mem: 4002\n",
      "Epoch: [1]  [ 8400/12099]  eta: 0:07:50  lr: 0.000300  loss: 0.0991 (0.1198)  loss_classifier: 0.0325 (0.0399)  loss_box_reg: 0.0622 (0.0669)  loss_objectness: 0.0036 (0.0088)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1287  data: 0.0369  max mem: 4002\n",
      "Epoch: [1]  [ 8500/12099]  eta: 0:07:37  lr: 0.000300  loss: 0.0923 (0.1198)  loss_classifier: 0.0259 (0.0399)  loss_box_reg: 0.0654 (0.0669)  loss_objectness: 0.0028 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1234  data: 0.0327  max mem: 4002\n",
      "Epoch: [1]  [ 8600/12099]  eta: 0:07:25  lr: 0.000300  loss: 0.1379 (0.1199)  loss_classifier: 0.0457 (0.0399)  loss_box_reg: 0.0803 (0.0670)  loss_objectness: 0.0044 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0042)  time: 0.1271  data: 0.0336  max mem: 4002\n",
      "Epoch: [1]  [ 8700/12099]  eta: 0:07:12  lr: 0.000300  loss: 0.0819 (0.1199)  loss_classifier: 0.0259 (0.0399)  loss_box_reg: 0.0381 (0.0669)  loss_objectness: 0.0031 (0.0088)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1310  data: 0.0380  max mem: 4002\n",
      "Epoch: [1]  [ 8800/12099]  eta: 0:06:59  lr: 0.000300  loss: 0.0909 (0.1198)  loss_classifier: 0.0252 (0.0399)  loss_box_reg: 0.0565 (0.0669)  loss_objectness: 0.0034 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0042)  time: 0.1249  data: 0.0340  max mem: 4002\n",
      "Epoch: [1]  [ 8900/12099]  eta: 0:06:46  lr: 0.000300  loss: 0.1125 (0.1199)  loss_classifier: 0.0260 (0.0399)  loss_box_reg: 0.0733 (0.0670)  loss_objectness: 0.0035 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0042)  time: 0.1255  data: 0.0359  max mem: 4002\n",
      "Epoch: [1]  [ 9000/12099]  eta: 0:06:33  lr: 0.000300  loss: 0.1119 (0.1198)  loss_classifier: 0.0261 (0.0399)  loss_box_reg: 0.0655 (0.0669)  loss_objectness: 0.0055 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1216  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [ 9100/12099]  eta: 0:06:21  lr: 0.000300  loss: 0.1127 (0.1197)  loss_classifier: 0.0301 (0.0398)  loss_box_reg: 0.0687 (0.0669)  loss_objectness: 0.0027 (0.0089)  loss_rpn_box_reg: 0.0010 (0.0042)  time: 0.1253  data: 0.0337  max mem: 4002\n",
      "Epoch: [1]  [ 9200/12099]  eta: 0:06:08  lr: 0.000300  loss: 0.1201 (0.1197)  loss_classifier: 0.0329 (0.0398)  loss_box_reg: 0.0721 (0.0669)  loss_objectness: 0.0030 (0.0089)  loss_rpn_box_reg: 0.0025 (0.0042)  time: 0.1282  data: 0.0359  max mem: 4002\n",
      "Epoch: [1]  [ 9300/12099]  eta: 0:05:55  lr: 0.000300  loss: 0.0993 (0.1196)  loss_classifier: 0.0274 (0.0398)  loss_box_reg: 0.0549 (0.0668)  loss_objectness: 0.0043 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0042)  time: 0.1230  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [ 9400/12099]  eta: 0:05:42  lr: 0.000300  loss: 0.0991 (0.1198)  loss_classifier: 0.0277 (0.0399)  loss_box_reg: 0.0653 (0.0668)  loss_objectness: 0.0027 (0.0089)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1249  data: 0.0328  max mem: 4002\n",
      "Epoch: [1]  [ 9500/12099]  eta: 0:05:29  lr: 0.000300  loss: 0.0916 (0.1198)  loss_classifier: 0.0234 (0.0398)  loss_box_reg: 0.0515 (0.0668)  loss_objectness: 0.0030 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0042)  time: 0.1251  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [ 9600/12099]  eta: 0:05:17  lr: 0.000300  loss: 0.1179 (0.1198)  loss_classifier: 0.0281 (0.0399)  loss_box_reg: 0.0663 (0.0668)  loss_objectness: 0.0033 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0042)  time: 0.1263  data: 0.0341  max mem: 4002\n",
      "Epoch: [1]  [ 9700/12099]  eta: 0:05:04  lr: 0.000300  loss: 0.1142 (0.1197)  loss_classifier: 0.0272 (0.0398)  loss_box_reg: 0.0706 (0.0668)  loss_objectness: 0.0044 (0.0089)  loss_rpn_box_reg: 0.0011 (0.0042)  time: 0.1255  data: 0.0351  max mem: 4002\n",
      "Epoch: [1]  [ 9800/12099]  eta: 0:04:51  lr: 0.000300  loss: 0.1145 (0.1197)  loss_classifier: 0.0327 (0.0398)  loss_box_reg: 0.0687 (0.0669)  loss_objectness: 0.0047 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0041)  time: 0.1230  data: 0.0329  max mem: 4002\n",
      "Epoch: [1]  [ 9900/12099]  eta: 0:04:39  lr: 0.000300  loss: 0.1020 (0.1198)  loss_classifier: 0.0337 (0.0398)  loss_box_reg: 0.0631 (0.0669)  loss_objectness: 0.0037 (0.0089)  loss_rpn_box_reg: 0.0017 (0.0041)  time: 0.1226  data: 0.0327  max mem: 4002\n",
      "Epoch: [1]  [10000/12099]  eta: 0:04:26  lr: 0.000300  loss: 0.1021 (0.1198)  loss_classifier: 0.0298 (0.0398)  loss_box_reg: 0.0592 (0.0669)  loss_objectness: 0.0034 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1262  data: 0.0363  max mem: 4002\n",
      "Epoch: [1]  [10100/12099]  eta: 0:04:13  lr: 0.000300  loss: 0.1064 (0.1197)  loss_classifier: 0.0332 (0.0398)  loss_box_reg: 0.0569 (0.0669)  loss_objectness: 0.0061 (0.0089)  loss_rpn_box_reg: 0.0023 (0.0041)  time: 0.1271  data: 0.0371  max mem: 4002\n",
      "Epoch: [1]  [10200/12099]  eta: 0:04:00  lr: 0.000300  loss: 0.0988 (0.1197)  loss_classifier: 0.0283 (0.0398)  loss_box_reg: 0.0469 (0.0668)  loss_objectness: 0.0026 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0041)  time: 0.1279  data: 0.0353  max mem: 4002\n",
      "Epoch: [1]  [10300/12099]  eta: 0:03:48  lr: 0.000300  loss: 0.0892 (0.1197)  loss_classifier: 0.0339 (0.0398)  loss_box_reg: 0.0521 (0.0668)  loss_objectness: 0.0050 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1260  data: 0.0351  max mem: 4002\n",
      "Epoch: [1]  [10400/12099]  eta: 0:03:35  lr: 0.000300  loss: 0.0941 (0.1197)  loss_classifier: 0.0287 (0.0398)  loss_box_reg: 0.0611 (0.0669)  loss_objectness: 0.0024 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1301  data: 0.0376  max mem: 4002\n",
      "Epoch: [1]  [10500/12099]  eta: 0:03:22  lr: 0.000300  loss: 0.1050 (0.1196)  loss_classifier: 0.0304 (0.0398)  loss_box_reg: 0.0572 (0.0669)  loss_objectness: 0.0033 (0.0089)  loss_rpn_box_reg: 0.0021 (0.0041)  time: 0.1247  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [10600/12099]  eta: 0:03:10  lr: 0.000300  loss: 0.0950 (0.1195)  loss_classifier: 0.0300 (0.0398)  loss_box_reg: 0.0476 (0.0668)  loss_objectness: 0.0030 (0.0088)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1226  data: 0.0330  max mem: 4002\n",
      "Epoch: [1]  [10700/12099]  eta: 0:02:57  lr: 0.000300  loss: 0.1212 (0.1196)  loss_classifier: 0.0396 (0.0398)  loss_box_reg: 0.0792 (0.0668)  loss_objectness: 0.0036 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1240  data: 0.0334  max mem: 4002\n",
      "Epoch: [1]  [10800/12099]  eta: 0:02:44  lr: 0.000300  loss: 0.0838 (0.1196)  loss_classifier: 0.0290 (0.0398)  loss_box_reg: 0.0543 (0.0669)  loss_objectness: 0.0038 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1318  data: 0.0384  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [10900/12099]  eta: 0:02:32  lr: 0.000300  loss: 0.1012 (0.1196)  loss_classifier: 0.0233 (0.0398)  loss_box_reg: 0.0655 (0.0668)  loss_objectness: 0.0039 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1240  data: 0.0337  max mem: 4002\n",
      "Epoch: [1]  [11000/12099]  eta: 0:02:19  lr: 0.000300  loss: 0.0815 (0.1195)  loss_classifier: 0.0187 (0.0397)  loss_box_reg: 0.0562 (0.0668)  loss_objectness: 0.0027 (0.0089)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1264  data: 0.0331  max mem: 4002\n",
      "Epoch: [1]  [11100/12099]  eta: 0:02:06  lr: 0.000300  loss: 0.1301 (0.1195)  loss_classifier: 0.0370 (0.0397)  loss_box_reg: 0.0637 (0.0668)  loss_objectness: 0.0053 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1266  data: 0.0336  max mem: 4002\n",
      "Epoch: [1]  [11200/12099]  eta: 0:01:53  lr: 0.000300  loss: 0.1037 (0.1194)  loss_classifier: 0.0236 (0.0397)  loss_box_reg: 0.0668 (0.0668)  loss_objectness: 0.0031 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1268  data: 0.0345  max mem: 4002\n",
      "Epoch: [1]  [11300/12099]  eta: 0:01:41  lr: 0.000300  loss: 0.0872 (0.1194)  loss_classifier: 0.0344 (0.0397)  loss_box_reg: 0.0537 (0.0668)  loss_objectness: 0.0033 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1241  data: 0.0339  max mem: 4002\n",
      "Epoch: [1]  [11400/12099]  eta: 0:01:28  lr: 0.000300  loss: 0.0886 (0.1194)  loss_classifier: 0.0299 (0.0397)  loss_box_reg: 0.0463 (0.0667)  loss_objectness: 0.0038 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1241  data: 0.0342  max mem: 4002\n",
      "Epoch: [1]  [11500/12099]  eta: 0:01:15  lr: 0.000300  loss: 0.1042 (0.1195)  loss_classifier: 0.0317 (0.0397)  loss_box_reg: 0.0637 (0.0668)  loss_objectness: 0.0037 (0.0089)  loss_rpn_box_reg: 0.0008 (0.0041)  time: 0.1238  data: 0.0344  max mem: 4002\n",
      "Epoch: [1]  [11600/12099]  eta: 0:01:03  lr: 0.000300  loss: 0.1178 (0.1194)  loss_classifier: 0.0259 (0.0397)  loss_box_reg: 0.0784 (0.0667)  loss_objectness: 0.0044 (0.0088)  loss_rpn_box_reg: 0.0011 (0.0041)  time: 0.1285  data: 0.0363  max mem: 4002\n",
      "Epoch: [1]  [11700/12099]  eta: 0:00:50  lr: 0.000300  loss: 0.1083 (0.1193)  loss_classifier: 0.0305 (0.0397)  loss_box_reg: 0.0678 (0.0667)  loss_objectness: 0.0030 (0.0088)  loss_rpn_box_reg: 0.0037 (0.0041)  time: 0.1402  data: 0.0413  max mem: 4002\n",
      "Epoch: [1]  [11800/12099]  eta: 0:00:37  lr: 0.000300  loss: 0.0903 (0.1193)  loss_classifier: 0.0285 (0.0397)  loss_box_reg: 0.0457 (0.0667)  loss_objectness: 0.0034 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0041)  time: 0.1239  data: 0.0340  max mem: 4002\n",
      "Epoch: [1]  [11900/12099]  eta: 0:00:25  lr: 0.000300  loss: 0.1158 (0.1193)  loss_classifier: 0.0295 (0.0397)  loss_box_reg: 0.0607 (0.0667)  loss_objectness: 0.0046 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0041)  time: 0.1367  data: 0.0390  max mem: 4002\n",
      "Epoch: [1]  [12000/12099]  eta: 0:00:12  lr: 0.000300  loss: 0.1256 (0.1193)  loss_classifier: 0.0363 (0.0397)  loss_box_reg: 0.0695 (0.0666)  loss_objectness: 0.0058 (0.0089)  loss_rpn_box_reg: 0.0032 (0.0041)  time: 0.1331  data: 0.0377  max mem: 4002\n",
      "Epoch: [1]  [12098/12099]  eta: 0:00:00  lr: 0.000300  loss: 0.1024 (0.1192)  loss_classifier: 0.0316 (0.0396)  loss_box_reg: 0.0619 (0.0666)  loss_objectness: 0.0034 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0041)  time: 0.1265  data: 0.0351  max mem: 4002\n",
      "Epoch: [1] Total time: 0:25:37 (0.1271 s / it)\n",
      "Epoch: [2]  [    0/12099]  eta: 0:22:10  lr: 0.000300  loss: 0.0360 (0.0360)  loss_classifier: 0.0084 (0.0084)  loss_box_reg: 0.0266 (0.0266)  loss_objectness: 0.0009 (0.0009)  loss_rpn_box_reg: 0.0002 (0.0002)  time: 0.1100  data: 0.0255  max mem: 4002\n",
      "Epoch: [2]  [  100/12099]  eta: 0:22:52  lr: 0.000300  loss: 0.0875 (0.1186)  loss_classifier: 0.0227 (0.0384)  loss_box_reg: 0.0596 (0.0682)  loss_objectness: 0.0041 (0.0087)  loss_rpn_box_reg: 0.0014 (0.0033)  time: 0.1141  data: 0.0314  max mem: 4002\n",
      "Epoch: [2]  [  200/12099]  eta: 0:23:03  lr: 0.000300  loss: 0.0899 (0.1183)  loss_classifier: 0.0254 (0.0391)  loss_box_reg: 0.0454 (0.0662)  loss_objectness: 0.0024 (0.0092)  loss_rpn_box_reg: 0.0011 (0.0037)  time: 0.1246  data: 0.0366  max mem: 4002\n",
      "Epoch: [2]  [  300/12099]  eta: 0:23:38  lr: 0.000300  loss: 0.1157 (0.1178)  loss_classifier: 0.0467 (0.0392)  loss_box_reg: 0.0564 (0.0657)  loss_objectness: 0.0069 (0.0093)  loss_rpn_box_reg: 0.0015 (0.0036)  time: 0.1149  data: 0.0306  max mem: 4002\n",
      "Epoch: [2]  [  400/12099]  eta: 0:23:18  lr: 0.000300  loss: 0.1006 (0.1205)  loss_classifier: 0.0329 (0.0404)  loss_box_reg: 0.0587 (0.0672)  loss_objectness: 0.0032 (0.0097)  loss_rpn_box_reg: 0.0011 (0.0033)  time: 0.1248  data: 0.0347  max mem: 4002\n",
      "Epoch: [2]  [  500/12099]  eta: 0:23:28  lr: 0.000300  loss: 0.0841 (0.1199)  loss_classifier: 0.0238 (0.0398)  loss_box_reg: 0.0574 (0.0672)  loss_objectness: 0.0030 (0.0093)  loss_rpn_box_reg: 0.0015 (0.0036)  time: 0.1293  data: 0.0364  max mem: 4002\n",
      "Epoch: [2]  [  600/12099]  eta: 0:23:13  lr: 0.000300  loss: 0.1046 (0.1210)  loss_classifier: 0.0375 (0.0406)  loss_box_reg: 0.0639 (0.0671)  loss_objectness: 0.0020 (0.0098)  loss_rpn_box_reg: 0.0018 (0.0035)  time: 0.1256  data: 0.0354  max mem: 4002\n",
      "Epoch: [2]  [  700/12099]  eta: 0:23:14  lr: 0.000300  loss: 0.0901 (0.1203)  loss_classifier: 0.0259 (0.0403)  loss_box_reg: 0.0561 (0.0668)  loss_objectness: 0.0028 (0.0096)  loss_rpn_box_reg: 0.0015 (0.0035)  time: 0.1365  data: 0.0406  max mem: 4002\n",
      "Epoch: [2]  [  800/12099]  eta: 0:23:00  lr: 0.000300  loss: 0.0983 (0.1210)  loss_classifier: 0.0274 (0.0403)  loss_box_reg: 0.0594 (0.0675)  loss_objectness: 0.0031 (0.0094)  loss_rpn_box_reg: 0.0014 (0.0037)  time: 0.1246  data: 0.0354  max mem: 4002\n",
      "Epoch: [2]  [  900/12099]  eta: 0:22:40  lr: 0.000300  loss: 0.0929 (0.1200)  loss_classifier: 0.0281 (0.0401)  loss_box_reg: 0.0556 (0.0668)  loss_objectness: 0.0024 (0.0093)  loss_rpn_box_reg: 0.0009 (0.0038)  time: 0.1128  data: 0.0295  max mem: 4002\n",
      "Epoch: [2]  [ 1000/12099]  eta: 0:22:30  lr: 0.000300  loss: 0.1138 (0.1203)  loss_classifier: 0.0256 (0.0401)  loss_box_reg: 0.0585 (0.0670)  loss_objectness: 0.0068 (0.0093)  loss_rpn_box_reg: 0.0032 (0.0038)  time: 0.1259  data: 0.0346  max mem: 4002\n",
      "Epoch: [2]  [ 1100/12099]  eta: 0:22:19  lr: 0.000300  loss: 0.0910 (0.1208)  loss_classifier: 0.0249 (0.0402)  loss_box_reg: 0.0624 (0.0674)  loss_objectness: 0.0025 (0.0095)  loss_rpn_box_reg: 0.0011 (0.0037)  time: 0.1128  data: 0.0296  max mem: 4002\n",
      "Epoch: [2]  [ 1200/12099]  eta: 0:22:07  lr: 0.000300  loss: 0.0907 (0.1207)  loss_classifier: 0.0269 (0.0401)  loss_box_reg: 0.0500 (0.0675)  loss_objectness: 0.0052 (0.0094)  loss_rpn_box_reg: 0.0009 (0.0036)  time: 0.1207  data: 0.0346  max mem: 4002\n",
      "Epoch: [2]  [ 1300/12099]  eta: 0:21:54  lr: 0.000300  loss: 0.1034 (0.1212)  loss_classifier: 0.0335 (0.0403)  loss_box_reg: 0.0607 (0.0679)  loss_objectness: 0.0062 (0.0093)  loss_rpn_box_reg: 0.0020 (0.0037)  time: 0.1246  data: 0.0366  max mem: 4002\n",
      "Epoch: [2]  [ 1400/12099]  eta: 0:21:40  lr: 0.000300  loss: 0.1267 (0.1212)  loss_classifier: 0.0329 (0.0403)  loss_box_reg: 0.0734 (0.0679)  loss_objectness: 0.0025 (0.0093)  loss_rpn_box_reg: 0.0015 (0.0037)  time: 0.1170  data: 0.0322  max mem: 4002\n",
      "Epoch: [2]  [ 1500/12099]  eta: 0:21:26  lr: 0.000300  loss: 0.1061 (0.1205)  loss_classifier: 0.0317 (0.0400)  loss_box_reg: 0.0601 (0.0674)  loss_objectness: 0.0047 (0.0092)  loss_rpn_box_reg: 0.0018 (0.0039)  time: 0.1199  data: 0.0332  max mem: 4002\n",
      "Epoch: [2]  [ 1600/12099]  eta: 0:21:14  lr: 0.000300  loss: 0.0861 (0.1200)  loss_classifier: 0.0225 (0.0398)  loss_box_reg: 0.0527 (0.0673)  loss_objectness: 0.0029 (0.0091)  loss_rpn_box_reg: 0.0009 (0.0038)  time: 0.1270  data: 0.0362  max mem: 4002\n",
      "Epoch: [2]  [ 1700/12099]  eta: 0:21:04  lr: 0.000300  loss: 0.1027 (0.1198)  loss_classifier: 0.0276 (0.0395)  loss_box_reg: 0.0698 (0.0673)  loss_objectness: 0.0024 (0.0091)  loss_rpn_box_reg: 0.0017 (0.0039)  time: 0.1468  data: 0.0447  max mem: 4002\n",
      "Epoch: [2]  [ 1800/12099]  eta: 0:20:55  lr: 0.000300  loss: 0.0841 (0.1197)  loss_classifier: 0.0258 (0.0395)  loss_box_reg: 0.0558 (0.0673)  loss_objectness: 0.0023 (0.0090)  loss_rpn_box_reg: 0.0013 (0.0038)  time: 0.1204  data: 0.0325  max mem: 4002\n",
      "Epoch: [2]  [ 1900/12099]  eta: 0:20:43  lr: 0.000300  loss: 0.0956 (0.1193)  loss_classifier: 0.0241 (0.0394)  loss_box_reg: 0.0502 (0.0671)  loss_objectness: 0.0018 (0.0090)  loss_rpn_box_reg: 0.0014 (0.0039)  time: 0.1125  data: 0.0288  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2]  [ 2000/12099]  eta: 0:20:30  lr: 0.000300  loss: 0.0911 (0.1191)  loss_classifier: 0.0218 (0.0392)  loss_box_reg: 0.0504 (0.0672)  loss_objectness: 0.0034 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0039)  time: 0.1360  data: 0.0416  max mem: 4002\n",
      "Epoch: [2]  [ 2100/12099]  eta: 0:20:23  lr: 0.000300  loss: 0.0820 (0.1193)  loss_classifier: 0.0323 (0.0394)  loss_box_reg: 0.0457 (0.0673)  loss_objectness: 0.0025 (0.0087)  loss_rpn_box_reg: 0.0011 (0.0038)  time: 0.1472  data: 0.0443  max mem: 4002\n",
      "Epoch: [2]  [ 2200/12099]  eta: 0:20:17  lr: 0.000300  loss: 0.0853 (0.1191)  loss_classifier: 0.0272 (0.0393)  loss_box_reg: 0.0450 (0.0672)  loss_objectness: 0.0051 (0.0087)  loss_rpn_box_reg: 0.0011 (0.0038)  time: 0.1279  data: 0.0344  max mem: 4002\n",
      "Epoch: [2]  [ 2300/12099]  eta: 0:20:11  lr: 0.000300  loss: 0.0920 (0.1192)  loss_classifier: 0.0341 (0.0394)  loss_box_reg: 0.0560 (0.0673)  loss_objectness: 0.0036 (0.0088)  loss_rpn_box_reg: 0.0021 (0.0038)  time: 0.1437  data: 0.0429  max mem: 4002\n",
      "Epoch: [2]  [ 2400/12099]  eta: 0:20:01  lr: 0.000300  loss: 0.1030 (0.1193)  loss_classifier: 0.0302 (0.0393)  loss_box_reg: 0.0673 (0.0675)  loss_objectness: 0.0035 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0038)  time: 0.1299  data: 0.0360  max mem: 4002\n",
      "Epoch: [2]  [ 2500/12099]  eta: 0:19:54  lr: 0.000300  loss: 0.0784 (0.1187)  loss_classifier: 0.0240 (0.0391)  loss_box_reg: 0.0459 (0.0672)  loss_objectness: 0.0017 (0.0087)  loss_rpn_box_reg: 0.0010 (0.0037)  time: 0.1373  data: 0.0385  max mem: 4002\n",
      "Epoch: [2]  [ 2600/12099]  eta: 0:19:42  lr: 0.000300  loss: 0.1044 (0.1191)  loss_classifier: 0.0285 (0.0392)  loss_box_reg: 0.0546 (0.0673)  loss_objectness: 0.0041 (0.0088)  loss_rpn_box_reg: 0.0018 (0.0038)  time: 0.1323  data: 0.0370  max mem: 4002\n",
      "Epoch: [2]  [ 2700/12099]  eta: 0:19:35  lr: 0.000300  loss: 0.0972 (0.1186)  loss_classifier: 0.0341 (0.0391)  loss_box_reg: 0.0542 (0.0670)  loss_objectness: 0.0061 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0038)  time: 0.1347  data: 0.0382  max mem: 4002\n",
      "Epoch: [2]  [ 2800/12099]  eta: 0:19:29  lr: 0.000300  loss: 0.0890 (0.1183)  loss_classifier: 0.0285 (0.0390)  loss_box_reg: 0.0340 (0.0668)  loss_objectness: 0.0054 (0.0088)  loss_rpn_box_reg: 0.0013 (0.0037)  time: 0.1410  data: 0.0414  max mem: 4002\n",
      "Epoch: [2]  [ 2900/12099]  eta: 0:19:19  lr: 0.000300  loss: 0.1319 (0.1183)  loss_classifier: 0.0401 (0.0390)  loss_box_reg: 0.0893 (0.0669)  loss_objectness: 0.0053 (0.0087)  loss_rpn_box_reg: 0.0017 (0.0037)  time: 0.1310  data: 0.0349  max mem: 4002\n",
      "Epoch: [2]  [ 3000/12099]  eta: 0:19:08  lr: 0.000300  loss: 0.1118 (0.1183)  loss_classifier: 0.0345 (0.0389)  loss_box_reg: 0.0529 (0.0667)  loss_objectness: 0.0055 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0038)  time: 0.1319  data: 0.0359  max mem: 4002\n",
      "Epoch: [2]  [ 3100/12099]  eta: 0:18:57  lr: 0.000300  loss: 0.0960 (0.1182)  loss_classifier: 0.0250 (0.0389)  loss_box_reg: 0.0530 (0.0667)  loss_objectness: 0.0025 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0038)  time: 0.1310  data: 0.0350  max mem: 4002\n",
      "Epoch: [2]  [ 3200/12099]  eta: 0:18:48  lr: 0.000300  loss: 0.0968 (0.1184)  loss_classifier: 0.0320 (0.0390)  loss_box_reg: 0.0463 (0.0667)  loss_objectness: 0.0059 (0.0088)  loss_rpn_box_reg: 0.0017 (0.0038)  time: 0.1448  data: 0.0416  max mem: 4002\n",
      "Epoch: [2]  [ 3300/12099]  eta: 0:18:40  lr: 0.000300  loss: 0.1004 (0.1185)  loss_classifier: 0.0309 (0.0391)  loss_box_reg: 0.0484 (0.0667)  loss_objectness: 0.0061 (0.0088)  loss_rpn_box_reg: 0.0022 (0.0039)  time: 0.1465  data: 0.0459  max mem: 4002\n",
      "Epoch: [2]  [ 3400/12099]  eta: 0:18:27  lr: 0.000300  loss: 0.1068 (0.1185)  loss_classifier: 0.0297 (0.0391)  loss_box_reg: 0.0675 (0.0667)  loss_objectness: 0.0023 (0.0089)  loss_rpn_box_reg: 0.0008 (0.0038)  time: 0.1407  data: 0.0423  max mem: 4002\n",
      "Epoch: [2]  [ 3500/12099]  eta: 0:18:16  lr: 0.000300  loss: 0.1180 (0.1182)  loss_classifier: 0.0266 (0.0389)  loss_box_reg: 0.0791 (0.0666)  loss_objectness: 0.0045 (0.0088)  loss_rpn_box_reg: 0.0012 (0.0038)  time: 0.1405  data: 0.0399  max mem: 4002\n",
      "Epoch: [2]  [ 3600/12099]  eta: 0:18:06  lr: 0.000300  loss: 0.0723 (0.1181)  loss_classifier: 0.0235 (0.0389)  loss_box_reg: 0.0455 (0.0665)  loss_objectness: 0.0032 (0.0088)  loss_rpn_box_reg: 0.0032 (0.0039)  time: 0.1336  data: 0.0379  max mem: 4002\n",
      "Epoch: [2]  [ 3700/12099]  eta: 0:17:56  lr: 0.000300  loss: 0.1262 (0.1181)  loss_classifier: 0.0345 (0.0389)  loss_box_reg: 0.0735 (0.0664)  loss_objectness: 0.0063 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0039)  time: 0.1327  data: 0.0372  max mem: 4002\n",
      "Epoch: [2]  [ 3800/12099]  eta: 0:17:45  lr: 0.000300  loss: 0.0836 (0.1180)  loss_classifier: 0.0238 (0.0388)  loss_box_reg: 0.0463 (0.0665)  loss_objectness: 0.0031 (0.0088)  loss_rpn_box_reg: 0.0010 (0.0039)  time: 0.1332  data: 0.0380  max mem: 4002\n",
      "Epoch: [2]  [ 3900/12099]  eta: 0:17:33  lr: 0.000300  loss: 0.1134 (0.1181)  loss_classifier: 0.0364 (0.0389)  loss_box_reg: 0.0660 (0.0665)  loss_objectness: 0.0050 (0.0088)  loss_rpn_box_reg: 0.0016 (0.0039)  time: 0.1334  data: 0.0364  max mem: 4002\n",
      "Epoch: [2]  [ 4000/12099]  eta: 0:17:21  lr: 0.000300  loss: 0.1293 (0.1181)  loss_classifier: 0.0324 (0.0389)  loss_box_reg: 0.0678 (0.0664)  loss_objectness: 0.0046 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1323  data: 0.0358  max mem: 4002\n",
      "Epoch: [2]  [ 4100/12099]  eta: 0:17:09  lr: 0.000300  loss: 0.0984 (0.1179)  loss_classifier: 0.0319 (0.0388)  loss_box_reg: 0.0536 (0.0663)  loss_objectness: 0.0030 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0040)  time: 0.1319  data: 0.0360  max mem: 4002\n",
      "Epoch: [2]  [ 4200/12099]  eta: 0:16:56  lr: 0.000300  loss: 0.0878 (0.1180)  loss_classifier: 0.0266 (0.0388)  loss_box_reg: 0.0556 (0.0663)  loss_objectness: 0.0040 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1319  data: 0.0363  max mem: 4002\n",
      "Epoch: [2]  [ 4300/12099]  eta: 0:16:44  lr: 0.000300  loss: 0.1002 (0.1180)  loss_classifier: 0.0323 (0.0388)  loss_box_reg: 0.0560 (0.0664)  loss_objectness: 0.0025 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1333  data: 0.0375  max mem: 4002\n",
      "Epoch: [2]  [ 4400/12099]  eta: 0:16:32  lr: 0.000300  loss: 0.1117 (0.1180)  loss_classifier: 0.0349 (0.0388)  loss_box_reg: 0.0732 (0.0664)  loss_objectness: 0.0050 (0.0089)  loss_rpn_box_reg: 0.0020 (0.0040)  time: 0.1318  data: 0.0360  max mem: 4002\n",
      "Epoch: [2]  [ 4500/12099]  eta: 0:16:20  lr: 0.000300  loss: 0.0961 (0.1179)  loss_classifier: 0.0266 (0.0388)  loss_box_reg: 0.0634 (0.0663)  loss_objectness: 0.0039 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0039)  time: 0.1323  data: 0.0363  max mem: 4002\n",
      "Epoch: [2]  [ 4600/12099]  eta: 0:16:08  lr: 0.000300  loss: 0.0994 (0.1178)  loss_classifier: 0.0315 (0.0388)  loss_box_reg: 0.0545 (0.0661)  loss_objectness: 0.0041 (0.0089)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1305  data: 0.0345  max mem: 4002\n",
      "Epoch: [2]  [ 4700/12099]  eta: 0:15:56  lr: 0.000300  loss: 0.0897 (0.1178)  loss_classifier: 0.0258 (0.0388)  loss_box_reg: 0.0547 (0.0661)  loss_objectness: 0.0048 (0.0089)  loss_rpn_box_reg: 0.0012 (0.0040)  time: 0.1308  data: 0.0352  max mem: 4002\n",
      "Epoch: [2]  [ 4800/12099]  eta: 0:15:43  lr: 0.000300  loss: 0.1058 (0.1178)  loss_classifier: 0.0287 (0.0388)  loss_box_reg: 0.0623 (0.0661)  loss_objectness: 0.0027 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0039)  time: 0.1338  data: 0.0375  max mem: 4002\n",
      "Epoch: [2]  [ 4900/12099]  eta: 0:15:31  lr: 0.000300  loss: 0.1367 (0.1177)  loss_classifier: 0.0374 (0.0388)  loss_box_reg: 0.0792 (0.0661)  loss_objectness: 0.0037 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0039)  time: 0.1322  data: 0.0357  max mem: 4002\n",
      "Epoch: [2]  [ 5000/12099]  eta: 0:15:19  lr: 0.000300  loss: 0.1028 (0.1176)  loss_classifier: 0.0291 (0.0387)  loss_box_reg: 0.0551 (0.0660)  loss_objectness: 0.0056 (0.0089)  loss_rpn_box_reg: 0.0017 (0.0039)  time: 0.1313  data: 0.0355  max mem: 4002\n",
      "Epoch: [2]  [ 5100/12099]  eta: 0:15:06  lr: 0.000300  loss: 0.1500 (0.1178)  loss_classifier: 0.0526 (0.0388)  loss_box_reg: 0.0700 (0.0661)  loss_objectness: 0.0072 (0.0089)  loss_rpn_box_reg: 0.0019 (0.0039)  time: 0.1305  data: 0.0342  max mem: 4002\n",
      "Epoch: [2]  [ 5200/12099]  eta: 0:14:53  lr: 0.000300  loss: 0.1083 (0.1178)  loss_classifier: 0.0307 (0.0388)  loss_box_reg: 0.0602 (0.0661)  loss_objectness: 0.0032 (0.0089)  loss_rpn_box_reg: 0.0017 (0.0039)  time: 0.1319  data: 0.0365  max mem: 4002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2]  [ 5300/12099]  eta: 0:14:41  lr: 0.000300  loss: 0.1137 (0.1179)  loss_classifier: 0.0354 (0.0389)  loss_box_reg: 0.0701 (0.0661)  loss_objectness: 0.0041 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0040)  time: 0.1327  data: 0.0374  max mem: 4002\n",
      "Epoch: [2]  [ 5400/12099]  eta: 0:14:28  lr: 0.000300  loss: 0.0938 (0.1178)  loss_classifier: 0.0256 (0.0388)  loss_box_reg: 0.0612 (0.0661)  loss_objectness: 0.0032 (0.0090)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1308  data: 0.0349  max mem: 4002\n",
      "Epoch: [2]  [ 5500/12099]  eta: 0:14:16  lr: 0.000300  loss: 0.1179 (0.1180)  loss_classifier: 0.0281 (0.0389)  loss_box_reg: 0.0704 (0.0662)  loss_objectness: 0.0024 (0.0090)  loss_rpn_box_reg: 0.0021 (0.0040)  time: 0.1316  data: 0.0364  max mem: 4002\n",
      "Epoch: [2]  [ 5600/12099]  eta: 0:14:04  lr: 0.000300  loss: 0.1226 (0.1182)  loss_classifier: 0.0365 (0.0390)  loss_box_reg: 0.0756 (0.0663)  loss_objectness: 0.0045 (0.0090)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1656  data: 0.0461  max mem: 4002\n",
      "Epoch: [2]  [ 5700/12099]  eta: 0:13:52  lr: 0.000300  loss: 0.0825 (0.1180)  loss_classifier: 0.0234 (0.0389)  loss_box_reg: 0.0497 (0.0662)  loss_objectness: 0.0039 (0.0089)  loss_rpn_box_reg: 0.0016 (0.0040)  time: 0.1410  data: 0.0423  max mem: 4002\n",
      "Epoch: [2]  [ 5800/12099]  eta: 0:13:40  lr: 0.000300  loss: 0.0909 (0.1179)  loss_classifier: 0.0313 (0.0389)  loss_box_reg: 0.0573 (0.0661)  loss_objectness: 0.0033 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1371  data: 0.0385  max mem: 4002\n",
      "Epoch: [2]  [ 5900/12099]  eta: 0:13:27  lr: 0.000300  loss: 0.1046 (0.1181)  loss_classifier: 0.0258 (0.0390)  loss_box_reg: 0.0732 (0.0662)  loss_objectness: 0.0035 (0.0090)  loss_rpn_box_reg: 0.0018 (0.0040)  time: 0.1250  data: 0.0338  max mem: 4002\n",
      "Epoch: [2]  [ 6000/12099]  eta: 0:13:13  lr: 0.000300  loss: 0.0926 (0.1183)  loss_classifier: 0.0280 (0.0391)  loss_box_reg: 0.0489 (0.0662)  loss_objectness: 0.0042 (0.0090)  loss_rpn_box_reg: 0.0018 (0.0040)  time: 0.1259  data: 0.0341  max mem: 4002\n",
      "Epoch: [2]  [ 6100/12099]  eta: 0:13:00  lr: 0.000300  loss: 0.1048 (0.1183)  loss_classifier: 0.0249 (0.0390)  loss_box_reg: 0.0608 (0.0662)  loss_objectness: 0.0043 (0.0090)  loss_rpn_box_reg: 0.0011 (0.0040)  time: 0.1259  data: 0.0358  max mem: 4002\n",
      "Epoch: [2]  [ 6200/12099]  eta: 0:12:47  lr: 0.000300  loss: 0.1007 (0.1185)  loss_classifier: 0.0262 (0.0391)  loss_box_reg: 0.0509 (0.0664)  loss_objectness: 0.0030 (0.0090)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1262  data: 0.0361  max mem: 4002\n",
      "Epoch: [2]  [ 6300/12099]  eta: 0:12:34  lr: 0.000300  loss: 0.0860 (0.1184)  loss_classifier: 0.0264 (0.0391)  loss_box_reg: 0.0472 (0.0663)  loss_objectness: 0.0033 (0.0090)  loss_rpn_box_reg: 0.0015 (0.0040)  time: 0.1291  data: 0.0383  max mem: 4002\n",
      "Epoch: [2]  [ 6400/12099]  eta: 0:12:21  lr: 0.000300  loss: 0.1314 (0.1185)  loss_classifier: 0.0381 (0.0391)  loss_box_reg: 0.0803 (0.0664)  loss_objectness: 0.0042 (0.0089)  loss_rpn_box_reg: 0.0014 (0.0039)  time: 0.1182  data: 0.0317  max mem: 4002\n",
      "Epoch: [2]  [ 6500/12099]  eta: 0:12:07  lr: 0.000300  loss: 0.1070 (0.1183)  loss_classifier: 0.0289 (0.0391)  loss_box_reg: 0.0580 (0.0664)  loss_objectness: 0.0062 (0.0089)  loss_rpn_box_reg: 0.0013 (0.0040)  time: 0.1238  data: 0.0356  max mem: 4002\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "cis_coco_evaluator_method = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans with method 3\n",
    "model.roi_heads.box_predictor.cls_score[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False) \n",
    "model.roi_heads.box_predictor.bbox_pred[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "trans_coco_evaluator_method = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn roi 4, method3.3 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn+roi 4, method3.3 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model with Method 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False),nn.Linear(in_channels, num_classes))\n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
