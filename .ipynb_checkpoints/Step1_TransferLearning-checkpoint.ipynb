{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RUQCnETVJhK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from local lib files\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "Example of data in annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08r4QhagWuKZ"
   },
   "source": [
    "Number of images per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tYL7gDDW09R"
   },
   "outputs": [],
   "source": [
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann= json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qbuy1jLsWuew",
    "outputId": "7ba79560-5443-4fea-f46c-c0e3acaa1588"
   },
   "outputs": [],
   "source": [
    "print(len(cis_test_ann['images']))\n",
    "print(len(cis_val_ann['images']))\n",
    "print(len(train_ann['images']))\n",
    "print(len(trans_test_ann['images']))\n",
    "print(len(trans_val_ann['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJSS3CO7heG3",
    "outputId": "32125207-6219-4017-8ad6-399632efd639"
   },
   "outputs": [],
   "source": [
    "trans_val_ann.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_xxRrJFRE29",
    "outputId": "57e66677-3c9d-478f-e48b-bb3226f53546"
   },
   "outputs": [],
   "source": [
    "trans_val_ann['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajXJCO_jTGnY",
    "outputId": "f023204a-68dc-48c2-c5b4-2dd2da34b97e"
   },
   "outputs": [],
   "source": [
    "trans_val_ann['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChHLy0fE5s8Z",
    "outputId": "f0fac9dd-5a98-4738-8de9-dc67919292e4"
   },
   "outputs": [],
   "source": [
    "trans_val_ann['annotations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnDM9gbwrQNx"
   },
   "source": [
    "## Horizontal flip debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "iOwBQIVEYWkN",
    "outputId": "c91a5292-95b4-4e30-9e96-a5cbbc6cdf66"
   },
   "outputs": [],
   "source": [
    "i = 500\n",
    "\n",
    "boxes = [trans_val_ann['annotations'][j]['bbox'] for j in range(len(trans_val_ann['annotations'])) \n",
    "         if trans_val_ann['annotations'][j]['image_id']==trans_val_ann['images'][i]['id'] \n",
    "         and 'bbox' in trans_val_ann['annotations'][j].keys()]\n",
    "\n",
    "img_path = os.path.join('eccv_18_all_images_sm', trans_val_ann['images'][i]['file_name']) # to change\n",
    "\n",
    "image = read_image(img_path)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image[0].squeeze(),cmap=\"gray\")\n",
    "\n",
    "scale_x = image.shape[2] / trans_val_ann['images'][i]['width'] \n",
    "scale_y = image.shape[1] / trans_val_ann['images'][i]['height']\n",
    "\n",
    "boxes = torch.as_tensor(boxes)\n",
    "\n",
    "for i in range(boxes.shape[0]):\n",
    "    boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "    boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "    boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "    boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "    boxes[i][2] = boxes[i][0] + boxes[i][2]\n",
    "    boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "target = {}\n",
    "target[\"boxes\"] = boxes\n",
    "\n",
    "rect = patches.Rectangle((boxes[0][0], boxes[0][1]), boxes[0][2]-boxes[0][0], \n",
    "                         boxes[0][3]-boxes[0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-RQS521hTUZ"
   },
   "outputs": [],
   "source": [
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "#image=conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnnzSWwDY21l",
    "outputId": "394a67d3-5c61-4588-e343-c8f8bd923405"
   },
   "outputs": [],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ6SPQCGhdE5"
   },
   "outputs": [],
   "source": [
    "conv = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQzZM7EvYkCE"
   },
   "outputs": [],
   "source": [
    "width, height = image.size[0], image.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNSinfTNYkIn",
    "outputId": "7ae7ecf8-a108-4042-d840-6a3951d10752"
   },
   "outputs": [],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CtVK02HYhfB-"
   },
   "outputs": [],
   "source": [
    "image_new = conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "\n",
    "data_transform = {\n",
    "    'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGYQbOaGzeYb"
   },
   "outputs": [],
   "source": [
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    # returns a list with the idx of images with at least one bounding box (img_wbbox) and \n",
    "    # a list with the number of bbox for each valid image (num_bbox)\n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "    if len(bboxes) != 0:\n",
    "        img_wbbox.append(i)\n",
    "        num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform=None, target_transform=None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        \n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        imag = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']] \n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0]=torch.round(boxes[i][0]*scale_x)\n",
    "            boxes[i][1]=torch.round(boxes[i][1]*scale_y)\n",
    "            boxes[i][2]=torch.round(boxes[i][2]*scale_x)\n",
    "            boxes[i][3]=torch.round(boxes[i][3]*scale_y)\n",
    "\n",
    "            boxes[i][2]=boxes[i][0]+boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3]=boxes[i][1]+boxes[i][3]\n",
    "\n",
    "            #boxes[i][0]*=scale_x\n",
    "            #boxes[i][1]*=scale_y\n",
    "            #boxes[i][2]*=scale_x\n",
    "            #boxes[i][3]*=scale_y\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] = iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpDLz3L47UEc"
   },
   "source": [
    "Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMPXY53859HC"
   },
   "outputs": [],
   "source": [
    "# get the images bounding boxes *takes about 20sec*\n",
    "train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "cis_val_valid_img,_ = get_img_with_bbox(cis_val_ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EVMl_FsOJ9Z"
   },
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_ann_path, img_folder, train_valid_img)\n",
    "valid_data = CustomImageDataset(cis_val_ann_path, img_folder, cis_val_valid_img)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n",
    "# In paper : ' We use a batch size of 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "CQ6gGqEal8d8",
    "outputId": "6fb81220-5e7b-4751-b47d-9c01f003d3aa"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Feature batch shape: {len(train_features)}\")\n",
    "print(f\"Target (Bbox) batch shape: {train_labels[0]['boxes'].size()}\")\n",
    "print(f\"Target (category) batch shape: {train_labels[0]['labels'].size()}\")\n",
    "\n",
    "img = train_features[0][0].squeeze()\n",
    "label = train_labels[0]['labels']\n",
    "label_categ = 'animal'\n",
    "\n",
    "if label[0] == 0:\n",
    "    label_categ = 'background'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img,cmap = \"gray\")\n",
    "rect = patches.Rectangle((train_labels[0]['boxes'][0][0], \n",
    "                          train_labels[0]['boxes'][0][1]), \n",
    "                         train_labels[0]['boxes'][0][2]-train_labels[0]['boxes'][0][0], \n",
    "                         train_labels[0]['boxes'][0][3]-train_labels[0]['boxes'][0][1], \n",
    "                         linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "print(f\"Label: {label_categ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal flip debugging p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5149muQLuh7"
   },
   "outputs": [],
   "source": [
    "# trans = data_transform['train']\n",
    "# img2, target2 = trans(image, target)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(img2[0].squeeze(), cmap=\"gray\")\n",
    "\n",
    "# rect = patches.Rectangle((target2['boxes'][0][0], target2['boxes'][0][1]), \n",
    "#                          target2['boxes'][0][2] - target2['boxes'][0][0], \n",
    "#                          target2['boxes'][0][3] - target2['boxes'][0][1], \n",
    "#                          linewidth=2, edgecolor='r', facecolor='none')\n",
    "# ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UGQCk7Gcoy7"
   },
   "outputs": [],
   "source": [
    "# Inspired from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbSuc8Jwc5qT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "# like in the paper\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNli84Y1V-Az"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdZtIwI_iy9l"
   },
   "outputs": [],
   "source": [
    "trans_val_valid_img,_ = get_img_with_bbox(trans_val_ann_path)   # takes about 1min to run on train data\n",
    "trans_valid_data = CustomImageDataset(trans_val_ann_path,img_folder, trans_val_valid_img)\n",
    "trans_valid_dataloader = DataLoader(trans_valid_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdGh66kZ_8av"
   },
   "outputs": [],
   "source": [
    "# cis_val_valid_img,_ = get_img_with_bbox(cis_val_ann_path)   # takes about 1min to run on train data\n",
    "cis_valid_data = CustomImageDataset(cis_val_ann_path,img_folder, cis_val_valid_img)\n",
    "cis_valid_dataloader = DataLoader(cis_valid_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNdYtR0kzbAO"
   },
   "outputs": [],
   "source": [
    "# !!TO DO : replace this 'return coco_evaluator' by this 'return metric_logger' on line \n",
    "# #109 of engine.py in evaluate function and CTRL+S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VTRNc5oohYv"
   },
   "outputs": [],
   "source": [
    "# way around weird behav\n",
    "# del evaluate\n",
    "# from engine_modif import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_loss_dict = [{k: loss.to('cpu') for k, loss in trans_loss_dict[0].items()}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKGZjyXBRVBn",
    "outputId": "017d37b3-eae9-48d1-aa8a-712c919ea9d9"
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "all_train_logs = []\n",
    "all_trans_valid_logs = []\n",
    "all_cis_valid_logs = []\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_logs = train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=100)\n",
    "    all_train_logs.append(train_logs)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "\n",
    "    for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            trans_loss_dict = model(images, targets)\n",
    "            trans_loss_dict = [{k: loss.to('cpu') for k, loss in trans_loss_dict.items()}]\n",
    "            all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "    for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cis_loss_dict = model(images, targets)\n",
    "            cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "            all_cis_valid_logs.append(cis_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'saved_models/model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model **NEED TO EVAL RIGHT AFTER**\n",
    "model.load_state_dict(torch.load('saved_models/model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[19].meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[19].meters['loss'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[19].meters['loss'].global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cis_valid_logs[0])\n",
    "print(\"total length:\", len(all_cis_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_cis_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trans_valid_logs[0])\n",
    "print(\"total length:\", len(all_trans_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_trans_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put train logs in json files\n",
    "There are multiple values coming from the MetricLogger so we can decide which we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the train logs from MetricLogger to string\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    print(type(logs))\n",
    "    for i in range(len(logs)):\n",
    "        n = len(logs[i].meters.items())\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = logs[i].meters[key].value\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the logs to string then to a json file\n",
    "all_train_logs_str = train_logs_to_lst(all_train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the logs in string format into a json file\n",
    "with open('jsons/train_logs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_train_logs_str, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put valid logs in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    for i in range(len(logs)):\n",
    "        logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "    return logs_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the valid logs from list of dictionnaries to string\n",
    "def valid_logs_to_lst(logs):\n",
    "    logs = merge_dict(logs)\n",
    "    lst = []\n",
    "    print(type(logs))\n",
    "    for i in range(len(logs)):\n",
    "        n = len(logs[i].items())\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the cis logs to string then to a json file\n",
    "all_cis_valid_logs_str = valid_logs_to_lst(all_cis_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the logs in string format into a json file\n",
    "with open('jsons/cis_valid_logs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_cis_valid_logs_str, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the trans logs to string then to a json file\n",
    "all_trans_valid_logs_str = valid_logs_to_lst(all_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the trans logs in string format into a json file\n",
    "with open('jsons/trans_valid_logs.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_trans_valid_logs_str, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats de l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[0].meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make better dict temporarily\n",
    "all_cis_valid_logs_better = merge_dict(all_cis_valid_logs)\n",
    "all_trans_valid_logs_better = merge_dict(all_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "o9jlXazqwHCv",
    "outputId": "a7c7c674-5462-4436-dd65-e1e5887c8bab"
   },
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "\n",
    "train_loss = []\n",
    "train_loss_box_reg = []\n",
    "train_loss_global_avg = []\n",
    "cis_valid_loss = []\n",
    "cis_valid_loss_box_reg = []\n",
    "trans_valid_loss = []\n",
    "trans_valid_loss_box_reg = []\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train_loss.append(all_train_logs[i].meters['loss'].value)\n",
    "    train_loss_box_reg.append(all_train_logs[i].meters['loss_box_reg'].value)\n",
    "\n",
    "    trans_valid_loss.append(all_trans_valid_logs_better[i]['loss_classifier'].numpy())\n",
    "    trans_valid_loss_box_reg.append(all_trans_valid_logs_better[i]['loss_box_reg'].numpy())\n",
    "\n",
    "    cis_valid_loss.append(all_cis_valid_logs_better[i]['loss_classifier'].numpy())\n",
    "    cis_valid_loss_box_reg.append(all_cis_valid_logs_better[i]['loss_box_reg'].numpy())\n",
    "\n",
    "ax[0].plot(np.arange(num_epochs),train_loss, label='train')\n",
    "ax[0].plot(np.arange(num_epochs),trans_valid_loss, label='trans valid')\n",
    "ax[0].plot(np.arange(num_epochs),cis_valid_loss, label='cis valid')\n",
    "ax[0].set_ylabel('loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(np.arange(num_epochs),train_loss_box_reg,label='train')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "train_loss_avg = []\n",
    "cis_valid_loss_avg = []\n",
    "trans_valid_loss_avg = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "To obtain predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred=model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[0][0].squeeze(),cmap=\"gray\")\n",
    "rect = patches.Rectangle((train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][1]), train_labels[0]['boxes'][0][2]-train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][3]-train_labels[0]['boxes'][0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "ax[1].imshow(train_features[0][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[0]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[0]['boxes'][i][0].cpu(), pred[0]['boxes'][i][1].cpu()), (pred[0]['boxes'][i][2]-pred[0]['boxes'][i][0]).cpu(), (pred[0]['boxes'][i][3]-pred[0]['boxes'][i][1]).cpu(), linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "#NMS\n",
    "iou_threshold=0.005 # param to tune\n",
    "boxes_to_keep=torchvision.ops.nms(pred[0]['boxes'],pred[0]['scores'],iou_threshold=iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[0][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "  rect = patches.Rectangle((pred[0]['boxes'][i][0].cpu(), pred[0]['boxes'][i][1].cpu()), (pred[0]['boxes'][i][2]-pred[0]['boxes'][i][0]).cpu(), (pred[0]['boxes'][i][3]-pred[0]['boxes'][i][1]).cpu(), linewidth=2, edgecolor='r', facecolor='none')\n",
    "  ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZDc5hxIvZoN"
   },
   "outputs": [],
   "source": [
    "# ' For evaluation, we consider a detected box to be correct if its IoU≥ 0.5 with a ground truth box.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7e4y2-nCND0"
   },
   "outputs": [],
   "source": [
    "# doesnt seem to change something to do this before the MAP \n",
    "pred[0]['boxes']=pred[0]['boxes'][boxes_to_keep]\n",
    "pred[0]['labels']=pred[0]['labels'][boxes_to_keep]\n",
    "pred[0]['scores']=pred[0]['scores'][boxes_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R--lgH4RXfG"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pjYowlRyHjRa",
    "outputId": "812bbc3c-3659-403d-eb41-bb7fe3f1f8eb"
   },
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision( max_detection_thresholds=[2])\n",
    "metric.update(pred, list(train_labels))\n",
    "from pprint import pprint\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaOCxXDmRmJd",
    "outputId": "27cee095-b172-43f3-fa06-af480c243c16"
   },
   "outputs": [],
   "source": [
    "model.roi_heads # original roi heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPLDTv-aPmSZ"
   },
   "outputs": [],
   "source": [
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (animal) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWhYPrH2d55v"
   },
   "outputs": [],
   "source": [
    "# We would prob need to change the size in \n",
    "# Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
    "\n",
    "\n",
    "## CEt Ligne est ecrit par Abdiel\n",
    "## Celle-ci est écrite par Kevin.\n",
    "\n",
    "# As well as the output size in the fc of the FastRCNN Predictor\n",
    "#(cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
    "#(bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
