{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "from PIL import Image\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports local modules downloaded from TorchVision repo v0.8.2, references/detection\n",
    "# https://github.com/pytorch/vision/tree/v0.8.2/references/detection\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from local lib files\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and initiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "# Set the paths to the annotation files that will retrieve the images with the split based on the annotations\n",
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'\n",
    "\n",
    "# Load the json files of the annotations for better exploring of each images\n",
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the device for pytorch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# Make and horizontal flip data transformation with 50% chance to use as data augmentation in a data loader\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "import torchvision.transforms as TorchTrans\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "colorTranformations = torch.nn.Sequential(\n",
    "                      TorchTrans.RandomInvert(1), # or 0.6\n",
    "                      TorchTrans.ColorJitter([.2,.3], [0.7,0.9], [.1,0.12])#jitter2 = T.ColorJitter([.2,.3], [0.7,0.9],  hue=.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "    \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "# Class used to create a custom dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"Gets and prints the spreadsheet's header columns\n",
    "\n",
    "    Args:\n",
    "        file_loc (str): The file location of the spreadsheet\n",
    "        print_cols (bool): A flag used to print the columns to the console\n",
    "            (default is False)\n",
    "\n",
    "    Returns:\n",
    "        list: a list of strings representing the header columns\n",
    "    \"\"\"\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None, rotation = False):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_img = valid_img\n",
    "        self.rotation = rotation  \n",
    "        self.rotate = T.RandomHorizontalFlip(0.5)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "        \n",
    "        \n",
    "        if self.rotation:\n",
    "            image, target= self.rotate(image, target)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "### Pre-trained models\n",
    "Inspred from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train (CNN layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "# Get a pretrained model and set to train the last layer (predictors (base) : model 1)\n",
    "def get_model_from_pretrained_base(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get a pretrained model and set to train the last 2 layers (ROI + predictors : model 2)\n",
    "def get_model_from_pretrained_roi(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get a pretrained model and set to train the last 3 layers (RPN + ROI + predictors : model 3)\n",
    "def get_model_from_pretrained_rpn(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a model based on a type preference between the 3 proposed\n",
    "def create_model(model_type, normalize, num_classes=2, milestones=[5, 10]):\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = num_classes\n",
    "\n",
    "    # get the model from the type we want using our helper function\n",
    "    if model_type==1 or model_type=='base':\n",
    "        model = get_model_from_pretrained_base(num_classes)\n",
    "    elif model_type==2 or model_type=='roi':\n",
    "        model = get_model_from_pretrained_roi(num_classes)\n",
    "    elif model_type==3 or model_type=='rpn':\n",
    "        model = get_model_from_pretrained_rpn(num_classes)\n",
    "    else:\n",
    "        print('Please select a valid model. 1:\"base\"- 2:\"roi\" - 3:\"rpn\"')\n",
    "        return None\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    \n",
    "    ## Mean and Std by chanel by pixel from the training set.  \n",
    "    if normalize:\n",
    "        model.transform.image_mean = [0.3321, 0.3406, 0.3210] # mean = [0.3321, 0.3406, 0.3210]\n",
    "        model.transform.image_std = [0.2359, 0.2369, 0.2313] # std = [0.2359, 0.2369, 0.2313]\n",
    "\n",
    "    # construct an SGD optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "    # like in the paper, construct the scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma=0.1)\n",
    "    \n",
    "    return model, optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "# Save the model, the optimizer and the scheduler into 3 separate files (~165MB)\n",
    "def save_model(file_name = time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    filename = file_name\n",
    "\n",
    "    torch.save(model.state_dict(), 'saved_models/' + filename + '_model.pt')\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/' + filename + '_optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), 'saved_models/' + filename + '_scheduler.pt')\n",
    "    print(\"Succesfully saved!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Load a model, an optimizer and a schduler into 3 different variables\n",
    "def load_model(model_type, model_type_file_name, num_classes=2, milestones=[5, 10]):\n",
    "    model, optimizer, lr_scheduler = create_model(model_type, num_classes, milestones)\n",
    "    \n",
    "    # load the model, the optimizer and the scheduler\n",
    "    model.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_model.pt'))\n",
    "    optimizer.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_scheduler.pt'))\n",
    "    \n",
    "    return model, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full/light dataloader with the full/light dataset\n",
    "def create_dataloader(ann_path, batch_size, light, transform=None, rotate=True, shuffle=True):\n",
    "    images_with_bbox,_ = get_img_with_bbox(ann_path)\n",
    "    if light:\n",
    "        index = np.random.choice(range(len(images_with_bbox)), 500)\n",
    "        images_with_bbox = [images_with_bbox[i] for i in index]\n",
    "    data = CustomImageDataset(ann_path, img_folder, images_with_bbox, transform, rotate)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_augment_dataloader(batch_size, light):\n",
    "    train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "    if light:\n",
    "        index = np.random.choice(range(len(train_valid_img)), 500)\n",
    "        train_valid_img = [train_valid_img[i] for i in index]\n",
    "    train_data = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, valid_img=train_valid_img)\n",
    "    train_data_colored = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, \n",
    "                                            # Transformations applied: transform=colorTranformations, rotate = True\n",
    "                                            valid_img=train_valid_img,transform=colorTranformations)\n",
    "    train_data_rotated = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, \n",
    "                                            valid_img=train_valid_img, rotation = True) \n",
    "\n",
    "    trainFinal = ConcatDataset([train_data, train_data_rotated, train_data_colored])\n",
    "    return DataLoader(trainFinal, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 'evaluate' fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the current model using the coco_evaluator passing through a test dataloader\n",
    "def evaluate(dataloader, coco, nms=True, iou=0.35):\n",
    "    apply_nms = nms\n",
    "    iou_threshold = iou # param to potentially tune (threshold for nms)\n",
    "    the_data_loader = dataloader # change to test set\n",
    "    \n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, targets in the_data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred=model(images)\n",
    "\n",
    "            if apply_nms:\n",
    "                boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "                pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "                pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "                pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "            outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "            res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "            coco_evaluator.update(res)\n",
    "    \n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    \n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the smoothed values to a dictionnary of each values\n",
    "def smoothed_value_to_str(smoothed_value):\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better\n",
    "    return None\n",
    "\n",
    "\n",
    "# Converts the valid logs from list of dictionnaries to string\n",
    "# TODO: add if type == list to not do anything if its already a list\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the cis validation logs into a json file with time dependent file name\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Puts the trans validation logs into a json file with time dependent file name\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part\n",
    "#### Parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data aumentation mode ('none', 'offline', 'online')\n",
    "data_augmentation_mode = 'none'\n",
    "\n",
    "# Set the model type (1:'predictors', 2:'ROI', 3:'RPN')\n",
    "model_depth = 3\n",
    "\n",
    "# Set the number of epochs \n",
    "# Time of training to expect (12909 train images ~ 23 minutes/epoch on GTX 1080 Ti, 1000 train images ~ 2.75 minutes/epoch)\n",
    "num_epochs = 10\n",
    "\n",
    "# If you use data augmentation, we use a specific normalization beforehand\n",
    "normalize = True\n",
    "if data_augmentation_mode =='none':\n",
    "    normalize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using lightweight mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lightweight mode\n"
     ]
    }
   ],
   "source": [
    "# Set the lightweight configuration mode to use subset of data, simpler architecture and few epochs\n",
    "# to quickly test the code for evaluation (False:0, True:1)\n",
    "lightweight_mode = 1\n",
    "\n",
    "if lightweight_mode:\n",
    "    light = 1\n",
    "    num_epochs = 10\n",
    "    print(\"Using lightweight mode\")\n",
    "else:\n",
    "    light = 0\n",
    "    print(\"Using non-lightweight mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if using the right device before training\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiation of the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison and the right options\n",
    "valid = True\n",
    "if data_augmentation_mode == 'none':\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, light)\n",
    "elif data_augmentation_mode == 'online':\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, light, transform=colorTranformations)\n",
    "elif data_augmentation_mode == 'offline':\n",
    "    train_dataloader = offline_augment_dataloader(1, light)\n",
    "else:\n",
    "    valid = False\n",
    "    print('Please enter a valid data_augmentation mode')\n",
    "\n",
    "if valid:\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10, light)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10, light)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10, light)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10, light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Loads the test dataset for coco evaluation later on (takes time)\n",
    "cis_coco = get_coco_api_from_dataset(cis_test_dataloader.dataset)\n",
    "trans_coco = get_coco_api_from_dataset(trans_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the model to create and the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BEFORE trying a second model in the same kernel, use this to clear the memory:\n",
    "\n",
    "# moddel = None\n",
    "# optimizer = None\n",
    "# lr_scheduler = None\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler = create_model(model_depth, normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/500]  eta: 0:01:18  lr: 0.000001  loss: 0.9548 (0.9548)  loss_classifier: 0.7870 (0.7870)  loss_box_reg: 0.1651 (0.1651)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0026 (0.0026)  time: 0.1570  data: 0.0325  max mem: 878\n",
      "Epoch: [0]  [100/500]  eta: 0:00:51  lr: 0.000061  loss: 0.2581 (0.4946)  loss_classifier: 0.1231 (0.3252)  loss_box_reg: 0.1042 (0.1223)  loss_objectness: 0.0289 (0.0403)  loss_rpn_box_reg: 0.0035 (0.0069)  time: 0.1278  data: 0.0302  max mem: 935\n",
      "Epoch: [0]  [200/500]  eta: 0:00:39  lr: 0.000121  loss: 0.2456 (0.3910)  loss_classifier: 0.1097 (0.2232)  loss_box_reg: 0.1296 (0.1289)  loss_objectness: 0.0020 (0.0332)  loss_rpn_box_reg: 0.0020 (0.0058)  time: 0.1336  data: 0.0335  max mem: 935\n",
      "Epoch: [0]  [300/500]  eta: 0:00:26  lr: 0.000181  loss: 0.2492 (0.3447)  loss_classifier: 0.0871 (0.1810)  loss_box_reg: 0.1142 (0.1276)  loss_objectness: 0.0043 (0.0303)  loss_rpn_box_reg: 0.0027 (0.0059)  time: 0.1358  data: 0.0338  max mem: 935\n",
      "Epoch: [0]  [400/500]  eta: 0:00:13  lr: 0.000241  loss: 0.2017 (0.3138)  loss_classifier: 0.0752 (0.1557)  loss_box_reg: 0.0940 (0.1212)  loss_objectness: 0.0054 (0.0311)  loss_rpn_box_reg: 0.0021 (0.0059)  time: 0.1440  data: 0.0391  max mem: 935\n",
      "Epoch: [0]  [499/500]  eta: 0:00:00  lr: 0.000300  loss: 0.1851 (0.2920)  loss_classifier: 0.0618 (0.1385)  loss_box_reg: 0.0938 (0.1172)  loss_objectness: 0.0154 (0.0304)  loss_rpn_box_reg: 0.0016 (0.0059)  time: 0.1408  data: 0.0373  max mem: 935\n",
      "Epoch: [0] Total time: 0:01:07 (0.1356 s / it)\n",
      "Epoch: [1]  [  0/500]  eta: 0:01:14  lr: 0.000300  loss: 0.1724 (0.1724)  loss_classifier: 0.0677 (0.0677)  loss_box_reg: 0.0968 (0.0968)  loss_objectness: 0.0070 (0.0070)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 0.1490  data: 0.0320  max mem: 4146\n",
      "Epoch: [1]  [100/500]  eta: 0:00:53  lr: 0.000300  loss: 0.1692 (0.1871)  loss_classifier: 0.0562 (0.0633)  loss_box_reg: 0.0931 (0.0963)  loss_objectness: 0.0061 (0.0225)  loss_rpn_box_reg: 0.0034 (0.0050)  time: 0.1323  data: 0.0340  max mem: 4146\n",
      "Epoch: [1]  [200/500]  eta: 0:00:40  lr: 0.000300  loss: 0.1428 (0.1829)  loss_classifier: 0.0496 (0.0634)  loss_box_reg: 0.0833 (0.0901)  loss_objectness: 0.0062 (0.0244)  loss_rpn_box_reg: 0.0018 (0.0050)  time: 0.1381  data: 0.0363  max mem: 4146\n",
      "Epoch: [1]  [300/500]  eta: 0:00:26  lr: 0.000300  loss: 0.1312 (0.1764)  loss_classifier: 0.0475 (0.0628)  loss_box_reg: 0.0658 (0.0858)  loss_objectness: 0.0025 (0.0229)  loss_rpn_box_reg: 0.0020 (0.0049)  time: 0.1328  data: 0.0332  max mem: 4146\n",
      "Epoch: [1]  [400/500]  eta: 0:00:13  lr: 0.000300  loss: 0.1298 (0.1718)  loss_classifier: 0.0445 (0.0608)  loss_box_reg: 0.0685 (0.0828)  loss_objectness: 0.0076 (0.0229)  loss_rpn_box_reg: 0.0017 (0.0053)  time: 0.1397  data: 0.0357  max mem: 4146\n",
      "Epoch: [1]  [499/500]  eta: 0:00:00  lr: 0.000300  loss: 0.1220 (0.1649)  loss_classifier: 0.0399 (0.0589)  loss_box_reg: 0.0556 (0.0800)  loss_objectness: 0.0095 (0.0209)  loss_rpn_box_reg: 0.0013 (0.0051)  time: 0.1392  data: 0.0361  max mem: 4146\n",
      "Epoch: [1] Total time: 0:01:07 (0.1354 s / it)\n",
      "Epoch: [2]  [  0/500]  eta: 0:01:04  lr: 0.000300  loss: 0.0963 (0.0963)  loss_classifier: 0.0233 (0.0233)  loss_box_reg: 0.0692 (0.0692)  loss_objectness: 0.0015 (0.0015)  loss_rpn_box_reg: 0.0023 (0.0023)  time: 0.1290  data: 0.0300  max mem: 4147\n",
      "Epoch: [2]  [100/500]  eta: 0:00:53  lr: 0.000300  loss: 0.1467 (0.1455)  loss_classifier: 0.0459 (0.0537)  loss_box_reg: 0.0654 (0.0668)  loss_objectness: 0.0054 (0.0198)  loss_rpn_box_reg: 0.0017 (0.0051)  time: 0.1316  data: 0.0327  max mem: 4147\n",
      "Epoch: [2]  [200/500]  eta: 0:00:39  lr: 0.000300  loss: 0.1455 (0.1486)  loss_classifier: 0.0588 (0.0557)  loss_box_reg: 0.0621 (0.0690)  loss_objectness: 0.0117 (0.0188)  loss_rpn_box_reg: 0.0022 (0.0052)  time: 0.1327  data: 0.0330  max mem: 4147\n",
      "Epoch: [2]  [300/500]  eta: 0:00:26  lr: 0.000300  loss: 0.1365 (0.1465)  loss_classifier: 0.0586 (0.0552)  loss_box_reg: 0.0599 (0.0678)  loss_objectness: 0.0149 (0.0186)  loss_rpn_box_reg: 0.0022 (0.0050)  time: 0.1315  data: 0.0325  max mem: 4147\n",
      "Epoch: [2]  [400/500]  eta: 0:00:13  lr: 0.000300  loss: 0.1215 (0.1467)  loss_classifier: 0.0369 (0.0555)  loss_box_reg: 0.0573 (0.0683)  loss_objectness: 0.0068 (0.0180)  loss_rpn_box_reg: 0.0011 (0.0048)  time: 0.1323  data: 0.0328  max mem: 4147\n",
      "Epoch: [2]  [499/500]  eta: 0:00:00  lr: 0.000300  loss: 0.1264 (0.1448)  loss_classifier: 0.0526 (0.0545)  loss_box_reg: 0.0620 (0.0678)  loss_objectness: 0.0064 (0.0177)  loss_rpn_box_reg: 0.0015 (0.0048)  time: 0.1309  data: 0.0320  max mem: 4147\n",
      "Epoch: [2] Total time: 0:01:06 (0.1323 s / it)\n",
      "Epoch: [3]  [  0/500]  eta: 0:01:03  lr: 0.000300  loss: 0.2365 (0.2365)  loss_classifier: 0.1331 (0.1331)  loss_box_reg: 0.0625 (0.0625)  loss_objectness: 0.0400 (0.0400)  loss_rpn_box_reg: 0.0009 (0.0009)  time: 0.1280  data: 0.0280  max mem: 4147\n",
      "Epoch: [3]  [100/500]  eta: 0:00:52  lr: 0.000300  loss: 0.1229 (0.1361)  loss_classifier: 0.0387 (0.0503)  loss_box_reg: 0.0602 (0.0674)  loss_objectness: 0.0072 (0.0139)  loss_rpn_box_reg: 0.0012 (0.0045)  time: 0.1368  data: 0.0337  max mem: 4147\n",
      "Epoch: [3]  [200/500]  eta: 0:00:41  lr: 0.000300  loss: 0.1268 (0.1386)  loss_classifier: 0.0448 (0.0521)  loss_box_reg: 0.0580 (0.0676)  loss_objectness: 0.0152 (0.0145)  loss_rpn_box_reg: 0.0018 (0.0044)  time: 0.1369  data: 0.0356  max mem: 4147\n",
      "Epoch: [3]  [300/500]  eta: 0:00:27  lr: 0.000300  loss: 0.1532 (0.1386)  loss_classifier: 0.0560 (0.0525)  loss_box_reg: 0.0668 (0.0662)  loss_objectness: 0.0067 (0.0159)  loss_rpn_box_reg: 0.0017 (0.0041)  time: 0.1430  data: 0.0365  max mem: 4147\n",
      "Epoch: [3]  [400/500]  eta: 0:00:13  lr: 0.000300  loss: 0.1281 (0.1385)  loss_classifier: 0.0411 (0.0521)  loss_box_reg: 0.0586 (0.0654)  loss_objectness: 0.0072 (0.0166)  loss_rpn_box_reg: 0.0034 (0.0044)  time: 0.1386  data: 0.0358  max mem: 4147\n",
      "Epoch: [3]  [499/500]  eta: 0:00:00  lr: 0.000300  loss: 0.1402 (0.1404)  loss_classifier: 0.0538 (0.0529)  loss_box_reg: 0.0780 (0.0659)  loss_objectness: 0.0094 (0.0170)  loss_rpn_box_reg: 0.0034 (0.0046)  time: 0.1352  data: 0.0340  max mem: 4147\n",
      "Epoch: [3] Total time: 0:01:09 (0.1391 s / it)\n",
      "Epoch: [4]  [  0/500]  eta: 0:01:12  lr: 0.000300  loss: 0.1126 (0.1126)  loss_classifier: 0.0573 (0.0573)  loss_box_reg: 0.0370 (0.0370)  loss_objectness: 0.0167 (0.0167)  loss_rpn_box_reg: 0.0016 (0.0016)  time: 0.1455  data: 0.0325  max mem: 4147\n",
      "Epoch: [4]  [100/500]  eta: 0:00:53  lr: 0.000300  loss: 0.1259 (0.1372)  loss_classifier: 0.0528 (0.0514)  loss_box_reg: 0.0673 (0.0650)  loss_objectness: 0.0060 (0.0151)  loss_rpn_box_reg: 0.0013 (0.0056)  time: 0.1349  data: 0.0342  max mem: 4147\n",
      "Epoch: [4]  [200/500]  eta: 0:00:40  lr: 0.000300  loss: 0.1350 (0.1382)  loss_classifier: 0.0422 (0.0519)  loss_box_reg: 0.0761 (0.0656)  loss_objectness: 0.0064 (0.0156)  loss_rpn_box_reg: 0.0033 (0.0051)  time: 0.1353  data: 0.0344  max mem: 4147\n",
      "Epoch: [4]  [300/500]  eta: 0:00:27  lr: 0.000300  loss: 0.1033 (0.1363)  loss_classifier: 0.0373 (0.0525)  loss_box_reg: 0.0482 (0.0653)  loss_objectness: 0.0043 (0.0140)  loss_rpn_box_reg: 0.0011 (0.0045)  time: 0.1351  data: 0.0340  max mem: 4147\n",
      "Epoch: [4]  [400/500]  eta: 0:00:13  lr: 0.000300  loss: 0.1144 (0.1334)  loss_classifier: 0.0412 (0.0508)  loss_box_reg: 0.0592 (0.0638)  loss_objectness: 0.0065 (0.0144)  loss_rpn_box_reg: 0.0011 (0.0044)  time: 0.1330  data: 0.0322  max mem: 4147\n",
      "Epoch: [4]  [499/500]  eta: 0:00:00  lr: 0.000300  loss: 0.1322 (0.1340)  loss_classifier: 0.0451 (0.0511)  loss_box_reg: 0.0497 (0.0636)  loss_objectness: 0.0103 (0.0150)  loss_rpn_box_reg: 0.0008 (0.0044)  time: 0.1368  data: 0.0344  max mem: 4147\n",
      "Epoch: [4] Total time: 0:01:07 (0.1355 s / it)\n",
      "Epoch: [5]  [  0/500]  eta: 0:01:00  lr: 0.000030  loss: 0.1159 (0.1159)  loss_classifier: 0.0586 (0.0586)  loss_box_reg: 0.0359 (0.0359)  loss_objectness: 0.0168 (0.0168)  loss_rpn_box_reg: 0.0045 (0.0045)  time: 0.1215  data: 0.0250  max mem: 4147\n",
      "Epoch: [5]  [100/500]  eta: 0:00:53  lr: 0.000030  loss: 0.1175 (0.1327)  loss_classifier: 0.0428 (0.0513)  loss_box_reg: 0.0553 (0.0631)  loss_objectness: 0.0103 (0.0141)  loss_rpn_box_reg: 0.0012 (0.0041)  time: 0.1302  data: 0.0313  max mem: 4147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5]  [200/500]  eta: 0:00:39  lr: 0.000030  loss: 0.0983 (0.1329)  loss_classifier: 0.0379 (0.0507)  loss_box_reg: 0.0556 (0.0624)  loss_objectness: 0.0096 (0.0152)  loss_rpn_box_reg: 0.0013 (0.0046)  time: 0.1312  data: 0.0328  max mem: 4147\n",
      "Epoch: [5]  [300/500]  eta: 0:00:26  lr: 0.000030  loss: 0.1167 (0.1328)  loss_classifier: 0.0375 (0.0510)  loss_box_reg: 0.0535 (0.0632)  loss_objectness: 0.0067 (0.0142)  loss_rpn_box_reg: 0.0021 (0.0044)  time: 0.1413  data: 0.0362  max mem: 4147\n",
      "Epoch: [5]  [400/500]  eta: 0:00:13  lr: 0.000030  loss: 0.1319 (0.1307)  loss_classifier: 0.0385 (0.0499)  loss_box_reg: 0.0610 (0.0619)  loss_objectness: 0.0099 (0.0149)  loss_rpn_box_reg: 0.0017 (0.0040)  time: 0.1384  data: 0.0365  max mem: 4147\n",
      "Epoch: [5]  [499/500]  eta: 0:00:00  lr: 0.000030  loss: 0.1265 (0.1310)  loss_classifier: 0.0430 (0.0499)  loss_box_reg: 0.0636 (0.0617)  loss_objectness: 0.0060 (0.0149)  loss_rpn_box_reg: 0.0018 (0.0044)  time: 0.1348  data: 0.0346  max mem: 4147\n",
      "Epoch: [5] Total time: 0:01:07 (0.1343 s / it)\n",
      "Epoch: [6]  [  0/500]  eta: 0:01:14  lr: 0.000030  loss: 0.1186 (0.1186)  loss_classifier: 0.0418 (0.0418)  loss_box_reg: 0.0717 (0.0717)  loss_objectness: 0.0046 (0.0046)  loss_rpn_box_reg: 0.0005 (0.0005)  time: 0.1480  data: 0.0340  max mem: 4147\n",
      "Epoch: [6]  [100/500]  eta: 0:00:53  lr: 0.000030  loss: 0.1190 (0.1305)  loss_classifier: 0.0334 (0.0495)  loss_box_reg: 0.0556 (0.0638)  loss_objectness: 0.0070 (0.0127)  loss_rpn_box_reg: 0.0024 (0.0045)  time: 0.1317  data: 0.0324  max mem: 4147\n",
      "Epoch: [6]  [200/500]  eta: 0:00:40  lr: 0.000030  loss: 0.1013 (0.1259)  loss_classifier: 0.0333 (0.0474)  loss_box_reg: 0.0486 (0.0611)  loss_objectness: 0.0050 (0.0129)  loss_rpn_box_reg: 0.0017 (0.0045)  time: 0.1463  data: 0.0378  max mem: 4147\n",
      "Epoch: [6]  [300/500]  eta: 0:00:27  lr: 0.000030  loss: 0.1141 (0.1282)  loss_classifier: 0.0383 (0.0489)  loss_box_reg: 0.0581 (0.0608)  loss_objectness: 0.0088 (0.0142)  loss_rpn_box_reg: 0.0023 (0.0043)  time: 0.1317  data: 0.0325  max mem: 4147\n",
      "Epoch: [6]  [400/500]  eta: 0:00:13  lr: 0.000030  loss: 0.1219 (0.1300)  loss_classifier: 0.0369 (0.0501)  loss_box_reg: 0.0664 (0.0608)  loss_objectness: 0.0051 (0.0148)  loss_rpn_box_reg: 0.0019 (0.0044)  time: 0.1331  data: 0.0325  max mem: 4147\n",
      "Epoch: [6]  [499/500]  eta: 0:00:00  lr: 0.000030  loss: 0.1300 (0.1317)  loss_classifier: 0.0529 (0.0504)  loss_box_reg: 0.0605 (0.0619)  loss_objectness: 0.0075 (0.0151)  loss_rpn_box_reg: 0.0014 (0.0044)  time: 0.1318  data: 0.0331  max mem: 4147\n",
      "Epoch: [6] Total time: 0:01:07 (0.1354 s / it)\n",
      "Epoch: [7]  [  0/500]  eta: 0:01:06  lr: 0.000030  loss: 0.2056 (0.2056)  loss_classifier: 0.0985 (0.0985)  loss_box_reg: 0.0487 (0.0487)  loss_objectness: 0.0523 (0.0523)  loss_rpn_box_reg: 0.0060 (0.0060)  time: 0.1325  data: 0.0320  max mem: 4147\n",
      "Epoch: [7]  [100/500]  eta: 0:00:54  lr: 0.000030  loss: 0.0953 (0.1177)  loss_classifier: 0.0343 (0.0449)  loss_box_reg: 0.0476 (0.0558)  loss_objectness: 0.0055 (0.0121)  loss_rpn_box_reg: 0.0016 (0.0049)  time: 0.1356  data: 0.0342  max mem: 4147\n",
      "Epoch: [7]  [200/500]  eta: 0:00:41  lr: 0.000030  loss: 0.1061 (0.1243)  loss_classifier: 0.0428 (0.0473)  loss_box_reg: 0.0532 (0.0593)  loss_objectness: 0.0063 (0.0131)  loss_rpn_box_reg: 0.0016 (0.0046)  time: 0.1390  data: 0.0366  max mem: 4147\n",
      "Epoch: [7]  [300/500]  eta: 0:00:27  lr: 0.000030  loss: 0.1201 (0.1260)  loss_classifier: 0.0380 (0.0478)  loss_box_reg: 0.0659 (0.0610)  loss_objectness: 0.0039 (0.0128)  loss_rpn_box_reg: 0.0023 (0.0045)  time: 0.1323  data: 0.0331  max mem: 4147\n",
      "Epoch: [7]  [400/500]  eta: 0:00:13  lr: 0.000030  loss: 0.0955 (0.1297)  loss_classifier: 0.0359 (0.0491)  loss_box_reg: 0.0482 (0.0612)  loss_objectness: 0.0083 (0.0147)  loss_rpn_box_reg: 0.0014 (0.0046)  time: 0.1344  data: 0.0333  max mem: 4147\n",
      "Epoch: [7]  [499/500]  eta: 0:00:00  lr: 0.000030  loss: 0.1145 (0.1296)  loss_classifier: 0.0438 (0.0495)  loss_box_reg: 0.0582 (0.0612)  loss_objectness: 0.0047 (0.0145)  loss_rpn_box_reg: 0.0015 (0.0043)  time: 0.1364  data: 0.0353  max mem: 4147\n",
      "Epoch: [7] Total time: 0:01:08 (0.1378 s / it)\n",
      "Epoch: [8]  [  0/500]  eta: 0:01:02  lr: 0.000030  loss: 0.0778 (0.0778)  loss_classifier: 0.0261 (0.0261)  loss_box_reg: 0.0473 (0.0473)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0042 (0.0042)  time: 0.1260  data: 0.0245  max mem: 4147\n",
      "Epoch: [8]  [100/500]  eta: 0:00:53  lr: 0.000030  loss: 0.1220 (0.1294)  loss_classifier: 0.0385 (0.0494)  loss_box_reg: 0.0586 (0.0604)  loss_objectness: 0.0069 (0.0153)  loss_rpn_box_reg: 0.0011 (0.0043)  time: 0.1343  data: 0.0344  max mem: 4147\n",
      "Epoch: [8]  [200/500]  eta: 0:00:40  lr: 0.000030  loss: 0.1151 (0.1308)  loss_classifier: 0.0385 (0.0513)  loss_box_reg: 0.0544 (0.0621)  loss_objectness: 0.0075 (0.0136)  loss_rpn_box_reg: 0.0012 (0.0039)  time: 0.1325  data: 0.0328  max mem: 4147\n",
      "Epoch: [8]  [300/500]  eta: 0:00:26  lr: 0.000030  loss: 0.0993 (0.1325)  loss_classifier: 0.0327 (0.0512)  loss_box_reg: 0.0484 (0.0627)  loss_objectness: 0.0076 (0.0141)  loss_rpn_box_reg: 0.0020 (0.0046)  time: 0.1394  data: 0.0345  max mem: 4147\n",
      "Epoch: [8]  [400/500]  eta: 0:00:13  lr: 0.000030  loss: 0.1019 (0.1339)  loss_classifier: 0.0380 (0.0516)  loss_box_reg: 0.0532 (0.0642)  loss_objectness: 0.0063 (0.0137)  loss_rpn_box_reg: 0.0014 (0.0044)  time: 0.1392  data: 0.0357  max mem: 4147\n",
      "Epoch: [8]  [499/500]  eta: 0:00:00  lr: 0.000030  loss: 0.0957 (0.1314)  loss_classifier: 0.0337 (0.0501)  loss_box_reg: 0.0368 (0.0624)  loss_objectness: 0.0071 (0.0146)  loss_rpn_box_reg: 0.0014 (0.0043)  time: 0.1361  data: 0.0354  max mem: 4147\n",
      "Epoch: [8] Total time: 0:01:07 (0.1359 s / it)\n",
      "Epoch: [9]  [  0/500]  eta: 0:01:07  lr: 0.000030  loss: 0.1029 (0.1029)  loss_classifier: 0.0618 (0.0618)  loss_box_reg: 0.0390 (0.0390)  loss_objectness: 0.0016 (0.0016)  loss_rpn_box_reg: 0.0005 (0.0005)  time: 0.1340  data: 0.0300  max mem: 4147\n",
      "Epoch: [9]  [100/500]  eta: 0:00:55  lr: 0.000030  loss: 0.0839 (0.1273)  loss_classifier: 0.0301 (0.0484)  loss_box_reg: 0.0487 (0.0645)  loss_objectness: 0.0067 (0.0116)  loss_rpn_box_reg: 0.0010 (0.0029)  time: 0.1325  data: 0.0329  max mem: 4147\n",
      "Epoch: [9]  [200/500]  eta: 0:00:40  lr: 0.000030  loss: 0.1202 (0.1247)  loss_classifier: 0.0452 (0.0465)  loss_box_reg: 0.0571 (0.0637)  loss_objectness: 0.0046 (0.0112)  loss_rpn_box_reg: 0.0016 (0.0033)  time: 0.1371  data: 0.0341  max mem: 4147\n",
      "Epoch: [9]  [300/500]  eta: 0:00:27  lr: 0.000030  loss: 0.1265 (0.1278)  loss_classifier: 0.0369 (0.0481)  loss_box_reg: 0.0489 (0.0623)  loss_objectness: 0.0074 (0.0132)  loss_rpn_box_reg: 0.0032 (0.0041)  time: 0.1408  data: 0.0355  max mem: 4147\n",
      "Epoch: [9]  [400/500]  eta: 0:00:14  lr: 0.000030  loss: 0.1130 (0.1280)  loss_classifier: 0.0414 (0.0488)  loss_box_reg: 0.0554 (0.0613)  loss_objectness: 0.0069 (0.0137)  loss_rpn_box_reg: 0.0025 (0.0042)  time: 0.1544  data: 0.0396  max mem: 4147\n",
      "Epoch: [9]  [499/500]  eta: 0:00:00  lr: 0.000030  loss: 0.1242 (0.1288)  loss_classifier: 0.0452 (0.0492)  loss_box_reg: 0.0537 (0.0608)  loss_objectness: 0.0133 (0.0145)  loss_rpn_box_reg: 0.0019 (0.0042)  time: 0.1382  data: 0.0355  max mem: 4147\n",
      "Epoch: [9] Total time: 0:01:10 (0.1401 s / it)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "n = len(train_logs)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(n):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(cis_valid_dataloader)):\n",
    "        loss_interm += cis_valid_logs[(len(cis_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # trans\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(trans_valid_dataloader)):\n",
    "        loss_interm += trans_valid_logs[(len(trans_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and valid Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, n + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, n + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, n + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on COCO metrics from data loaders\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU ≥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test with full dataloader\n",
    "cis_coco_evaluator = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on trans_test with full dataloader\n",
    "trans_coco_evaluator = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn + roi online data augmentation')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn + roi online data augmentation')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## (OPTIONAL) Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10 random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "# Loads 10 images and makes the model do predictions on these images\n",
    "# WARNING: Takes GPU ram space\n",
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.001 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 3 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(train_labels[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][1]), \n",
    "                            train_labels[image_i]['boxes'][i][2]-train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][3]-train_labels[image_i]['boxes'][i][1], \n",
    "                            linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some new imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports needed to use the method\n",
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utils method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X, center_row=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if center_row:\n",
    "        # center data per row\n",
    "        scaler_row = StandardScaler()\n",
    "        X_scaled_row = scaler_row.fit_transform(X.T)\n",
    "\n",
    "        # center data per column\n",
    "        scaler_col = StandardScaler()\n",
    "        X_scaled = scaler_col.fit_transform(X_scaled_row.T)\n",
    "        return X_scaled\n",
    "    else:\n",
    "        # center data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        \n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes))\n",
    "        \n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes * 4))\n",
    "        \n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the same model that we just trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can just load a model by uncommenting the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT THE FOLLOWING LINE TO LOAD A MODEL:\n",
    "# model, optimizer, lr_scheduler = load_model(3, \"10_rpn_roi_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of dimensions to keep in PCA\n",
    "d_pca = 512\n",
    "\n",
    "# Tell the program to save the matrix created\n",
    "save_matrixes = False\n",
    "save_projected_matrixes = True\n",
    "save_name = 'col_10_rpn_roi_online_512'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thres_IoU = 0.50\n",
    "count = 0\n",
    "\n",
    "X_source = torch.tensor([])\n",
    "bbox_idx = torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    # \n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    # \n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat = outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "    \n",
    "    bbox_idx_to_keep = torch.tensor([])\n",
    "    \n",
    "    # \n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs = bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "if save_matrixes:\n",
    "    torch.save(X_source, 'saved_matrixes/X_source_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data\n",
    "X_source_scaled = center_data(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=d_pca)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projected matrix\n",
    "if save_projected_matrixes:\n",
    "    torch.save(X_source_proj, 'saved_matrixes/X_source_proj_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison\n",
    "trans_test_batch1_dataloader = create_dataloader(trans_test_ann_path, 1, light=light)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 minutes\n",
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "if save_matrixes:\n",
    "    torch.save(X_target, 'saved_matrixes/X_target_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data\n",
    "X_target_scaled = center_data(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca_proj = PCA(n_components=d_pca)\n",
    "pca_proj.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca_proj.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_proj.explained_variance_ratio_) # we keep d dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projected matrix\n",
    "if save_projected_matrixes:\n",
    "    torch.save(X_target_proj, 'saved_matrixes/X_target_proj_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "𝑀 is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the device for pytorch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the matrixes on the right devices\n",
    "M.to(device)\n",
    "Xa.to(device)\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained_rpn(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/10_rpn_roi_4_model.pt'))\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "    print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of weights in the optimizer\n",
    "for i in range(len(optimizer.param_groups[0]['params'])):\n",
    "    print(optimizer.param_groups[0]['params'][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 15\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "cis_coco_evaluator_method = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans with method 3\n",
    "model.roi_heads.box_predictor.cls_score[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False) \n",
    "model.roi_heads.box_predictor.bbox_pred[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "trans_coco_evaluator_method = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn roi 4, method3.1 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn+roi 4, method3.1 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator_method.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
