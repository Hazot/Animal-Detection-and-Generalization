{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3RUQCnETVJhK"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from local lib files\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for evaluation from local lib files\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and initiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08r4QhagWuKZ"
   },
   "source": [
    "## Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5tYL7gDDW09R"
   },
   "outputs": [],
   "source": [
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qbuy1jLsWuew",
    "outputId": "7ba79560-5443-4fea-f46c-c0e3acaa1588",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cis test set length: 15827\n",
      "cis val set length: 3484\n",
      "train set length: 13553\n",
      "trans test set length: 23275\n",
      "trans val set length: 1725\n"
     ]
    }
   ],
   "source": [
    "print('cis test set length:', len(cis_test_ann['images']))\n",
    "print('cis val set length:', len(cis_val_ann['images']))\n",
    "print('train set length:', len(train_ann['images']))\n",
    "print('trans test set length:', len(trans_test_ann['images']))\n",
    "print('trans val set length:', len(trans_val_ann['images']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transforms as T\n",
    "# import torchvision.transforms as TorchTrans\n",
    "# # In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "# colorTranformations = torch.nn.Sequential(\n",
    "#                       TorchTrans.RandomInvert(0.6),\n",
    "#                       TorchTrans.ColorJitter([.2,.3], [0.8,0.9], [.1,0.12])#jitter2 = T.ColorJitter([.2,.3], [0.7,0.9],  hue=.1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomImageDataset(Dataset):\n",
    "#     def __init__(self, label_path, img_dir, valid_img, transform = None, rotation = False):\n",
    "#         self.label_file = json.load(open(label_path))\n",
    "#         self.img_dir = img_dir\n",
    "#         self.transform = transform\n",
    "#         self.valid_img = valid_img\n",
    "#         self.rotation = rotation  \n",
    "#         self.rotate= T.RandomHorizontalFlip(0.5)\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.valid_img)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "#         img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "#         image = read_image(img_path)\n",
    "\n",
    "#         conv = torchvision.transforms.ToTensor()\n",
    "#         # if image.shape[0]==1:\n",
    "#         # some images have only one channel, we convert them to rgb\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         image = conv(image)\n",
    "\n",
    "#         boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "#                  for j in range(len(self.label_file['annotations'])) \n",
    "#                  if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "#         label = [self.label_file['annotations'][j]['category_id'] \n",
    "#                  for j in range(len(self.label_file['annotations'])) \n",
    "#                  if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "#         # transform bbox coords to adjust for resizing\n",
    "#         scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "#         scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "#         boxes = torch.as_tensor(boxes)\n",
    "#         for i in range(boxes.shape[0]):\n",
    "#             boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "#             boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "#             boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "#             boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "#             boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "#             boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "#         label = torch.as_tensor(label)\n",
    "#         label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "#         image_id = torch.tensor([idx])\n",
    "#         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "#         iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "#         target = {}\n",
    "#         target[\"boxes\"] = boxes\n",
    "#         target[\"labels\"] = label\n",
    "#         target[\"image_id\"] = image_id\n",
    "#         target['area']=area\n",
    "#         target['iscrowd']=iscrowd\n",
    "#         # TO DO : resize all to same size\n",
    "#         if self.rotation:\n",
    "#             image, target= self.rotate(image, target)\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "## Pre-trained models\n",
    "Inspred from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with deeper layers to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with even deeper layers to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hbSuc8Jwc5qT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "# like in the paper, construct the scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # our dataset has two classes only - background and person\n",
    "# num_classes = 2\n",
    "\n",
    "# # get the model using our helper function\n",
    "# model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# ## Mean and Std by chanel by pixel from the training set.  \n",
    "# model.transform.image_mean = [0.3321, 0.3406, 0.3210] # mean = [0.3321, 0.3406, 0.3210]\n",
    "# model.transform.image_std = [0.2359, 0.2369, 0.2313] # std = [0.2359, 0.2369, 0.2313]\n",
    "\n",
    "# # move model to the right device\n",
    "# model.to(device)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "# # like in the paper, construct the scheduler\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL - Loading/Importing a model\n",
    "#### Need to initiate the model, the optimizer and de scheduler before loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NEED TO INITIATE THE MODEL, THE OPTIMIZER AND THE SCHEDULER BEFOREHAND (if )\n",
    "# load the model, the optimizer and the scheduler\n",
    "model.load_state_dict(torch.load('saved_models/50_rpn_roi_1_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/50_rpn_roi_1_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/50_rpn_roi_1_scheduler.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(ann_path, batch_size, shuffle=True, transform=None):\n",
    "    images_with_bbox,_ = get_img_with_bbox(ann_path)\n",
    "    data = CustomImageDataset(ann_path, img_folder, images_with_bbox, transform)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can specify the data augmentation transformation at will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_dataloader(train_ann_path, 1)\n",
    "cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10)\n",
    "trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10)\n",
    "cis_test_dataloader = create_dataloader(cis_test_ann_path, 10)\n",
    "trans_test_dataloader = create_dataloader(trans_test_ann_path, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 'evaluate' fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the test datasetfor coco evaluation\n",
    "cis_coco = get_coco_api_from_dataset(cis_test_dataloader.dataset)\n",
    "trans_coco = get_coco_api_from_dataset(trans_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: méthode pour évaluer \n",
    "def evaluate(dataloader, coco, nms=True, iou=0.35):\n",
    "    apply_nms = nms\n",
    "    iou_threshold = iou # param to potentially tune (threshold for nms)\n",
    "    the_data_loader = dataloader # change to test set\n",
    "    \n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, targets in the_data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred=model(images)\n",
    "\n",
    "            if apply_nms:\n",
    "                boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "                pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "                pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "                pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "            outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "            res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "            coco_evaluator.update(res)\n",
    "    \n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the smoothed values to a dictionnary of each values\n",
    "def smoothed_value_to_str(smoothed_value):\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values\n",
    "\n",
    "\n",
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the valid logs from list of dictionnaries to string\n",
    "# TODO: add if type == list to not do anything if its already a list\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the cis validation logs into a json file with time dependent file name\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the trans validation logs into a json file with time dependent file name\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train function\n",
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 50\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the last training logs to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL - Saving manually every logs from training to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the log with the same time\n",
    "train_logs_to_json(last_train_logs)\n",
    "trans_valid_logs_to_json(last_trans_valid_logs)\n",
    "cis_valid_logs_to_json(last_cis_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL - Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model, the optimizer and the scheduler\n",
    "# filetime = \"25_epochs_roi_3_augment_method_3\"\n",
    "filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the raw logs\n",
    "##### Only look at the MetricLogger if you just trained the model. You cannot import the model and then check the MetricLogger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[0].meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[0].meters['loss_box_reg'].global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we check the amount of logs per epoch for each categories and the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cis_valid_logs[0])\n",
    "print(\"total length:\", len(all_cis_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_cis_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trans_valid_logs[0])\n",
    "print(\"total length:\", len(all_trans_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_trans_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at/Loading the logs in convenient ways\n",
    "Here we define the variables \"train_logs\", \"cis_valid_logs\" and \"trans_valid_logs\" that will be used in the methods for the results and the visualisations.\n",
    "\n",
    "We can import logs or use the ones from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL - Can load some logs right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported logs - format: name = \"NAME_OR_TIME\"      Exemple file format: \"NAME_OR_TIME_train_logs\"\n",
    "\n",
    "file_time_or_nickname = '10_roi' # VALUE TO CHANGE TO THE IMPORTED FILES\n",
    "\n",
    "# Import training logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_train_logs.json', \"r\") as f:\n",
    "    train_logs = json.load(f)\n",
    "\n",
    "# Import cis valid logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_cis_valid_logs.json', \"r\") as f:\n",
    "    cis_valid_logs = json.load(f)\n",
    "\n",
    "# Import trans valid logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_trans_valid_logs.json', \"r\") as f:\n",
    "    trans_valid_logs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the last trained logs into convenient list variables\n",
    "#### (USE THIS CELL ONLY IF MODEL HAVE BEEN TRAINED IN THIS KERNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "len(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = len(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_valid_logs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_rpn_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # trans\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_rpn_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10 random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "# Loads 10 images and makes the model do predictions on these images\n",
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "# TODO: faire des méthodes pour simplifier le code\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.001 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val_ann['images'][train_labels[3]['image_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 3 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(train_labels[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][1]), \n",
    "                            train_labels[image_i]['boxes'][i][2]-train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][3]-train_labels[image_i]['boxes'][i][1], \n",
    "                            linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[image_i]['boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[image_i]['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on COCO metrics from data loaders\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU ≥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "cis_coco_evaluator = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test\n",
    "trans_coco_evaluator = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 50 epochs rpn + roi 2')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans test 50 epochs rpn + roi 2')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers \n",
    "\n",
    " 1. https://arxiv.org/pdf/1507.05578.pdf\n",
    "\n",
    " 2.  https://openaccess.thecvf.com/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 20 minutes\n",
    "thres_IoU = 0.50\n",
    "count = 0\n",
    "\n",
    "X_source = torch.tensor([])\n",
    "bbox_idx = torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat = outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "\n",
    "    bbox_idx_to_keep = torch.tensor([])\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs = bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source, 'saved_data/X_source_05_50_rpn_roi_1_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_source_scaled = scaler.fit_transform(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source_proj, 'saved_data/X_source_proj_05_50_rpn_roi_1_100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data/distribution = trans test set - Batch Size 1\n",
    "trans_test_batch1_img,_ = get_img_with_bbox(trans_test_ann_path)\n",
    "trans_test_batch1_data = CustomImageDataset(trans_test_ann_path, img_folder, trans_test_batch1_img)\n",
    "trans_test_batch1_dataloader = DataLoader(trans_test_batch1_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 30 minutes\n",
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target, 'saved_data/X_target_05_50_rpn_roi_1_100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_target_scaled = scaler.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca_proj = PCA(n_components=100)\n",
    "pca_proj.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca_proj.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_proj.explained_variance_ratio_) # we keep 100 dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_target_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target_proj, 'saved_data/X_target_proj_05_50_rpn_roi_1_100.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "𝑀 is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x Xa (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet target data in target subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x X_target_proj.T (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load because it takes time to generate the following matrices so they are saved\n",
    "X_source_proj = torch.load('saved_data/X_source_proj_05_50_rpn_roi_1_512.pt')\n",
    "X_target_proj = torch.load('saved_data/X_target_proj_05_50_rpn_roi_1_512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source_proj.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) # transformation matrix\n",
    "\n",
    "Xa = torch.matmul(X_source_proj.T,M) # target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FastRCNNPredictor_custom(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Standard classification + bounding box regression layers\n",
    "#     for Fast R-CNN.\n",
    "\n",
    "#     Args:\n",
    "#         in_channels (int): number of input channels\n",
    "#         num_classes (int): number of output classes (including background)\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, in_channels, num_classes, m_transfo):\n",
    "#         super(FastRCNNPredictor_custom, self).__init__()\n",
    "#         self.cls_score = nn.Sequential(nn.Linear(in_features = 1024, out_features = 100, bias=False), nn.Linear(in_channels, num_classes))\n",
    "#         self.bbox_pred = nn.Sequential(nn.Linear(in_features = 1024, out_features = 100, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "#         self.cls_score[0].weight= nn.Parameter(m_transfo, requires_grad = False)\n",
    "#         self.bbox_pred[0].weight= nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if x.dim() == 4:\n",
    "#             assert list(x.shape[2:]) == [1, 1]\n",
    "#         x = x.flatten(start_dim=1)\n",
    "#         scores = self.cls_score(x)\n",
    "#         bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "#         return scores, bbox_deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False),nn.Linear(in_channels, num_classes))\n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_model.pt'))\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "    print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of weights in the optimizer\n",
    "for i in range(len(optimizer.param_groups[0]['params'])):\n",
    "    print(optimizer.param_groups[0]['params'][i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 25\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "cis_coco_evaluator_method = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans with method 3\n",
    "model.roi_heads.box_predictor.cls_score[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False) \n",
    "model.roi_heads.box_predictor.bbox_pred[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "trans_coco_evaluator_method = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 50 epochs rpn+roi, method3.2 with 25 epochs & d=512')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('trans test 50 epochs rpn+roi, method3.2 with 25 epochs & d=512')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a model with Method 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box regression layers\n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False),nn.Linear(in_channels, num_classes))\n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, out_features = in_channels, bias=False), nn.Linear(in_channels, num_classes * 4))\n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/50_rpn_roi_1_method3.2_512_scheduler.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
