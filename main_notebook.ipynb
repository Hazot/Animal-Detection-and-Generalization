{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "from PIL import Image\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports local modules downloaded from TorchVision repo v0.8.2, references/detection\n",
    "# https://github.com/pytorch/vision/tree/v0.8.2/references/detection\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports from local lib files\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and initiations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "# Set the paths to the annotation files that will retrieve the images with the split based on the annotations\n",
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'\n",
    "\n",
    "# Load the json files of the annotations for better exploring of each images\n",
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the device for pytorch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# Make and horizontal flip data transformation with 50% chance to use as data augmentation in a data loader\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "import torchvision.transforms as TorchTrans\n",
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "colorTranformations = torch.nn.Sequential(\n",
    "                      TorchTrans.RandomInvert(1), # or 0.6\n",
    "                      #jitter2 = T.ColorJitter([.2,.3], [0.7,0.9],  hue=.1)\n",
    "                      TorchTrans.ColorJitter([.2,.3], [0.7,0.9], [.1,0.12])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_with_bbox(file_path):\n",
    "    \"\"\"\n",
    "    Method that returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "    list with the number of bbox for each valid image (num_bbox).\n",
    "\n",
    "    Args:\n",
    "        file_path (string): path to the annotation file\n",
    "\n",
    "    Returns:\n",
    "        list: index in the annotations of the images containing bounding boxes\n",
    "        int: number of bounding boxes for each image in the dict according to the indexes\n",
    "    \"\"\"\n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "# Class used to create a custom dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class used to create a dataset of images to use for the DataLoader class of pytorch\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None, rotation = False):\n",
    "        \"\"\"\n",
    "        Initialize the custom image dataset with some parameters\n",
    "\n",
    "        Args:\n",
    "            label_path (string): path to the annotations\n",
    "            img_dir (str): path to the images directory\n",
    "            valid_img (list): indexes of the valid images\n",
    "            transform (Sequential, optional): transformation to apply to the dataset. Defaults to None.\n",
    "            rotation (bool, optional): if true applies a horizontal flip with proba 0.5 . Defaults to False.\n",
    "        \"\"\"\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_img = valid_img\n",
    "        self.rotation = rotation  \n",
    "        self.rotate = T.RandomHorizontalFlip(0.5)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images with bounding boxes in the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: length of the number of image in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Used to get images from the dataset directly from the image directory without loading them into memory\n",
    "\n",
    "        Args:\n",
    "            idx (list or int): index of the images to get (used only by the dataloader)\n",
    "\n",
    "        Returns:\n",
    "            tensor: images in tensor used by the dataloader (dataloader magic)\n",
    "            dict: information about the tensor images and the bounding boxes of the ground truth (dataloader magic)\n",
    "        \"\"\"\n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "        \n",
    "        \n",
    "        if self.rotation:\n",
    "            image, target= self.rotate(image, target)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "### Pre-trained models\n",
    "Inspred from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train (CNN layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "def get_model_from_pretrained_base(num_classes):\n",
    "    \"\"\"\n",
    "    Gives a pretrained model and sets to train the only \n",
    "    the last layer which are the predictors (base : model 1)\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        FasterRCNN: model which is a FasterRCNN from pytorch detection models\n",
    "    \"\"\"\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_from_pretrained_roi(num_classes):\n",
    "    \"\"\"\n",
    "    Gives a pretrained model and sets to train the only \n",
    "    the last 2 layers (ROI + predictors : model 2)\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        FasterRCNN: model which is a FasterRCNN from pytorch detection models\n",
    "    \"\"\"\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_from_pretrained_rpn(num_classes):\n",
    "    \"\"\"\n",
    "    Gives a pretrained model and sets to train the only \n",
    "    the last 3 layers (RPN + ROI + predictors : model 3)\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        FasterRCNN: model which is a FasterRCNN from pytorch detection models\n",
    "    \"\"\"\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.rpn.parameters():\n",
    "\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(model_type, normalize, num_classes=2, milestones=[5, 10]):\n",
    "    \"\"\"\n",
    "    Creates a model based on a type preference between the 3 proposed\n",
    "\n",
    "    Args:\n",
    "        model_type (int or string): type of model according to the depth\n",
    "        normalize (boolean): choose to normalize with precalculated numbers the first layers of the model\n",
    "        num_classes (int, optional): number of classes. Defaults to 2.\n",
    "        milestones (list, optional): number of epochs before dividing the learning rate by 10. Defaults to [5, 10].\n",
    "\n",
    "    Returns:\n",
    "        FasterRCNN: model which is a FasterRCNN from pytorch detection models\n",
    "        SGD: optimizer for the model\n",
    "        MultiStepLR: scheduler for the model\n",
    "    \"\"\"\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = num_classes\n",
    "\n",
    "    # get the model from the type we want using our helper function\n",
    "    if model_type==1 or model_type=='base':\n",
    "        model = get_model_from_pretrained_base(num_classes)\n",
    "    elif model_type==2 or model_type=='roi':\n",
    "        model = get_model_from_pretrained_roi(num_classes)\n",
    "    elif model_type==3 or model_type=='rpn':\n",
    "        model = get_model_from_pretrained_rpn(num_classes)\n",
    "    else:\n",
    "        print('Please select a valid model. 1:\"base\"- 2:\"roi\" - 3:\"rpn\"')\n",
    "        return None\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "    \n",
    "    ## Mean and Std by chanel by pixel from the training set.  \n",
    "    if normalize:\n",
    "        model.transform.image_mean = [0.3321, 0.3406, 0.3210] # mean = [0.3321, 0.3406, 0.3210]\n",
    "        model.transform.image_std = [0.2359, 0.2369, 0.2313] # std = [0.2359, 0.2369, 0.2313]\n",
    "\n",
    "    # construct an SGD optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "    # like in the paper, construct the scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma=0.1)\n",
    "    \n",
    "    return model, optimizer, lr_scheduler\n",
    "\n",
    "\n",
    "# Save the model, the optimizer and the scheduler into 3 separate files (~165MB)\n",
    "def save_model(file_name = time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \"\"\"\n",
    "    Saves the model, the optimizer and the scheduler into 3 separate files (~165MB)\n",
    "\n",
    "    Args:\n",
    "        file_name (string, optional): to choose a file name that you like. Defaults to time.strftime(\"%Y%m%d_%H%M%S\").\n",
    "\n",
    "    Returns:\n",
    "        None: nothing if the method is successful\n",
    "    \"\"\"\n",
    "    filename = file_name\n",
    "\n",
    "    torch.save(model.state_dict(), 'saved_models/' + filename + '_model.pt')\n",
    "    torch.save(optimizer.state_dict(), 'saved_models/' + filename + '_optimizer.pt')\n",
    "    torch.save(lr_scheduler.state_dict(), 'saved_models/' + filename + '_scheduler.pt')\n",
    "    print(\"Succesfully saved!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_model(model_type, model_type_file_name, num_classes=2, milestones=[5, 10]):\n",
    "    \"\"\"\n",
    "    Loads a model based on a type preference between the 3 proposed\n",
    "\n",
    "    Args:\n",
    "        model_type (int or string): type of model according to the depth\n",
    "        model_type_file_name (string): name of the files corresponding to the same model\n",
    "        num_classes (int, optional): number of classes. Defaults to 2.\n",
    "        milestones (list, optional): number of epochs before dividing the learning rate by 10. Defaults to [5, 10].\n",
    "\n",
    "    Returns:\n",
    "        FasterRCNN: model which is a FasterRCNN from pytorch detection models\n",
    "        SGD: optimizer for the model\n",
    "        MultiStepLR: scheduler for the model\n",
    "    \"\"\"\n",
    "    model, optimizer, lr_scheduler = create_model(model_type, num_classes, milestones)\n",
    "    \n",
    "    # load the model, the optimizer and the scheduler\n",
    "    model.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_model.pt'))\n",
    "    optimizer.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_optimizer.pt'))\n",
    "    lr_scheduler.load_state_dict(torch.load('saved_models/' + model_type_file_name + '_scheduler.pt'))\n",
    "    \n",
    "    return model, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.container.Sequential"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(colorTranformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(ann_path, batch_size, light, transform=None, rotate=True, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates the full/light dataloader with the full/light dataset using CustomImageDataset\n",
    "\n",
    "    Args:\n",
    "        ann_path (string): path to the annotation file\n",
    "        batch_size (int): batch size to use before each step\n",
    "        light (boolean): choose between lightweight dataset or full dataset\n",
    "        transform (Sequential, optional): transformation to apply to the dataset. Defaults to None.\n",
    "        rotate (bool, optional): if true applies a horizontal flip with probability 0.5 . Defaults to True.\n",
    "        shuffle (bool, optional): shuffle the order of the indexes used in the dataset. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: used to feed the data efficiently to the model while training or evaluating\n",
    "    \"\"\"\n",
    "    images_with_bbox,_ = get_img_with_bbox(ann_path)\n",
    "    if light:\n",
    "        index = np.random.choice(range(len(images_with_bbox)), 500)\n",
    "        images_with_bbox = [images_with_bbox[i] for i in index]\n",
    "    data = CustomImageDataset(ann_path, img_folder, images_with_bbox, transform, rotate)\n",
    "    return DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_augment_dataloader(batch_size, light):\n",
    "    \"\"\"\n",
    "    Creates the full/light training dataloader for the offline data augmentation technique\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): batch size to use before each step\n",
    "        light (boolean): choose between lightweight dataset or full dataset\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "        DataLoader: used to feed the data efficiently to the model while training or evaluating\n",
    "    \"\"\"\n",
    "    train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "    if light:\n",
    "        index = np.random.choice(range(len(train_valid_img)), 500)\n",
    "        train_valid_img = [train_valid_img[i] for i in index]\n",
    "    train_data = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, valid_img=train_valid_img)\n",
    "    train_data_colored = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, \n",
    "                                            # Transformations applied: transform=colorTranformations, rotate = True\n",
    "                                            valid_img=train_valid_img,transform=colorTranformations)\n",
    "    train_data_rotated = CustomImageDataset(label_path=train_ann_path, img_dir=img_folder, \n",
    "                                            valid_img=train_valid_img, rotation = True) \n",
    "\n",
    "    trainFinal = ConcatDataset([train_data, train_data_rotated, train_data_colored])\n",
    "    return DataLoader(trainFinal, batch_size=batch_size, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the 'evaluate' fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader, coco, nms=True, iou=0.35):\n",
    "    \"\"\"\n",
    "    Evaluates the current model using the coco_evaluator passing through a test dataloader\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): image feeder to the model coresponding to the cis or trans datasets\n",
    "        coco (COCO): coco dataset associated to the vis or trans dataloader\n",
    "        nms (bool, optional): decides to apply nms. Defaults to True.\n",
    "        iou (float, optional): iou threshold before used before evaluation to eliminate some of predictions. \n",
    "                               Defaults to 0.35.\n",
    "\n",
    "    Returns:\n",
    "        coco_evaluator: coco evaluator class that stores the test's information into a summary and gives useful methods.\n",
    "    \"\"\"\n",
    "    apply_nms = nms\n",
    "    iou_threshold = iou # param to potentially tune (threshold for nms)\n",
    "    the_data_loader = dataloader # change to test set\n",
    "    \n",
    "    iou_types = _get_iou_types(model)\n",
    "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for images, targets in the_data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred=model(images)\n",
    "\n",
    "            if apply_nms:\n",
    "                boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "                pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "                pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "                pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "            outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "            res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "            coco_evaluator.update(res)\n",
    "    \n",
    "    coco_evaluator.synchronize_between_processes()\n",
    "    coco_evaluator.accumulate()\n",
    "    coco_evaluator.summarize()\n",
    "    \n",
    "    return coco_evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_value_to_str(smoothed_value):\n",
    "    \"\"\"\n",
    "    Converts the smoothed values to a dictionnary of each values\n",
    "\n",
    "    Args:\n",
    "        smoothed_value (smoothed_value): track a series of values and provide easy access to various values\n",
    "\n",
    "    Returns:\n",
    "        dict: dict corresponding to every type of smoothed_values\n",
    "    \"\"\"\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values\n",
    "\n",
    "\n",
    "def train_logs_to_lst(logs):\n",
    "    \"\"\"\n",
    "    Converts the train logs from MetricLogger to a list\n",
    "\n",
    "    Args:\n",
    "        logs (dict): dict containing every MetricLogger metrics and it's smoothed values\n",
    "\n",
    "    Returns:\n",
    "        list: every train logs matrics in an easier to use format which is a list\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \"\"\"\n",
    "    Puts the training logs into a json file with time dependent file name\n",
    "\n",
    "    Args:\n",
    "        logs (MetricLogger): dictionnary storing specific loss metrics with smoothed values for each\n",
    "        ftime (string, optional): prefix name to add to the json file name. Defaults to time.strftime(\"%Y%m%d_%H%M%S\").\n",
    "        \n",
    "    Returns:\n",
    "        None: nothing if the method is successful\n",
    "    \"\"\"\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dict(logs):\n",
    "    \"\"\"\n",
    "    Merge the dicts of a dict together. It's used to merge dictionnaries containing a single key and value from the\n",
    "    smoothed values together to form a bigger dictionnnary containing all the keys and the values corresponding to the\n",
    "    smoothed values. We then get a list of dictionnaries.\n",
    "\n",
    "    Args:\n",
    "        logs (dict): dictionnary smaller dictionnary containing only a single key and value\n",
    "\n",
    "    Returns:\n",
    "        list: list of dictionnaries containing smoothed values\n",
    "    \"\"\"\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better\n",
    "    return None\n",
    "\n",
    "\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    \"\"\"\n",
    "    Converts the smoothed values inside the dictionnary to numpy float\n",
    "\n",
    "    Args:\n",
    "        valid_logs (list): validation logs from the training inside a dictionnary\n",
    "\n",
    "    Returns:\n",
    "        list: list of dictionnaries containing numpy float\n",
    "    \"\"\"\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst\n",
    "\n",
    "\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \"\"\"\n",
    "    Puts the cis validation logs into a json file with time dependent file name\n",
    "\n",
    "    Args:\n",
    "        logs (list): list of dictionnary containing smoothed values\n",
    "        ftime (string, optional): prefix name to add to the json file name. Defaults to time.strftime(\"%Y%m%d_%H%M%S\").\n",
    "\n",
    "    Returns:\n",
    "        None: nothing if the method is successful\n",
    "    \"\"\"\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None\n",
    "\n",
    "\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    \"\"\"\n",
    "    Puts the trans validation logs into a json file with time dependent file name\n",
    "\n",
    "    Args:\n",
    "        logs (list): list of dictionnary containing smoothed values\n",
    "        ftime (string, optional): prefix name to add to the json file name. Defaults to time.strftime(\"%Y%m%d_%H%M%S\").\n",
    "\n",
    "    Returns:\n",
    "        None: nothing if the method is successful\n",
    "    \"\"\"\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \"\"\"\n",
    "    Trains the parameters of the model using the optimizer and the scheduler\n",
    "\n",
    "    Args:\n",
    "        dataloader (DataLoader): image feeder to the model coresponding to the cis or trans datasets\n",
    "        num_epochs (int): number of epochs for the training\n",
    "        save_logs (bool, optional): decides if the logs are saved. Defaults to True.\n",
    "        save_model (bool, optional): decides if the model, the optimizer and the scheduler are saved. Defaults to True.\n",
    "        print_freq (int, optional): choose the print frequency. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        list: list containing the train logs which uses the MetricLogger as a dictionnary with smoothed values\n",
    "        list: 2 lists containing the cis and trans valid logs which only uses smoothed values\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Part\n",
    "#### Parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data aumentation mode ('none', 'offline', 'online')\n",
    "data_augmentation_mode = 'none'\n",
    "\n",
    "# Set the model type (1:'predictors', 2:'ROI', 3:'RPN')\n",
    "model_depth = 3\n",
    "\n",
    "# Set the number of epochs \n",
    "# Time of training to expect (12909 train images ~ 23 minutes/epoch on GTX 1080 Ti, 1000 train images ~ 2.75 minutes/epoch)\n",
    "num_epochs = 10\n",
    "\n",
    "# If you use data augmentation, we use a specific normalization beforehand\n",
    "normalize = True\n",
    "if data_augmentation_mode =='none':\n",
    "    normalize = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using lightweight mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lightweight mode\n"
     ]
    }
   ],
   "source": [
    "# Set the lightweight configuration mode to use subset of data, simpler architecture and few epochs\n",
    "# to quickly test the code for evaluation (False:0, True:1)\n",
    "lightweight_mode = 1\n",
    "\n",
    "if lightweight_mode:\n",
    "    light = 1\n",
    "    num_epochs = 10\n",
    "    print(\"Using lightweight mode\")\n",
    "else:\n",
    "    light = 0\n",
    "    print(\"Using non-lightweight mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if using the right device before training\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiation of the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomImageDataset'>\n",
      "<class '__main__.CustomImageDataset'>\n",
      "<class '__main__.CustomImageDataset'>\n",
      "<class '__main__.CustomImageDataset'>\n",
      "<class '__main__.CustomImageDataset'>\n"
     ]
    }
   ],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison and the right options\n",
    "valid = True\n",
    "if data_augmentation_mode == 'none':\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, light)\n",
    "elif data_augmentation_mode == 'online':\n",
    "    train_dataloader = create_dataloader(train_ann_path, 1, light, transform=colorTranformations)\n",
    "elif data_augmentation_mode == 'offline':\n",
    "    train_dataloader = offline_augment_dataloader(1, light)\n",
    "else:\n",
    "    valid = False\n",
    "    print('Please enter a valid data_augmentation mode')\n",
    "\n",
    "if valid:\n",
    "    cis_valid_dataloader = create_dataloader(cis_val_ann_path, 10, light)\n",
    "    trans_valid_dataloader = create_dataloader(trans_val_ann_path, 10, light)\n",
    "    cis_test_dataloader = create_dataloader(cis_test_ann_path, 10, light)\n",
    "    trans_test_dataloader = create_dataloader(trans_test_ann_path, 10, light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method COCO.info of <pycocotools.coco.COCO object at 0x000002818FEFC348>>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cis_coco.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Loads the test dataset for coco evaluation later on (takes time)\n",
    "cis_coco = get_coco_api_from_dataset(cis_test_dataloader.dataset)\n",
    "trans_coco = get_coco_api_from_dataset(trans_test_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the model to create and the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BEFORE trying a second model in the same kernel, use this to clear the memory:\n",
    "\n",
    "# moddel = None\n",
    "# optimizer = None\n",
    "# lr_scheduler = None\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, lr_scheduler = create_model(model_depth, normalize=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "n = len(train_logs)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(n):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(cis_valid_dataloader)):\n",
    "        loss_interm += cis_valid_logs[(len(cis_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # trans\n",
    "\n",
    "for i in range(n):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(trans_valid_dataloader)):\n",
    "        loss_interm += trans_valid_logs[(len(trans_valid_dataloader) * i) + j]['loss_rpn_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and valid Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, n + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, n + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, n + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on COCO metrics from data loaders\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU â‰¥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on cis_test with full dataloader\n",
    "cis_coco_evaluator = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# takes +- 25min to run on trans_test with full dataloader\n",
    "trans_coco_evaluator = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn + roi online data augmentation')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn + roi online data augmentation')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## (OPTIONAL) Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 10 random predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "# Loads 10 images and makes the model do predictions on these images\n",
    "# WARNING: Takes GPU ram space\n",
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.001 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 3 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(train_labels[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][1]), \n",
    "                            train_labels[image_i]['boxes'][i][2]-train_labels[image_i]['boxes'][i][0], \n",
    "                            train_labels[image_i]['boxes'][i][3]-train_labels[image_i]['boxes'][i][1], \n",
    "                            linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some new imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More imports needed to use the method\n",
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some utils method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(X, center_row=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if center_row:\n",
    "        # center data per row\n",
    "        scaler_row = StandardScaler()\n",
    "        X_scaled_row = scaler_row.fit_transform(X.T)\n",
    "\n",
    "        # center data per column\n",
    "        scaler_col = StandardScaler()\n",
    "        X_scaled = scaler_col.fit_transform(X_scaled_row.T)\n",
    "        return X_scaled\n",
    "    else:\n",
    "        # center data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRCNNPredictor_custom(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification and bounding box regression layers\n",
    "    for Fast R-CNN. At the beginning, we add a linear layer with weight matrix\n",
    "    equals to our transformation matrix and with no biases. This weight matrix is not trained.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "        m_transfo (float): transformation matrix for subspace alignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes, m_transfo):\n",
    "        super(FastRCNNPredictor_custom, self).__init__()\n",
    "        \n",
    "        self.cls_score = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes))\n",
    "        \n",
    "        self.bbox_pred = nn.Sequential(nn.Linear(in_features=1024, \n",
    "                                                 out_features = in_channels, \n",
    "                                                 bias=False), \n",
    "                                       nn.Linear(in_channels, num_classes * 4))\n",
    "        \n",
    "        self.cls_score[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "        self.bbox_pred[0].weight = nn.Parameter(m_transfo, requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            assert list(x.shape[2:]) == [1, 1]\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "\n",
    "        return scores, bbox_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the same model that we just trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can just load a model by uncommenting the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNCOMMENT THE FOLLOWING LINE TO LOAD A MODEL:\n",
    "# model, optimizer, lr_scheduler = load_model(3, \"10_rpn_roi_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of dimensions to keep in PCA\n",
    "d_pca = 512\n",
    "\n",
    "# Tell the program to save the matrix created\n",
    "save_matrixes = False\n",
    "save_projected_matrixes = False\n",
    "save_name = 'col_10_rpn_roi_online_512'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thres_IoU = 0.50\n",
    "count = 0\n",
    "\n",
    "X_source = torch.tensor([])\n",
    "bbox_idx = torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# for each image in source data\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    \n",
    "    # forward hooks to obtain the feature representation at the output of \"box_head\" for each proposed region and their\n",
    "    # coordinates\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    # coordinates of proposed regions\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    \n",
    "    # feature representation of proposed regions\n",
    "    feat = outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "    bbox_idx_to_keep = torch.tensor([])\n",
    "    \n",
    "    # keeps a feature representation only if the IoU of its associated region is higher than thres_IoU\n",
    "    # with a ground truth bounding box\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs = bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "if save_matrixes:\n",
    "    torch.save(X_source, 'saved_matrixes/X_source_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data\n",
    "X_source_scaled = center_data(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=d_pca)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projected matrix\n",
    "if save_projected_matrixes:\n",
    "    torch.save(X_source_proj, 'saved_matrixes/X_source_proj_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the dataloaders with batch size from the paper for better comparison\n",
    "trans_test_batch1_dataloader = create_dataloader(trans_test_ann_path, 1, light=light)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# for each image in target data\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "        \n",
    "    # forward hook and forward pass to sub-parts of the model to obtain the feature representation at the output\n",
    "    # of \"box_head\" for each proposed region and their confidence scores\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    # keeps a feature representation only if its confidence score is higher than thres_conf_score  \n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the matrix\n",
    "if save_matrixes:\n",
    "    torch.save(X_target, 'saved_matrixes/X_target_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data\n",
    "X_target_scaled = center_data(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only an amount of first components which gives the Projected source matrix\n",
    "\n",
    "pca_proj = PCA(n_components=d_pca)\n",
    "pca_proj.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca_proj.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca_proj.explained_variance_ratio_) # we keep d dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the projected matrix\n",
    "if save_projected_matrixes:\n",
    "    torch.save(X_target_proj, 'saved_matrixes/X_target_proj_05_' + save_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "ð‘€ is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa = torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the device for pytorch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the matrixes on the right devices\n",
    "M.to(device)\n",
    "Xa.to(device)\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained_rpn(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# load fine-tuned weights from the model of the projections\n",
    "model.load_state_dict(torch.load('saved_models/10_rpn_roi_4_model.pt'))\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "# vector are of size 100 after the transformation\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor_custom(M.shape[0], 2, Xa.T.float())\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor_custom(in_channels=100, num_classes=2, m_transfo=Xa.T.float()) \n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 10\n",
    "\n",
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the log results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converts the logs to lists and the tensors to numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])\n",
    "    \n",
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(cis_valid_dataloader)):\n",
    "        loss_interm += cis_valid_logs[(len(cis_valid_dataloader) * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)\n",
    "\n",
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(len(trans_valid_dataloader)):\n",
    "        loss_interm += trans_valid_logs[(len(trans_valid_dataloader) * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making of a figure containing a plot for the training loss and another for the valid losses\n",
    "# (RPN or ROI) base on the type of model\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figure\n",
    "fig.savefig(\"saved_figures/\" + time.strftime(\"%Y%m%d_%H%M%S\") + \"_figure.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "cis_coco_evaluator_method = evaluate(cis_test_dataloader, cis_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans with method 3\n",
    "model.roi_heads.box_predictor.cls_score[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False) \n",
    "model.roi_heads.box_predictor.bbox_pred[0].weight = nn.Parameter(X_target_proj.float(), requires_grad = False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes +- 15min to run on cis_test\n",
    "trans_coco_evaluator_method = evaluate(trans_test_dataloader, trans_coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cis test 10 epochs rpn roi 4, method3.1 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "cis_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('trans test 10 epochs rpn+roi 4, method3.1 with 15 epochs & d=100')\n",
    "print('_'*80)\n",
    "trans_coco_evaluator_method.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
