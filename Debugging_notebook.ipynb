{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578fe4b4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2aa2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad7356",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5adfe821",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2abcf0",
   "metadata": {},
   "source": [
    "## Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c31db297",
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a366a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cis test set length: 15827\n",
      "cis val set length: 3484\n",
      "train set length: 13553\n",
      "trans test set length: 23275\n",
      "trans val set length: 1725\n"
     ]
    }
   ],
   "source": [
    "print('cis test set length:', len(cis_test_ann['images']))\n",
    "print('cis val set length:', len(cis_val_ann['images']))\n",
    "print('train set length:', len(train_ann['images']))\n",
    "print('trans test set length:', len(trans_test_ann['images']))\n",
    "print('trans val set length:', len(trans_val_ann['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55e7c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = random.randint(0, len(train_ann['images'])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(range(len(train_ann['images'])), 1000)\n",
    "images = [train_ann['images'][i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_ann.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e398d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_ann['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_val_ann['images'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43731239",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ann['images'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f2075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cis_test_ann['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_ann['images'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689394f",
   "metadata": {},
   "source": [
    "## Horizontal flip debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "378fa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(range(len(train_ann['images'])), 1000)\n",
    "images = [train_ann['images'][i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "print(trans_test_ann['images'][i])\n",
    "img_path = os.path.join('eccv_18_all_images_sm', trans_test_ann['images'][i]['file_name']) # to change\n",
    "\n",
    "image = read_image(img_path)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image[0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885c147",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "boxes = [trans_test_ann['annotations'][j]['bbox'] for j in range(len(trans_test_ann['annotations'])) \n",
    "         if trans_test_ann['annotations'][j]['image_id']==trans_test_ann['images'][i]['id'] \n",
    "         and 'bbox' in trans_test_ann['annotations'][j].keys()]\n",
    "\n",
    "img_path = os.path.join('eccv_18_all_images_sm', trans_test_ann['images'][i]['file_name']) # to change\n",
    "\n",
    "image = read_image(img_path)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image[0].squeeze(), cmap='gray')\n",
    "\n",
    "scale_x = image.shape[2] / trans_test_ann['images'][i]['width'] \n",
    "scale_y = image.shape[1] / trans_test_ann['images'][i]['height']\n",
    "\n",
    "boxes = torch.as_tensor(boxes)\n",
    "\n",
    "for i in range(boxes.shape[0]):\n",
    "    boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "    boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "    boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "    boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "    boxes[i][2] = boxes[i][0] + boxes[i][2]\n",
    "    boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "target = {}\n",
    "target[\"boxes\"] = boxes\n",
    "\n",
    "rect = patches.Rectangle((boxes[0][0], boxes[0][1]), boxes[0][2]-boxes[0][0], \n",
    "                         boxes[0][3]-boxes[0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ea727",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979432fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = Image.open(img_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa230744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8da8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = image2.size[0], image2.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecea1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('width:', width)\n",
    "print('height:', height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_new = conv(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('image_new.shape:', image_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f62458",
   "metadata": {},
   "source": [
    "## Utils part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from local files (from github repo)\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11229c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None, target_transform=None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "            #boxes[i][0]*=scale_x\n",
    "            #boxes[i][1]*=scale_y\n",
    "            #boxes[i][2]*=scale_x\n",
    "            #boxes[i][3]*=scale_y\n",
    "\n",
    "        label=torch.as_tensor(label)\n",
    "        label=torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed87d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some helper functions ####\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "def date_to_integer(dt_time):\n",
    "    '''\n",
    "    importing datetime from image and converting to int\n",
    "    arg@ datatime in format str \"yyyy/mm/dd hh:mm:ss.ss\"\n",
    "    return: int@ yyyymmdd\n",
    "    '''\n",
    "    strDate = dt_time.replace(\"-\", \"/\")\n",
    "    date_object = datetime.strptime(strDate, \"%Y/%m/%d %H:%M:%S\")\n",
    "    return 10000*date_object.year + 100*date_object.month + date_object.day\n",
    "\n",
    "def getYear(dt_time):\n",
    "    '''\n",
    "    extract the year from image infromation\n",
    "    return: year = int\n",
    "    '''\n",
    "    strDate = dt_time.replace(\"-\", \"/\")\n",
    "    date_object = datetime.strptime(strDate, \"%Y/%m/%d %H:%M:%S\")\n",
    "    return date_object.year\n",
    "\n",
    "def getMonth(dt_time):\n",
    "    '''\n",
    "    extract the month from image infromation\n",
    "    return: year = int\n",
    "    '''\n",
    "    strDate = dt_time.replace(\"-\", \"/\")\n",
    "    date_object = datetime.strptime(strDate, \"%Y/%m/%d %H:%M:%S\")\n",
    "    return date_object.month\n",
    "\n",
    "def get_image_RGB(filename):\n",
    "    '''\n",
    "    # read an image form the dataset in : '/content/drive/MyDrive/AdvancedProjectML/data/eccv_18_all_images_sm'\n",
    "    '''\n",
    "    allImagesPath = '/content/drive/MyDrive/AdvancedProjectML/data/eccv_18_all_images_sm'\n",
    "    img_path = os.path.join(allImagesPath,filename) \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_image_tensor(filename):\n",
    "    '''\n",
    "    # read an image form the dataset in : '/content/drive/MyDrive/AdvancedProjectML/data/eccv_18_all_images_sm'\n",
    "    '''\n",
    "    allImagesPath = '/content/drive/MyDrive/AdvancedProjectML/data/eccv_18_all_images_sm'\n",
    "    img_path = os.path.join(allImagesPath,filename) \n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    conv=torchvision.transforms.ToTensor()\n",
    "    imageTensor=conv(image)\n",
    "    return  imageTensor\n",
    "\n",
    "## Saving as jpge ###\n",
    "def saveImage(image, path, thisImageFileName):\n",
    "    '''\n",
    "    Saving as jpge.\n",
    "    arg@ timage : Tensor\n",
    "    arg@ thisImageFileName : str \n",
    "    arg @ path : str. The project path : '/content/drive/MyDrive/AdvancedProjectML/data'\n",
    "\n",
    "    self.quality : int between 1 - 100. Define the image quality at save time \n",
    "    Ex from source: write_jpeg(input: torch.Tensor, filename: str, quality: int(1-100) = 75). It works with int = 100\n",
    "    '''\n",
    "    quality = 100\n",
    "    if path == None:\n",
    "        path = '/content/drive/MyDrive/AdvancedProjectML/data'\n",
    "    if thisImageFileName == None:\n",
    "        thisImageFileName = 'first'\n",
    "    destinyPath = os.path.join(path, thisImageFileName)\n",
    "    print(\"Saving...\")\n",
    "    write_jpeg(image, destinyPath, quality)\n",
    "\n",
    "def showImageRGB(image): \n",
    "    '''\n",
    "    input@ image filename\n",
    "    e.i. fileName =  trans_val_ann['images'][i]['file_name'] \n",
    "    '''\n",
    "    conv=torchvision.transforms.ToTensor()\n",
    "    image_new=conv(image)\n",
    "    plt.rcParams['font.size'] = 2\n",
    "    fig, ax = plt.subplots(1,3,figsize=(8,4), dpi=150)\n",
    "    ax[0].imshow(image_new[0], cmap='BuGn_r') #,cmap=\"gray\"\n",
    "    ax[1].imshow(image_new[1], cmap='BuGn_r')\n",
    "    ax[2].imshow(image_new[2], cmap='BuGn_r')\n",
    "\n",
    "def showImageTensor(image): \n",
    "    '''\n",
    "    input@ image filename\n",
    "    e.i. fileName =  trans_val_ann['images'][i]['file_name'] \n",
    "    '''\n",
    "    plt.rcParams['font.size'] = 2\n",
    "    fig, ax = plt.subplots(1,3,figsize=(6,2), dpi=150)\n",
    "    ax[0].imshow(image[0]) #,cmap=\"gray\"\n",
    "    ax[1].imshow(image[1])\n",
    "    ax[2].imshow(image[2])\n",
    "\n",
    "\n",
    "### Import list of images names from a file\n",
    "def importImageNamesListFromFile(finalPath):\n",
    "    '''\n",
    "    Import the names of all files in the directory as a list of str\n",
    "    To Adapt: the directory path after tha main path.\n",
    "    main path for this project: '/content/drive/MyDrive/AdvancedProjectML/data/....\n",
    "    ex: finalPath = 'Background'\n",
    "    '''\n",
    "    mainPath = '/content/drive/MyDrive/AdvancedProjectML/data/'\n",
    "    destinyPath = os.path.join(mainPath, finalPath)\n",
    "    onlyFilesNames = [f for f in listdir(destinyPath)]\n",
    "    return onlyFilesNames\n",
    "\n",
    "def get_bbox(dataset,idx):\n",
    "    '''\n",
    "    retur@ list@ bbox from the image with filename=filename\n",
    "    '''\n",
    "    print(idx)\n",
    "    return dataset['annotations'][idx]['bbox']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6d6a7",
   "metadata": {},
   "source": [
    "## Exemple of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d141254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images bounding boxes *takes about 25sec*\n",
    "train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "cis_val_valid_img,_ = get_img_with_bbox(cis_val_ann_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb69fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CustomImageDataset(train_ann_path, img_folder, train_valid_img)\n",
    "valid_data = CustomImageDataset(cis_val_ann_path, img_folder, cis_val_valid_img)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)\n",
    "\n",
    "# In paper : ' We use a batch size of 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72456c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "#print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Target (Bbox) batch shape: {train_labels[0]['boxes'].size()}\")\n",
    "print(f\"Target (category) batch shape: {train_labels[0]['labels'].size()}\")\n",
    "\n",
    "img = train_features[0][0].squeeze()\n",
    "label = train_labels[0]['labels']\n",
    "label_categ='animal'\n",
    "\n",
    "if label[0]==0:\n",
    "    label_categ='background'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img,cmap=\"gray\")\n",
    "rect = patches.Rectangle((train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][1]), train_labels[0]['boxes'][0][2]-train_labels[0]['boxes'][0][0], train_labels[0]['boxes'][0][3]-train_labels[0]['boxes'][0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)\n",
    "print(f\"Label: {label_categ}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f70a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = data_transform['train']\n",
    "img2, target2 = trans(image, target)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img2, cmap=\"gray\")\n",
    "\n",
    "rect = patches.Rectangle((target2['boxes'][0][0], target2['boxes'][0][1]), \n",
    "                         target2['boxes'][0][2] - target2['boxes'][0][0], \n",
    "                         target2['boxes'][0][3] - target2['boxes'][0][1], \n",
    "                         linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax.add_patch(rect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
