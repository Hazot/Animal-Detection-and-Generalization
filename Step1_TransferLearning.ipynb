{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3RUQCnETVJhK"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "import pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from local lib files\n",
    "import utils\n",
    "import transforms\n",
    "import coco_eval\n",
    "from engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFo8FhOT4-Yf"
   },
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IlU99PhcSNDv"
   },
   "outputs": [],
   "source": [
    "output_path = 'output'\n",
    "img_folder = 'eccv_18_all_images_sm'\n",
    "\n",
    "cis_test_ann_path = 'eccv_18_annotation_files/cis_test_annotations.json'\n",
    "cis_val_ann_path = 'eccv_18_annotation_files/cis_val_annotations.json'\n",
    "train_ann_path = 'eccv_18_annotation_files/train_annotations.json'\n",
    "trans_test_ann_path = 'eccv_18_annotation_files/trans_test_annotations.json'\n",
    "trans_val_ann_path = 'eccv_18_annotation_files/trans_val_annotations.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08r4QhagWuKZ"
   },
   "source": [
    "## Basic data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5tYL7gDDW09R"
   },
   "outputs": [],
   "source": [
    "cis_test_ann = json.load(open(cis_test_ann_path))\n",
    "cis_val_ann = json.load(open(cis_val_ann_path))\n",
    "train_ann = json.load(open(train_ann_path))\n",
    "trans_test_ann = json.load(open(trans_test_ann_path))\n",
    "trans_val_ann = json.load(open(trans_val_ann_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qbuy1jLsWuew",
    "outputId": "7ba79560-5443-4fea-f46c-c0e3acaa1588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cis test set length: 15827\n",
      "cis val set length: 3484\n",
      "train set length: 13553\n",
      "trans test set length: 23275\n",
      "trans val set length: 1725\n"
     ]
    }
   ],
   "source": [
    "print('cis test set length:', len(cis_test_ann['images']))\n",
    "print('cis val set length:', len(cis_val_ann['images']))\n",
    "print('train set length:', len(train_ann['images']))\n",
    "print('trans test set length:', len(trans_test_ann['images']))\n",
    "print('trans val set length:', len(trans_val_ann['images']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "iOwBQIVEYWkN",
    "outputId": "c91a5292-95b4-4e30-9e96-a5cbbc6cdf66"
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "\n",
    "# boxes = [trans_val_ann['annotations'][j]['bbox'] for j in range(len(trans_val_ann['annotations'])) \n",
    "#          if trans_val_ann['annotations'][j]['image_id']==trans_val_ann['images'][i]['id'] \n",
    "#          and 'bbox' in trans_val_ann['annotations'][j].keys()]\n",
    "\n",
    "# img_path = os.path.join('eccv_18_all_images_sm', trans_val_ann['images'][i]['file_name']) # to change\n",
    "\n",
    "# image = read_image(img_path)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(image[0].squeeze(),cmap=\"gray\")\n",
    "\n",
    "# scale_x = image.shape[2] / trans_val_ann['images'][i]['width'] \n",
    "# scale_y = image.shape[1] / trans_val_ann['images'][i]['height']\n",
    "\n",
    "# boxes = torch.as_tensor(boxes)\n",
    "\n",
    "# for i in range(boxes.shape[0]):\n",
    "#     boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "#     boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "#     boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "#     boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "#     boxes[i][2] = boxes[i][0] + boxes[i][2]\n",
    "#     boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "# target = {}\n",
    "# target[\"boxes\"] = boxes\n",
    "\n",
    "# rect = patches.Rectangle((boxes[0][0], boxes[0][1]), boxes[0][2]-boxes[0][0], \n",
    "#                          boxes[0][3]-boxes[0][1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "# ax.add_patch(rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OMhB4CM354Px"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3mHaZNrt7D98"
   },
   "outputs": [],
   "source": [
    "# In paper :  ' ... and employ horizontal flipping for data augmentation. ( for detection)\n",
    "\n",
    "import transforms as T   # from git hub repo\n",
    "\n",
    "data_transform = {'train': T.RandomHorizontalFlip(0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list with the idx of images with at least one bounding box (img_wbbox) and a \n",
    "# list with the number of bbox for each valid image (num_bbox)\n",
    "def get_img_with_bbox(file_path):\n",
    "  \n",
    "    file = json.load(open(file_path))\n",
    "    img_wbbox = []\n",
    "    num_bbox = []\n",
    "\n",
    "    for i in range(len(file['images'])):\n",
    "        bboxes = [file['annotations'][j]['bbox'] \n",
    "                  for j in range(len(file['annotations'])) \n",
    "                  if file['annotations'][j]['image_id']==file['images'][i]['id'] \n",
    "                  and 'bbox' in file['annotations'][j].keys()]\n",
    "\n",
    "        if len(bboxes)!=0:\n",
    "            img_wbbox.append(i)\n",
    "\n",
    "            num_bbox.append(len(bboxes))\n",
    "\n",
    "    return img_wbbox, num_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SdJaZm5aOJ6y"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_path, img_dir, valid_img, transform = None, target_transform=None):\n",
    "        self.label_file = json.load(open(label_path))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.valid_img = valid_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.valid_img[idx] # consider only images with bbox annotations\n",
    "        img_path = os.path.join(self.img_dir, self.label_file['images'][idx]['file_name'])\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        conv = torchvision.transforms.ToTensor()\n",
    "        # if image.shape[0]==1:\n",
    "        # some images have only one channel, we convert them to rgb\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = conv(image)\n",
    "\n",
    "        boxes = [self.label_file['annotations'][j]['bbox'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "        \n",
    "        label = [self.label_file['annotations'][j]['category_id'] \n",
    "                 for j in range(len(self.label_file['annotations'])) \n",
    "                 if self.label_file['annotations'][j]['image_id']==self.label_file['images'][idx]['id']]\n",
    "\n",
    "        # transform bbox coords to adjust for resizing\n",
    "        scale_x = image.shape[2] / self.label_file['images'][idx]['width'] \n",
    "        scale_y = image.shape[1] / self.label_file['images'][idx]['height']\n",
    "\n",
    "        boxes = torch.as_tensor(boxes)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            boxes[i][0] = torch.round(boxes[i][0] * scale_x)\n",
    "            boxes[i][1] = torch.round(boxes[i][1] * scale_y)\n",
    "            boxes[i][2] = torch.round(boxes[i][2] * scale_x)\n",
    "            boxes[i][3] = torch.round(boxes[i][3] * scale_y)\n",
    "\n",
    "            boxes[i][2] = boxes[i][0] + boxes[i][2] # to transform to pytorch bbox format\n",
    "            boxes[i][3] = boxes[i][1] + boxes[i][3]\n",
    "\n",
    "            #boxes[i][0]*=scale_x\n",
    "            #boxes[i][1]*=scale_y\n",
    "            #boxes[i][2]*=scale_x\n",
    "            #boxes[i][3]*=scale_y\n",
    "\n",
    "        label = torch.as_tensor(label)\n",
    "        label = torch.where(label==30,0,1)  # 0 if empty (categ id = 30), 1 if animal\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = label\n",
    "        target[\"image_id\"] = image_id\n",
    "        target['area']=area\n",
    "        target['iscrowd']=iscrowd\n",
    "\n",
    "        # TO DO : resize all to same size\n",
    "\n",
    "        if self.transform:\n",
    "            # transform image AND target\n",
    "            image, target = self.transform(image, target)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuPqrCPG8wsr"
   },
   "source": [
    "## Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6UGQCk7Gcoy7"
   },
   "outputs": [],
   "source": [
    "# Inspired from https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=YjNHjVMOyYlH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with only the last layer to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuz8DJUgccUx"
   },
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with deeper layers to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_pretrained(num_classes):\n",
    "\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    for param in model.parameters(): # to freeze all existing weights\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.roi_heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "hbSuc8Jwc5qT"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "# like in the paper, construct the scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataloaders\n",
    "To load the data of the dataset efficiently for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hNli84Y1V-Az"
   },
   "outputs": [],
   "source": [
    "train_valid_img,_ = get_img_with_bbox(train_ann_path)\n",
    "train_data = CustomImageDataset(train_ann_path, img_folder, train_valid_img)\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NdZtIwI_iy9l"
   },
   "outputs": [],
   "source": [
    "trans_val_valid_img,_ = get_img_with_bbox(trans_val_ann_path)\n",
    "trans_valid_data = CustomImageDataset(trans_val_ann_path, img_folder, trans_val_valid_img)\n",
    "trans_valid_dataloader = DataLoader(trans_valid_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VdGh66kZ_8av"
   },
   "outputs": [],
   "source": [
    "cis_val_valid_img,_ = get_img_with_bbox(cis_val_ann_path)\n",
    "cis_valid_data = CustomImageDataset(cis_val_ann_path, img_folder, cis_val_valid_img)\n",
    "cis_valid_dataloader = DataLoader(cis_valid_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cis_test_img,_ = get_img_with_bbox(cis_test_ann_path)\n",
    "cis_test_data = CustomImageDataset(cis_test_ann_path,img_folder, cis_test_img)\n",
    "cis_test_dataloader = DataLoader(cis_test_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test_img,_ = get_img_with_bbox(trans_test_ann_path)\n",
    "trans_test_data = CustomImageDataset(trans_test_ann_path,img_folder, trans_test_img)\n",
    "trans_test_dataloader = DataLoader(trans_test_data, batch_size=10, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading/Importing a model\n",
    "#### Need to initiate the model, the optimizer and de scheduler before loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEED TO INITIATE THE MODEL, THE OPTIMIZER AND THE SCHEDULER BEFOREHAND (if )\n",
    "# load the model, the optimizer and the scheduler\n",
    "model.load_state_dict(torch.load('saved_models/25_roi_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/25_roi_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/25_roi_scheduler.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the smoothed values to a dictionnary of each values\n",
    "def smoothed_value_to_str(smoothed_value):\n",
    "    d_values = {}\n",
    "    d_values['median'] = smoothed_value.median\n",
    "    d_values['avg'] = smoothed_value.avg\n",
    "    d_values['global_avg'] = smoothed_value.global_avg\n",
    "    d_values['max'] = smoothed_value.max\n",
    "    d_values['value'] = smoothed_value.value\n",
    "    return d_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the train logs from MetricLogger to list\n",
    "def train_logs_to_lst(logs):\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].meters.keys():\n",
    "            d[key] = smoothed_value_to_str(logs[i].meters[key])\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the training logs into a json file with time dependent file name\n",
    "def train_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    train_metric_logs = train_logs_to_lst(logs)\n",
    "    filename = ftime + \"_train_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid logs utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dicts of a list \n",
    "def merge_dict(logs):\n",
    "    logs_better = []\n",
    "    try:\n",
    "        for i in range(len(logs)):\n",
    "            logs_better.append({**logs[i][0], **logs[i][1], **logs[i][2], **logs[i][3]})\n",
    "        return logs_better\n",
    "    except:\n",
    "        print(logs[0])\n",
    "        logs_better = logs\n",
    "        return logs_better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the valid logs from list of dictionnaries to string\n",
    "def valid_logs_to_lst(valid_logs):\n",
    "    logs = merge_dict(valid_logs)\n",
    "    lst = []\n",
    "    for i in range(len(logs)):\n",
    "        d = {}\n",
    "        for key in logs[i].keys():\n",
    "            d[key] = logs[i][key].cpu().numpy().tolist()\n",
    "        lst.append(d)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the cis validation logs into a json file with time dependent file name\n",
    "def cis_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_cis_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the trans validation logs into a json file with time dependent file name\n",
    "def trans_valid_logs_to_json(logs, ftime=time.strftime(\"%Y%m%d_%H%M%S\")):\n",
    "    valid_metric_logs = valid_logs_to_lst(logs)\n",
    "    filename = ftime + \"_trans_valid_logs.json\"\n",
    "    \n",
    "    with open('saved_logs/' + filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(valid_metric_logs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS TO TUNE BEFORE TRAINING\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1080 Ti'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHECK DEVICE BEFORE TRAINING\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train function\n",
    "def train(dataloader, num_epochs, save_logs=True, save_model=True, print_freq=100):\n",
    "    \n",
    "    all_train_logs = []\n",
    "    all_cis_valid_logs = []\n",
    "    all_trans_valid_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # train for one epoch, printing every 100 images\n",
    "        train_logs = train_one_epoch(model, optimizer, dataloader, device, epoch, print_freq)\n",
    "        all_train_logs.append(train_logs)\n",
    "        \n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # evaluate on the validation dataset after training one epoch\n",
    "        for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                trans_loss_dict = model(images, targets)\n",
    "                trans_loss_dict = [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "                all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "        for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cis_loss_dict = model(images, targets)\n",
    "                cis_loss_dict = [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "                all_cis_valid_logs.append(cis_loss_dict)\n",
    "    \n",
    "    filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if save_logs:\n",
    "        \n",
    "        # save the train, cis valid and trans valid logs\n",
    "        train_logs_to_json(all_train_logs, filetime)\n",
    "        cis_valid_logs_to_json(all_cis_valid_logs, filetime)\n",
    "        trans_valid_logs_to_json(all_trans_valid_logs, filetime)\n",
    "        \n",
    "    if save_model:\n",
    "        \n",
    "        # save the model, the optimizer and the scheduler\n",
    "        torch.save(model.state_dict(), 'saved_models/' + filetime + 'model.pt')\n",
    "        torch.save(optimizer.state_dict(), 'saved_models/' + filetime + 'optimizer.pt')\n",
    "        torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + 'scheduler.pt')\n",
    "    \n",
    "    return all_train_logs, all_trans_valid_logs, all_cis_valid_logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This next cell starts the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "all_train_logs, all_trans_valid_logs, all_cis_valid_logs = train(dataloader=train_dataloader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the last training logs to variables\n",
    "##### Ensures that if you hit the training cell, you don't lose the variables containing the logs from the last run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_train_logs = all_train_logs\n",
    "last_trans_valid_logs = all_trans_valid_logs\n",
    "last_cis_valid_logs = all_cis_valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving manually every logs from training to json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the log with the same time\n",
    "train_logs_to_json(last_train_logs)\n",
    "trans_valid_logs_to_json(last_trans_valid_logs)\n",
    "cis_valid_logs_to_json(last_cis_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model, the optimizer and the scheduler\n",
    "filetime = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "torch.save(model.state_dict(), 'saved_models/' + filetime + '_model.pt')\n",
    "torch.save(optimizer.state_dict(), 'saved_models/' + filetime + '_optimizer.pt')\n",
    "torch.save(lr_scheduler.state_dict(), 'saved_models/' + filetime + '_scheduler.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of the raw logs\n",
    "##### Only look at the MetricLogger if you just trained the model. You cannot import the model and then check the MetricLogger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[0].meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_logs[0].meters['loss_box_reg'].global_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we check the amount of logs per epoch for each categories and the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_cis_valid_logs[0])\n",
    "print(\"total length:\", len(all_cis_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_cis_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_trans_valid_logs[0])\n",
    "print(\"total length:\", len(all_trans_valid_logs))\n",
    "print(\"-\"*8)\n",
    "print(\"per epoch length:\", len(all_trans_valid_logs)/num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at/Loading the logs in convenient ways\n",
    "Here we define the variables \"train_logs\", \"cis_valid_logs\" and \"trans_valid_logs\" that will be used in the methods for the results and the visualisations.\n",
    "\n",
    "We can import logs or use the ones from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL - Can load some logs right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported logs - format: name = \"NAME_OR_TIME\"      Exemple file format: \"NAME_OR_TIME_train_logs\"\n",
    "\n",
    "file_time_or_nickname = '25_base' # VALUE TO CHANGE TO THE IMPORTED FILES\n",
    "\n",
    "# Import training logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_train_logs.json', \"r\") as f:\n",
    "    train_logs = json.load(f)\n",
    "\n",
    "# Import cis valid logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_cis_valid_logs.json', \"r\") as f:\n",
    "    cis_valid_logs = json.load(f)\n",
    "\n",
    "# Import trans valid logs\n",
    "with open('saved_logs/' + file_time_or_nickname + '_trans_valid_logs.json', \"r\") as f:\n",
    "    trans_valid_logs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the last trained logs into convenient list variables\n",
    "##### Converts the logs to lists and the tensors to numpy - ONLY IF MODEL HAVE BEEN TRAINED IN THIS KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs = train_logs_to_lst(last_train_logs)\n",
    "cis_valid_logs = valid_logs_to_lst(last_cis_valid_logs)\n",
    "trans_valid_logs = valid_logs_to_lst(last_trans_valid_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To confirm that the data is loaded properly\n",
    "len(train_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loss to print (here we use global_avg but we can use: value, median, avg, max or global_avg)\n",
    "results_train_loss = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    results_train_loss.append(train_logs[i]['loss_box_reg']['global_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cis valid loss to print\n",
    "results_cis_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(167):\n",
    "        loss_interm += cis_valid_logs[(167 * i) + j]['loss_box_reg']\n",
    "    results_cis_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trans valid loss to print\n",
    "results_trans_valid_loss = [] # cis\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    loss_interm = 0\n",
    "    for j in range(154):\n",
    "        loss_interm += trans_valid_logs[(154 * i) + j]['loss_box_reg']\n",
    "    results_trans_valid_loss.append(loss_interm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the different plots\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax[0].plot(np.arange(1, num_epochs + 1), results_train_loss, label='train')\n",
    "ax[0].set_title('Train loss per epoch')\n",
    "ax[0].set_ylabel('loss_box_reg')\n",
    "ax[0].set_xlabel('epoch')\n",
    "\n",
    "plt.title('Train loss per epoch')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_cis_valid_loss, label='cis')\n",
    "ax[1].plot(np.arange(1, num_epochs + 1), results_trans_valid_loss, label='trans')\n",
    "ax[1].set_title('Valid loss per epoch')\n",
    "ax[1].set_ylabel('loss_box_reg')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the figure to pdf format in the figures folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"figures/25_epochs_normal_baseline.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPV4Pxyajekr"
   },
   "source": [
    "## Make Predictions with a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL - Can load a model right here to test the predictions quickly\n",
    "#### Need to initiate the model, the optimizer and de scheduler before loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO INITIATE THE MODEL, THE OPTIMIZER AND THE SCHEDULER BEFOREHAND (if )\n",
    "# load the model, the optimizer and the scheduler\n",
    "model.load_state_dict(torch.load('saved_models/25_roi_model.pt'))\n",
    "optimizer.load_state_dict(torch.load('saved_models/25_roi_optimizer.pt'))\n",
    "lr_scheduler.load_state_dict(torch.load('saved_models/25_roi_scheduler.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTlEPVhRVHE"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(trans_valid_dataloader))\n",
    "image = list(image.to(device) for image in train_features)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "      pred = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints 10 images with the predictions before and after NMS\n",
    "for image_i in range(len(image)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "    ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                              train_labels[image_i]['boxes'][0][1]), \n",
    "                             train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                             train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[0].add_patch(rect)\n",
    "    ax[0].set_title('Ground truth')\n",
    "\n",
    "    # Predictions\n",
    "    ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in range(len(pred[image_i]['boxes'])):\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[1].add_patch(rect)\n",
    "    ax[1].set_title('Pred')\n",
    "\n",
    "    # Predictions after NMS\n",
    "    iou_threshold = 0.01 # param to tune\n",
    "    boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "    ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "    for i in boxes_to_keep:\n",
    "        rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                                  pred[image_i]['boxes'][i][1].cpu()), \n",
    "                                 (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                                 (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                                 linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax[2].add_patch(rect)\n",
    "\n",
    "    ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "tkwIJ6cCeRq3",
    "outputId": "33012e5f-2664-46ef-fa1e-b54bf5e2faf5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a single image chosen by index from the last batch of 10 predictions\n",
    "image_i = 0 # from 0 to 9 included\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(24,16))\n",
    "\n",
    "ax[0].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "rect = patches.Rectangle((train_labels[image_i]['boxes'][0][0], \n",
    "                          train_labels[image_i]['boxes'][0][1]), \n",
    "                         train_labels[image_i]['boxes'][0][2]-train_labels[image_i]['boxes'][0][0], \n",
    "                         train_labels[image_i]['boxes'][0][3]-train_labels[image_i]['boxes'][0][1], \n",
    "                         linewidth=2, edgecolor='r', facecolor='none')\n",
    "ax[0].add_patch(rect)\n",
    "ax[0].set_title('Ground truth')\n",
    "\n",
    "# Predictions\n",
    "ax[1].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in range(len(pred[image_i]['boxes'])):\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[1].add_patch(rect)\n",
    "ax[1].set_title('Pred')\n",
    "\n",
    "# Predictions after NMS\n",
    "iou_threshold = 0.01 # param to tune\n",
    "boxes_to_keep = torchvision.ops.nms(pred[image_i]['boxes'], pred[image_i]['scores'], iou_threshold = iou_threshold).cpu()\n",
    "ax[2].imshow(train_features[image_i][0].squeeze(),cmap=\"gray\")\n",
    "for i in boxes_to_keep:\n",
    "    rect = patches.Rectangle((pred[image_i]['boxes'][i][0].cpu(), \n",
    "                              pred[image_i]['boxes'][i][1].cpu()), \n",
    "                             (pred[image_i]['boxes'][i][2]-pred[image_i]['boxes'][i][0]).cpu(), \n",
    "                             (pred[image_i]['boxes'][i][3]-pred[image_i]['boxes'][i][1]).cpu(), \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax[2].add_patch(rect)\n",
    "\n",
    "ax[2].set_title('After NMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[image_i]['boxes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[image_i]['boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutate on COCO detection metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cis Test on COCO metrics\n",
    "##### 'For evaluation, we consider a detected box to be correct if its IoU ≥ 0.5 with a ground truth box.'\n",
    "\n",
    "We need to look at the precison score with IoU=0.5, area=all and maxDets=100.\n",
    "For the recall score, by default it's IoU=0.5:IoU=0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perfo on COCO detection metrics\n",
    "\n",
    "# takes +- 25min to run on cis_test\n",
    "\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types \n",
    "\n",
    "apply_nms = True\n",
    "iou_threshold = 0.35 # param to potentially tune\n",
    "the_data_loader = cis_test_dataloader # change to test set\n",
    "\n",
    "coco = get_coco_api_from_dataset(the_data_loader.dataset)\n",
    "iou_types = _get_iou_types(model)\n",
    "coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in the_data_loader:\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred=model(images)\n",
    "\n",
    "        if apply_nms:\n",
    "            boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "            pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "            pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "            pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "        outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "print('_'*20)\n",
    "print('Cis Test Data - Summary')\n",
    "print(\" \")\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trans Test on COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate perfo on COCO detection metrics\n",
    "\n",
    "# takes +- 25min to run on trans_test\n",
    "\n",
    "from coco_utils import get_coco_api_from_dataset\n",
    "from coco_eval import CocoEvaluator\n",
    "from engine import _get_iou_types \n",
    "\n",
    "apply_nms = True\n",
    "iou_threshold = 0.35 # param to potentially tune\n",
    "the_data_loader = trans_test_dataloader # change to test set\n",
    "\n",
    "coco = get_coco_api_from_dataset(the_data_loader.dataset)\n",
    "iou_types = _get_iou_types(model)\n",
    "coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in the_data_loader:\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred=model(images)\n",
    "\n",
    "        if apply_nms:\n",
    "            boxes_to_keep = torchvision.ops.nms(pred[0]['boxes'], pred[0]['scores'], iou_threshold=iou_threshold).cpu()\n",
    "            pred[0]['boxes'] = pred[0]['boxes'][boxes_to_keep]\n",
    "            pred[0]['labels'] = pred[0]['labels'][boxes_to_keep]\n",
    "            pred[0]['scores'] = pred[0]['scores'][boxes_to_keep]\n",
    "\n",
    "        outputs = [{k: v.cpu() for k, v in t.items()} for t in pred]\n",
    "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
    "        coco_evaluator.update(res)\n",
    "\n",
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "print('_'*20)\n",
    "print('Trans Test Data - Summary')\n",
    "print(\" \")\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 (Subspace alignment based Domain adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.ops.boxes as bops\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Papers \n",
    "\n",
    " 1. https://arxiv.org/pdf/1507.05578.pdf\n",
    "\n",
    " 2.  https://openaccess.thecvf.com/content_iccv_2013/papers/Fernando_Unsupervised_Visual_Domain_2013_ICCV_paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct source matrix:** \n",
    "\n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, it has to have a IoU > thres_IoU with the ground truth of the given image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\miniconda3\\envs\\animals\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "# 20 minutes\n",
    "thres_IoU= 0.50\n",
    "count=0\n",
    "\n",
    "X_source=torch.tensor([])\n",
    "bbox_idx=torch.arange(1000)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in train_dataloader: \n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        hook = model.rpn.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "\n",
    "        outputs1 = []\n",
    "        hook1 = model.roi_heads.box_head.register_forward_hook(\n",
    "        lambda self, input, output: outputs1.append(output))\n",
    "\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "        hook1.remove()\n",
    "\n",
    "    coords = outputs[0][0][0].cpu() # [1000,4]\n",
    "    feat=outputs1[0].cpu() # [1000, 1024]\n",
    "\n",
    "    gt = targets[0]['boxes'].cpu()\n",
    "\n",
    "    bbox_idx_to_keep=torch.tensor([])\n",
    "    for i in range(gt.shape[0]):\n",
    "\n",
    "        IoUs=bops.box_iou(gt[i].reshape(1,4), coords)\n",
    "        IoUs = IoUs.reshape(1000)\n",
    "        bbox_idx_to_keep = torch.cat((bbox_idx_to_keep, bbox_idx[IoUs >= thres_IoU]),dim=0)\n",
    "\n",
    "    X_source = torch.cat((X_source,feat[torch.unique(bbox_idx_to_keep).long()]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78525, 1024])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source, 'saved_data/X_source_05_roi.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_source_scaled = scaler.fit_transform(X_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only the first 100 components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_source_scaled)\n",
    "\n",
    "X_source_proj = pca.components_\n",
    "X_source_proj = torch.from_numpy(X_source_proj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_source_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_source_proj, 'saved_data/X_source_proj_05_roi.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target data with batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target data/distribution = trans test set - Batch Size 1\n",
    "trans_test_batch1_img,_ = get_img_with_bbox(trans_test_ann_path)\n",
    "trans_test_batch1_data = CustomImageDataset(trans_test_ann_path, img_folder, trans_test_batch1_img)\n",
    "trans_test_batch1_dataloader = DataLoader(trans_test_batch1_data, batch_size=1, shuffle=True, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Construct target matrix:** \n",
    " \n",
    "We keep output of model.roi_heads.box_head (vector of size 1024) as feature representations of bounding boxes\n",
    " extracted by the RPN (region proposal network). For us to stack a box representation to the source matrix, the predicted bbox associated with the feature has to have a confidence score > thres_conf_score (since we don't use target labels we can't use the IoU here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "# 30 minutes\n",
    "thres_conf_score= 0.50 \n",
    "count=0\n",
    "\n",
    "X_target=torch.tensor([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for images, targets in trans_test_batch1_dataloader: # trans location valid AND test ?\n",
    "    images = [image.to(device) for image in images]\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    count+=1\n",
    "\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = []\n",
    "        hook = model.backbone.register_forward_hook(\n",
    "        lambda self, input, output: outputs.append(output))\n",
    "        res = model(images)\n",
    "        hook.remove()\n",
    "\n",
    "        box_features = model.roi_heads.box_roi_pool(outputs[0], [r['boxes'] for r in res], [i.shape[-2:] for i in images])\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "\n",
    "    X_target = torch.cat((X_target,box_features[res[0]['scores']>=thres_conf_score].cpu()), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30901, 1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target, 'saved_data/X_target_05_roi.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center data\n",
    "scaler = StandardScaler()\n",
    "X_target_scaled = scaler.fit_transform(X_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA, keep only the first 100 components which gives the Projected source matrix\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_target_scaled)\n",
    "\n",
    "X_target_proj = pca.components_\n",
    "X_target_proj = torch.from_numpy(X_target_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.explained_variance_ratio_) # we keep 100 dimensions\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1024])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_target_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_target_proj, 'saved_data/X_target_proj_05_roi.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation matrix M\n",
    "\n",
    "𝑀 is obtained by minimizing the following Bregman matrix divergence (following closed-form solution given in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project source data into target aligned source subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa= torch.matmul(X_source_proj.T,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x Xa (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet target data in target subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To project a given feature\n",
    "\n",
    "# feat(1,1024) x X_target_proj.T (1024,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loda because it takes time to generate the following matrices so they are saved\n",
    "X_source_proj = torch.load('saved_data/X_source_proj_05.pt')\n",
    "X_target_proj = torch.load('saved_data/X_target_proj_05.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_source_proj.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([1024, 100])\n"
     ]
    }
   ],
   "source": [
    "M = torch.matmul(X_source_proj, X_target_proj.T) # transformation matrix\n",
    "print(M.shape)\n",
    "\n",
    "Xa = torch.matmul(X_source_proj.T,M) # target aligned source subspace\n",
    "print(Xa.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.3206e-02,  8.6303e-01,  1.4386e-01,  ...,  1.8172e-02,\n",
       "          8.6311e-03,  2.3675e-03],\n",
       "        [ 7.7480e-01,  1.3245e-01, -7.4946e-03,  ...,  4.0069e-03,\n",
       "          4.4142e-03, -1.6273e-02],\n",
       "        [-1.7604e-01,  1.6074e-01,  3.3943e-01,  ..., -5.0102e-02,\n",
       "         -4.0499e-02,  6.5427e-04],\n",
       "        ...,\n",
       "        [ 2.1474e-03,  1.2598e-03, -1.7591e-03,  ...,  3.9705e-02,\n",
       "         -4.7052e-02, -4.1430e-02],\n",
       "        [-1.1055e-02,  5.6693e-04,  1.3001e-02,  ..., -1.9011e-02,\n",
       "         -7.1657e-02, -5.2998e-02],\n",
       "        [ 1.4891e-02, -2.4838e-03,  3.5215e-03,  ..., -8.3762e-02,\n",
       "         -6.9656e-02, -2.7392e-02]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0357,  0.0360, -0.0238,  ...,  0.0073, -0.0138,  0.0083],\n",
       "        [-0.0327,  0.0501,  0.0064,  ...,  0.0173,  0.0304,  0.0372],\n",
       "        [-0.0044, -0.0373, -0.0271,  ..., -0.0026,  0.0218, -0.0077],\n",
       "        ...,\n",
       "        [ 0.0277,  0.0458, -0.0143,  ..., -0.0438, -0.0245,  0.0184],\n",
       "        [-0.0575,  0.0260, -0.0327,  ...,  0.0186, -0.0134, -0.0029],\n",
       "        [ 0.0359,  0.0597, -0.0025,  ...,  0.0019, -0.0027,  0.0059]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xa.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_from_pretrained(num_classes)\n",
    "\n",
    "# load fine-tuned weights\n",
    "model.load_state_dict(torch.load('saved_models/25_roi_model.pt'))\n",
    "\n",
    "\n",
    "for param in model.parameters(): # to freeze all existing weights\n",
    "\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Tricks to include transformation to subspace in the model\n",
    "model.roi_heads.box_head.add_module('transfo',nn.Linear(in_features=1024, out_features=100, bias=False)) # no bias\n",
    "model.roi_heads.box_head.transfo.weight = nn.Parameter(Xa, requires_grad = False) # we want to keep these weightd (which are Xa) fixed\n",
    "\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(100, 2) # vector are of size 100 after the transformation\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "# We will only retrain model.roi_heads.box_predictor (2 last layers)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.0003, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[5,10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights to learn\n",
    "for i in range(4):\n",
    "\n",
    "    print(params[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "all_train_logs=[]\n",
    "all_trans_valid_logs=[]\n",
    "all_cis_valid_logs=[]\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # train for one epoch, printing every 100 images\n",
    "    train_logs = train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=100)\n",
    "    all_train_logs.append(train_logs)\n",
    "    \n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "\n",
    "    for images, targets in trans_valid_dataloader: # can do batch of 10 prob.\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            trans_loss_dict = model(images, targets)\n",
    "            trans_loss_dict= [{k: loss.to('cpu')} for k, loss in trans_loss_dict.items()]\n",
    "            all_trans_valid_logs.append(trans_loss_dict)\n",
    "\n",
    "\n",
    "    for images, targets in cis_valid_dataloader: # can do batch of 10 prob.\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cis_loss_dict = model(images, targets)\n",
    "            cis_loss_dict= [{k: loss.to('cpu')} for k, loss in cis_loss_dict.items()]\n",
    "            all_cis_valid_logs.append(cis_loss_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before testing the model on TRANS TEST, Xa (weights of model.roi_heads.box_head.transfo), \n",
    "# has to be replaced by X_target_proj.T\n",
    "\n",
    "# Should probably do also for trans valid losses?..\n",
    "\n",
    "model.roi_heads.box_head.transfo.weight = nn.Parameter(X_target_proj.T, requires_grad = False) \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Step1_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Animals",
   "language": "python",
   "name": "animals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
